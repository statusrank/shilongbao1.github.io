<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="google-site-verification" content="true">














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  
    
      
    

    
  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Georgia:300,300italic,400,400italic,700,700italic|Georgia:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">



  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png?v=5.1.4">







  <meta name="keywords" content="Deep Learning,">










<meta name="description" content="作业地址可查看github 一、初始化1.为什么神经网络的所有参数不能全部初始化为0&amp;gt;?若w初始化为0 ,很可能导致模型失效,无法收敛。也就是说如果我们初始将所有的w初始化为0,那么进行前向传播时每一层得到的值都是一样,这样一来当我们使用反向传播时,传回的梯度也是一样的,这就导致了我们更新参数后w还是一样的,这就使得我们的NN不能各自学习到很好的特征了。可以看这里">
<meta name="keywords" content="Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="deeplearning.ai 改善深度神经网络(正则化、优化、mini-batch等)">
<meta property="og:url" content="https://statusrank.xyz/articles/65d241ff.html">
<meta property="og:site_name" content="Statusrank&#39;s Blog">
<meta property="og:description" content="作业地址可查看github 一、初始化1.为什么神经网络的所有参数不能全部初始化为0&amp;gt;?若w初始化为0 ,很可能导致模型失效,无法收敛。也就是说如果我们初始将所有的w初始化为0,那么进行前向传播时每一层得到的值都是一样,这样一来当我们使用反向传播时,传回的梯度也是一样的,这就导致了我们更新参数后w还是一样的,这就使得我们的NN不能各自学习到很好的特征了。可以看这里">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://statusrank.xyz/articles/65d241ff/2.png">
<meta property="og:image" content="https://statusrank.xyz/articles/65d241ff/1.png">
<meta property="og:image" content="https://statusrank.xyz/articles/65d241ff/3.png">
<meta property="og:image" content="https://statusrank.xyz/articles/65d241ff/4.png">
<meta property="og:image" content="https://statusrank.xyz/articles/65d241ff/6.png">
<meta property="og:image" content="https://statusrank.xyz/articles/65d241ff/7.png">
<meta property="og:updated_time" content="2018-12-20T13:28:52.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="deeplearning.ai 改善深度神经网络(正则化、优化、mini-batch等)">
<meta name="twitter:description" content="作业地址可查看github 一、初始化1.为什么神经网络的所有参数不能全部初始化为0&amp;gt;?若w初始化为0 ,很可能导致模型失效,无法收敛。也就是说如果我们初始将所有的w初始化为0,那么进行前向传播时每一层得到的值都是一样,这样一来当我们使用反向传播时,传回的梯度也是一样的,这就导致了我们更新参数后w还是一样的,这就使得我们的NN不能各自学习到很好的特征了。可以看这里">
<meta name="twitter:image" content="https://statusrank.xyz/articles/65d241ff/2.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      appId: 'QRGHA1F933',
      apiKey: '159fa757deb7d0636724933110ae0f0d',
      indexName: 'dev_NAME',
      hits: {"per_page":10},
      labels: {"input_placeholder":"输入关键字","hits_empty":"没有找到与 ${query}有关的内容","hits_stats":"${hits}条相关记录,共耗时 ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://statusrank.xyz/articles/65d241ff.html">




  <title>deeplearning.ai 改善深度神经网络(正则化、优化、mini-batch等) | Statusrank's Blog</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-125051968-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?9d6a7ddabbfd7e0943f928cf28065aaf";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>
<script type="text/javascript" src="/lib/clipboard/clipboard.js"></script>
<script type="text/javascript" src="/js/src/custom.js"></script>
<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner">﻿<div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
      
        <span class="site-title">Statusrank's Blog</span>
            </a>
    </div>
      
        <p class="site-subtitle">佛系搬砖工</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            全部文章
          </a>
        </li>
      
        
        <li class="menu-item menu-item-top">
          <a href="/top/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-signal"></i> <br>
            
            热门文章
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://statusrank.xyz/articles/65d241ff.html">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Statusrank">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/tou.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Statusrank's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">deeplearning.ai 改善深度神经网络(正则化、优化、mini-batch等)</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-15T11:09:42+08:00">
                2018-08-15
              </time>
            

            

            
          </span>
	
  <span class="post-updated">
    &nbsp; | &nbsp; 更新于
    <time itemprop="dateUpdated" datetime="2018-12-20T21:28:52+08:00" content="2018-12-20">
      2018-12-20
    </time>
  </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/articles/65d241ff.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/articles/65d241ff.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/articles/65d241ff.html" class="leancloud_visitors" data-flag-title="deeplearning.ai 改善深度神经网络(正则化、优化、mini-batch等)">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><a href="https://github.com/statusrank/deeplearning.ai-note" target="_blank" rel="noopener">作业地址可查看github</a></p>
<h2 id="一、初始化"><a href="#一、初始化" class="headerlink" title="一、初始化"></a>一、初始化</h2><h3 id="1-为什么神经网络的所有参数不能全部初始化为0-gt"><a href="#1-为什么神经网络的所有参数不能全部初始化为0-gt" class="headerlink" title="1.为什么神经网络的所有参数不能全部初始化为0&gt;?"></a>1.为什么神经网络的所有参数不能全部初始化为0&gt;?</h3><p>若w初始化为0 ,很可能导致模型失效,无法收敛。也就是说如果我们初始将所有的w初始化为0,那么进行前向传播时每一层得到的值都是一样,这样一来当我们使用反向传播时,传回的梯度也是一样的,这就导致了我们更新参数后w还是一样的,这就使得我们的NN不能各自学习到很好的特征了。<a href="https://zhuanlan.zhihu.com/p/27190255" target="_blank" rel="noopener">可以看这里</a><br><a id="more"></a></p>
<h3 id="2-Xavier-Initialization"><a href="#2-Xavier-Initialization" class="headerlink" title="2.Xavier Initialization"></a>2.Xavier Initialization</h3><p>  Xavier Initialization 初始化的基本思想就是保持输入和输出的方差一致,这样就避免了所有的输出值趋向于0.<br>  首先对于前向传播,我们需要确保所有层的激活值方差近似相等,因此每个训练样本传播经过网络的信息才能保持平滑的属性。同样对于反向传播,每层梯度保持近似的方差将允许信息平滑地反向流动以更新权重。近似方差的梯度同样确保误差数据能够反馈到所有层级,因此它是整个训练中的关键。<br><a href="https://www.jiqizhixin.com/articles/2018-01-08-3" target="_blank" rel="noopener">这位大佬写的很不错</a></p>
<p>将W的方差变为 $ \frac{1}{layersdims[l - 1]} $</p>
<h3 id="3-He-Initialization"><a href="#3-He-Initialization" class="headerlink" title="3.He Initialization"></a>3.He Initialization</h3><p>He Initialization 在使用Relu作为非线性激活函数时具有更好的效果,因为Relu函数将所有的负数都变为0,所以W的整体的方差需要变为原来的二倍，$ \frac{2}{layersdims[l - 1]} $</p>
<h2 id="二、正则化"><a href="#二、正则化" class="headerlink" title="二、正则化"></a>二、正则化</h2><h3 id="1-什么是过拟合-overfitting"><a href="#1-什么是过拟合-overfitting" class="headerlink" title="1.什么是过拟合(overfitting)?"></a>1.什么是过拟合(overfitting)?</h3><p>过拟合就是指在我们的训练数据中存在一些噪声,而我们的模型拟合的很好将这些噪声也都给拟合进去了,使我们的分类器过于严格。这时候我们得到的参数往往就对我们的train data表现的很好,对其他数据表现的很差。</p>
<h3 id="2-L2-Regularization"><a href="#2-L2-Regularization" class="headerlink" title="2.L2 Regularization"></a>2.L2 Regularization</h3><p>L2正则化,又叫L2范式。基本格式: $ \frac{\lambda}{2}\sum_i^{m} W_i^2 $ (除2是为了求导时抵消)<br>也就是把我们所有的参数,都加入一个平方的乘法项，因为加入了平方惩罚项,所以在进行拟合时我们使得所有得到的参数都比较小,我们一般认为参数小的模型都比较简单可以适应不同的数据集,从而一定程度上避免了过拟合。<br>但是同时这里也增加了一个超参数 $ \lambda $</p>
<h3 id="3-Dropout"><a href="#3-Dropout" class="headerlink" title="3.Dropout"></a>3.Dropout</h3><p>Dropout在深度学习中是一种广泛使用的技术。在每一层中,他会以一定的概率使一些神经元停止工作,这样使得各个神经元之间不相互依赖,从而提高模型的泛化能力。<br>每一个神经元有一个将其保留下来的概率,keep_prob(每一层的概率不一定相同)，那么当你在进行每一次迭代的时候对每一层的每个神经元你都有一定的概率(1 - keep_prob)，也就是将其激活值置为0,停止的神经元对后面的前向传播和反向传播不起作用。<br><strong>注意在输入和输出层我们不需要使用dropout</strong><br>Dropout的实现:<br>  (1) 通过np.random.randn()初始化一个和A^L 一样的矩阵,并将此看为把每个神经元保留的概率<br>  (2) D = D &lt;= keep_prob<br>  (3) A = A * D (数乘,一一对应)<br>  (4) $ A = \frac{A}{keep_prob} $ 通过这一步使得loss的期望值和没有dropout时是一样的</p>
<p>与此同时,在进行反向传播时也需要做同样的操作</p>
<font color="red">使用Dropout需要注意以下几点:</font>
  Dropout是一种正则化技术
  只是在训练阶段使用dropout,在test时不要使用
  记得需要除以 keep_prob从而使得期望值不变
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"> GRADED FUNCTION: forward_propagation_with_dropout</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_with_dropout</span><span class="params">(X, parameters, keep_prob = <span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (20, 2)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (20, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (3, 20)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (3, 1)</span></span><br><span class="line"><span class="string">                    W3 -- weight matrix of shape (1, 3)</span></span><br><span class="line"><span class="string">                    b3 -- bias vector of shape (1, 1)</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A3 -- last activation value, output of the forward propagation, of shape (1,1)</span></span><br><span class="line"><span class="string">    cache -- tuple, information stored for computing the backward propagation</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># retrieve parameters</span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]</span><br><span class="line">    W3 = parameters[<span class="string">'W3'</span>]</span><br><span class="line">    b3 = parameters[<span class="string">'b3'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span></span><br><span class="line">    D1 = np.random.rand(A1.shape[<span class="number">0</span>],A1.shape[<span class="number">1</span>])    <span class="comment"># Step 1: initialize matrix D1 = np.random.rand(..., ...)</span></span><br><span class="line">    D1 = (D1 &lt;= keep_prob)                              <span class="comment"># Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span></span><br><span class="line">    A1 = A1 * D1                                   <span class="comment"># Step 3: shut down some neurons of A1</span></span><br><span class="line">    A1 = A1 / keep_prob                               <span class="comment"># Step 4: scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)</span></span><br><span class="line">    D2 = np.random.rand(A2.shape[<span class="number">0</span>],A2.shape[<span class="number">1</span>])     <span class="comment"># Step 1: initialize matrix D2 = np.random.rand(..., ...)</span></span><br><span class="line">    D2 = (D2 &lt;= keep_prob)                              <span class="comment"># Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)</span></span><br><span class="line">    A2 = A2 * D2                                    <span class="comment"># Step 3: shut down some neurons of A2</span></span><br><span class="line">    A2 = A2 / keep_prob                              <span class="comment"># Step 4: scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A3, cache</span><br></pre></td></tr></table></figure>

代码:
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: backward_propagation_with_dropout</span><br><span class="line"></span><br><span class="line">def backward_propagation_with_dropout(X, Y, cache, keep_prob):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implements the backward propagation of our baseline model to which we added dropout.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input dataset, of shape (2, number of examples)</span><br><span class="line">    Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)</span><br><span class="line">    cache -- cache output from forward_propagation_with_dropout()</span><br><span class="line">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = 1./m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)</span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    dA2 = dA2 * D2 # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span><br><span class="line">    dA2 = dA2 / keep_prob       # Step 2: Scale the value of neurons that haven&apos;t been shut down</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))</span><br><span class="line">    dW2 = 1./m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    dA1 = dA1 * D1              # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span><br><span class="line">    dA1 = dA1 / keep_prob             # Step 2: Scale the value of neurons that haven&apos;t been shut down</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))</span><br><span class="line">    dW1 = 1./m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;&quot;dZ3&quot;: dZ3, &quot;dW3&quot;: dW3, &quot;db3&quot;: db3,&quot;dA2&quot;: dA2,</span><br><span class="line">                 &quot;dZ2&quot;: dZ2, &quot;dW2&quot;: dW2, &quot;db2&quot;: db2, &quot;dA1&quot;: dA1, </span><br><span class="line">                 &quot;dZ1&quot;: dZ1, &quot;dW1&quot;: dW1, &quot;db1&quot;: db1&#125;</span><br><span class="line">    </span><br><span class="line">    return gradients</span><br></pre></td></tr></table></figure>

##三、优化
###1.Mini_batch Gradient descent

1.首先需要介绍Batch gradient descent，即完全采用全数据集的形式,由全数据集确定的方向能更好的代表样本总体,从而更准确的朝极值的方向。但是由于数据的海量增长和内存的限制,一次性载入全部的数据变得异常困难。
2.stochastic gradient descent，随机梯度下降,每次只采用一个样本进行梯度下降。这大大减少了训练时间,但是以每个样本各自的梯度方向修正,横冲直撞各自为政很难最后达到收敛。
3.这时候就产生了Mini——batch gradient descent.他是介于Batch gradient descent 和stochastic gradient descent之间的一种方法。当数据量很小时,我们直接采用Batch gradient descent ,当数据量很大时我们采用mini-batch,这时候每一个mini-batch的大小,我们称为batch_size。
其实当batch_size = m 就是batch gradient descent ,batch_size = 1 就是stochastic gradient descent.

常用的batch——size大小为: 64,128,256,512 一般为2的幂。

####为什么mini-batch有优势?
假设我们现在有100万样本,如果我们采用batch gradient descent 遍历一次所有的数据(epoch)只进行了一次迭代,如果我们将整个样本分成1000份,也就是batch_size = 1000,则会有1000个子集,然后当我们用for循环遍历这些子集时,针对每个子集进行一次梯度下降,这样当遍历完所有样本一次(epoch)我们相当于在梯度下降中进行了1000次迭代,这样就大大提高了我们算法的速度,同时因为每个子集只有1000个样本,也提高了内存的利用率。

#### mini_batch 的效果
<img src="/articles/65d241ff/2.png">
如上图，左边是full batch的梯度下降效果。 可以看到每一次迭代成本函数都呈现下降趋势，这是好的现象，说明我们w和b的设定一直再减少误差。 这样一直迭代下去我们就可以找到最优解。 右边是mini batch的梯度下降效果，可以看到它是上下波动的，成本函数的值有时高有时低,但是总体呈现下降的趋势。
这个也是正常的,<font color="red">因为我们每一次梯度下降都是在mini_batch 上跑的而不是在整个数据集上,所以数据的差异可能会导致这样的效果，可能某段数据效果特别好,m藕断数据效果不好,但是他总体还是呈现下降趋势的。</font>
<img src="/articles/65d241ff/1.png">

<font color="red">对于batch_size大小的选择，我们不能太大,因为太大会接近full batch的行为;也不能太小,太小了可能算法永远不会收敛。</font>

<h4 id="mini-batch-实现"><a href="#mini-batch-实现" class="headerlink" title="mini_batch 实现"></a>mini_batch 实现</h4><p>(1)为了避免偶然性,我们需要打乱我们的train data,通过np.random.permutation()随机打乱下标,并保存为list. 然后根据列表值重新获得矩阵.<br>(2)根据batch_size 大小进行划分.注意不能除尽的情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: random_mini_batches</span><br><span class="line"></span><br><span class="line">def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Creates a list of random minibatches from (X, Y)</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input data, of shape (input size, number of examples)</span><br><span class="line">    Y -- true &quot;label&quot; vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span><br><span class="line">    mini_batch_size -- size of the mini-batches, integer</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(seed)            # To make your &quot;random&quot; minibatches the same as ours</span><br><span class="line">    m = X.shape[1]                  # number of training examples</span><br><span class="line">    mini_batches = []</span><br><span class="line">        </span><br><span class="line">    # Step 1: Shuffle (X, Y)</span><br><span class="line">    permutation = list(np.random.permutation(m)) # 随机打乱数组,permutation 参数可以是int,返回一个list</span><br><span class="line">    shuffled_X = X[:, permutation]    # 按照permutation列表中的数字顺序作为下标重新得到一个列表,从而实现打乱矩阵的功能</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((1,m))</span><br><span class="line">    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span><br><span class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning</span><br><span class="line">    for k in range(0, num_complete_minibatches):</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        mini_batch_X = shuffled_X[:,k * mini_batch_size : (k + 1) * mini_batch_size]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:,k * mini_batch_size : (k + 1) * mini_batch_size]</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    # Handling the end case (last mini-batch &lt; mini_batch_size)</span><br><span class="line">    if m % mini_batch_size != 0:</span><br><span class="line">        ### START CODE HERE ### (approx. 2 lines)</span><br><span class="line">        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size : m]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size : m]</span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    return mini_batches</span><br></pre></td></tr></table></figure>
<h3 id="2-momentum动量"><a href="#2-momentum动量" class="headerlink" title="2.momentum动量"></a>2.momentum动量</h3><p>因为mini_bacth梯度下降使参数每次只在子集上更新自己的参数,所以它每次下降的方向会有很多抖动,也就是说梯度下降会很大幅度的变动徘徊这向最低点前进(convergence 收敛)。使用momentum可以减少相应的抖动。<br>momentum会把过去的梯度考虑在内，使得梯度变的更平滑.<br><img src="/articles/65d241ff/3.png"></p>
<h4 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h4><p>举例说明，下面是一个同学的某一科的考试成绩：<br>平时测验 80， 期中 90， 期末 95<br>学校规定的科目成绩的计算方式是：<br>平时测验占 20%；<br>期中成绩占 30%；<br>期末成绩占 50%；<br>这里，每个成绩所占的比重叫做权数或权重。那么，<br>加权平均值 = $80×0.2 + 90×0.3 + 95×0.5 = 90.5$<br>算数平均值 = (80 + 90 + 95)/3 = 88.3<br>另外一个例子,我们需要计算某地温度的移动平均值。<br>我们现在先直接给出一个公式 $V_t = \beta*V_{t-1} + (1 - \beta) \theta_t$ 其中$V_t$表示到第t天的平均温度值, $ \theta_t$ 表示第t填的温度值,$\beta$是可调节的超参数。<br>假设现在$\beta$值为0.9，那么大体就是前一日的v加上 0.1的今天的温度,对此我们可以理解为V的指数加权平均值约等于 $\frac{1}{1 - \beta} $ 也就是说大概 $\beta$ = 0.9 就是温度十天以来的加权平均值,$ \beta$ = 0.98就是50天以内的温度加权平均值.<br><img src="/articles/65d241ff/4.png"><br>通过上面的表达式我们可以知道,$V_100$ 等于每天的温度乘以一个权值,本质就是以指数形式递减加权的移动平均,各数值的加权随时间而指数式递减,越近期的数据加权越重,而我们的算数平均每一项的权值都是 $\frac{1}{n}$</p>
<h4 id="偏差修正"><a href="#偏差修正" class="headerlink" title="偏差修正"></a>偏差修正</h4><p>首先给出带偏差修正的指数加权平均公式:<br>$V_t = \beta V_{t-1} + (1-\beta)\theta_{t}$<br>$\hat(V_t) = \frac{V_t}{1-\beta^t} $<br>$ \frac{\beta V_{t-1} + (1-\beta)\theta_t}{1-\beta^t} $<br>随着t的增大,$ \beta^t $逐渐趋近于0,从而偏差修正不在起作用。<br>实际上,上面的例子在 $\beta = 0.98$时,实际上我们得到的不是绿色曲线,而是紫色曲线,因为使用指数加权平均的方法在前期会有很大的偏差,为此我们引入了偏差修正的概念。<br><img src="/articles/65d241ff/6.png"></p>
<font color="red">在机器学习中,在计算指数加权平均数的大部分时候,大家不太在乎偏差修正,大部分宁愿熬过初始阶段,拿到具有偏差的估测,然后继续计算下去.
如果你关心初始时期的偏差,修正偏差能帮助你在早期获得更好的估测</font>

<h4 id="momentum公式"><a href="#momentum公式" class="headerlink" title="momentum公式"></a>momentum公式</h4><p>以前我们在进行梯度下降时,对参数W更新, $ W = W - \alpha dw $,由于我们前面说过梯度下降会有很多抖动,所以这里我们就是在更新W的时候做些手脚。<br>$ V_{dw} = \beta V_{dw} + (1 - \beta)dw$<br>$ W = W - \alpha V_{dw}$<br>$V_{db} = \beta V_{db} + (1-beta)db $<br>$b = b - \alpha V_{db}$</p>
<p>$V_{dw},V_{db}$ 表示对w的导数dw,b的导数db求指数加权平均,然后更新参数时减去的是指数加权平均的值而不是导数的值。<br>关于 $ \beta$的值的选择,我们选择的值越大则结果越平滑,因为对过去的权重占的越多,对自己本身占的少,但是如果很大就超出我们想要的效果了。所以一般我们取 $\beta$ = 0.9</p>
<p>效果如图:<img src="/articles/65d241ff/7.png"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"> GRADED FUNCTION: update_parameters_with_momentum</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_momentum</span><span class="params">(parameters, grads, v, beta, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using Momentum</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity:</span></span><br><span class="line"><span class="string">                    v['dW' + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v['db' + str(l)] = ...</span></span><br><span class="line"><span class="string">    beta -- the momentum hyperparameter, scalar</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- python dictionary containing your updated velocities</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Momentum update for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 4 lines)</span></span><br><span class="line">        <span class="comment"># compute velocities</span></span><br><span class="line">        v[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] = beta * v[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] + (<span class="number">1.</span> - beta) * grads[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] = beta * v[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] + (<span class="number">1.</span> - beta) * grads[<span class="string">'db'</span> + str(l + <span class="number">1</span>)]</span><br><span class="line">        <span class="comment"># update parameters</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l + <span class="number">1</span>)] = parameters[<span class="string">'W'</span> + str(l + <span class="number">1</span>)] - learning_rate * v[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l + <span class="number">1</span>)] = parameters[<span class="string">'b'</span> + str(l + <span class="number">1</span>)] - learning_rate * v[<span class="string">'db'</span> + str(l + <span class="number">1</span>)]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters, v</span><br></pre></td></tr></table></figure></p>
<h3 id="3-RMSprop-均方根"><a href="#3-RMSprop-均方根" class="headerlink" title="3.RMSprop(均方根)"></a>3.RMSprop(均方根)</h3><p>RMSprop全称为 root mean square prop 均方根。他也可以用来加速梯度下降</p>
<h4 id="公式"><a href="#公式" class="headerlink" title="公式:"></a>公式:</h4><p>$ S_{dw} = \beta S_{dw} + (1 - \beta)(dw)^2 $<br>$ S_{db} = \beta S_{db} + (1 - \beta)(db)^2 $</p>
<h4 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h4><p>$ W = W - \alpha \frac{dw}{\sqrt(S_{dw} + \epsilon)}$<br>$ b = b - \alpha \frac{db}{\sqrt(S_{db} + \epsilon )}$<br>$ \epsilon $防止分母为0.一般为很小的数, $10^{-8}$</p>
<h3 id="4-Adam"><a href="#4-Adam" class="headerlink" title="4.Adam"></a>4.Adam</h3><p>Adam算法是训练神经网络最有效的优化算法之一,它结合了Momentum和RMSprop.</p>
<h4 id="公式-1"><a href="#公式-1" class="headerlink" title="公式"></a>公式</h4><p>$V_{dw} = \beta_1 V_{dw} + (1-\beta_1)dw $<br>$V_{dw}^{corrected} = \frac{V_{dw}}{(1-\beta_1^t)}$<br>$S_{dw} = \beta_2 S_{dw} + (1-\beta_2)(dw)^2$<br>$S_{dw}^{corrected} = \frac{S_{dw}}{1-\beta_2^t}$<br>$W = W - \alpha \frac{V_{dw}}{\sqrt( S_{dw} + \epsilon)}$</p>
<p>b同理,这里不再列出</p>
<p>这里我们可以看到,有了两个参数, $ \beta_1$ 和 $\beta_2$ ,通常情况下我们让<br>$ \beta_1 = 0.9  \beta_2 = 0.999   \epsilon = 10 ^{-8}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters_with_adam</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_adam</span><span class="params">(parameters, grads, v, s, t, learning_rate = <span class="number">0.01</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>,  epsilon = <span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using Adam</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></span><br><span class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span>                 <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    v_corrected = &#123;&#125;                         <span class="comment"># Initializing first moment estimate, python dictionary</span></span><br><span class="line">    s_corrected = &#123;&#125;                         <span class="comment"># Initializing second moment estimate, python dictionary</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Perform Adam update on all parameters</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        <span class="comment"># Moving average of the gradients. Inputs: "v, grads, beta1". Output: "v".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        v[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] = beta1 * v[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] + (<span class="number">1.</span> - beta1) * grads[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] = beta1 * v[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] + (<span class="number">1.</span> - beta1) * grads[<span class="string">'db'</span> + str(l + <span class="number">1</span>)]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute bias-corrected first moment estimate. Inputs: "v, beta1, t". Output: "v_corrected".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        v_corrected[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] = v[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] / (<span class="number">1.</span> - beta1**t)</span><br><span class="line">        v_corrected[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] = v[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] / (<span class="number">1.</span> - beta1**t)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Moving average of the squared gradients. Inputs: "s, grads, beta2". Output: "s".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        s[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] = beta2 * s[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] + (<span class="number">1.</span> - beta2) * grads[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)]**<span class="number">2</span></span><br><span class="line">        s[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] = beta2 * s[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] + (<span class="number">1.</span> - beta2) * grads[<span class="string">'db'</span> + str(l + <span class="number">1</span>)]**<span class="number">2</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute bias-corrected second raw moment estimate. Inputs: "s, beta2, t". Output: "s_corrected".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        s_corrected[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] = s[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] / (<span class="number">1.</span> - beta2**t)</span><br><span class="line">        s_corrected[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] = s[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] / (<span class="number">1.</span> - beta2**t)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters. Inputs: "parameters, learning_rate, v_corrected, s_corrected, epsilon". Output: "parameters".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l + <span class="number">1</span>)] = parameters[<span class="string">'W'</span> + str(l + <span class="number">1</span>)] - learning_rate * v_corrected[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] / \</span><br><span class="line">        (np.sqrt(s_corrected[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)]) + epsilon)</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l + <span class="number">1</span>)] = parameters[<span class="string">'b'</span> + str(l + <span class="number">1</span>)] - learning_rate * v_corrected[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] / \</span><br><span class="line">        (np.sqrt(s_corrected[<span class="string">'db'</span> + str(l + <span class="number">1</span>)]) + epsilon)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters, v, s</span><br></pre></td></tr></table></figure>
      
    </div>
    
    
    
<div>
  
    ﻿<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
  
</div>
<div>
    
      ﻿
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

  <!-- JS库 sweetalert 可修改路径 -->
  <script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"></script>
  <script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"></script>
  <link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css">
  <p><span>本文标题:</span><a href="/articles/65d241ff.html">deeplearning.ai 改善深度神经网络(正则化、优化、mini-batch等)</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 Statusrank 的个人博客">Statusrank</a></p>
  <p><span>CSDN博客</span><a href="https://blog.csdn.net/howardemily/">欢迎来访!</a></p>
  <p><span>发布时间:</span>2018年08月15日 - 11:08</p>
  <p><span>最后更新:</span>2018年12月20日 - 21:12</p>
  <p><span>原始链接:</span><a href="/articles/65d241ff.html" title="deeplearning.ai 改善深度神经网络(正则化、优化、mini-batch等)">https://statusrank.xyz/articles/65d241ff.html</a>
    <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://statusrank.xyz/articles/65d241ff.html" aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>
</div>
<script>
    var clipboard = new Clipboard('.fa-clipboard');
    clipboard.on('success', $(function(){
      $(".fa-clipboard").click(function(){
        swal({
          title: "",
          text: '复制成功',
          html: false,
          timer: 500,
          showConfirmButton: false
        });
      });
    }));
</script>

    
  </div>
    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>万水千山总是情,就给五毛行不行!</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/weixin.png" alt="Statusrank 微信">
        <p>微信</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/ali.png" alt="Statusrank 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        
          <div class="wp_rating">
		<div style="color: rgba(0, 0, 0, 0.75); font-size:13px; letter-spacing:3px">(&gt;看完记得五星好评哦亲&lt;)</div>
            <div id="wpac-rating"></div>
          </div>
        

        

        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/articles/875e7fed.html" rel="next" title="POJ3181 Dollar Dayz(完全背包+拆两个longlong模拟高精度)">
                <i class="fa fa-chevron-left"></i> POJ3181 Dollar Dayz(完全背包+拆两个longlong模拟高精度)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/articles/31693f5f.html" rel="prev" title="TensorFlow入门">
                TensorFlow入门 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>
 
	<h3> 相关文章：</h3><ul class="related-posts"><li><a href="/articles/e5e71180.html">深度学习----神经风格迁移(neural style transfer)</a></li><li><a href="/articles/b04eb2da.html">深度学习之图像分类-----------K最邻近算法（KNN）</a></li><li><a href="/articles/74255015.html">GoogLeNet(从Inception v1到v4的演进)</a></li><li><a href="/articles/e8d78fff.html">深度学习——--残差网络(ResNet)</a></li><li><a href="/articles/6fc928f5.html">DeepLearning.ai 卷积神经网络-目标检测</a></li><li><a href="/articles/1798e6c8.html">神经网络中的梯度消失、爆炸原因及解决办法</a></li><li><a href="/articles/206a47f4.html">deeplearning.ai-note 结构化机器学习项目</a></li><li><a href="/articles/31693f5f.html">TensorFlow入门</a></li><li><a href="/articles/a0a72429.html">卷积神经网络原理与实例</a></li><li><a href="/articles/70ac7c7c.html">深度学习之图像分类----多分类支持向量机（Multi-SVM）与softmax分类</a></li></ul> 



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            博主概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/tou.jpg" alt="Statusrank">
            
              <p class="site-author-name" itemprop="name">Statusrank</p>
              <p class="site-description motion-element" itemprop="description">退役Acmer<br>努力摆脱四非加成光环的菜鸡<br></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">103</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">54</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/statusrank" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="bao_sl@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/howardemily/" title="原CSDN博客" target="_blank">原CSDN博客</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、初始化"><span class="nav-text">一、初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-为什么神经网络的所有参数不能全部初始化为0-gt"><span class="nav-text">1.为什么神经网络的所有参数不能全部初始化为0&gt;?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Xavier-Initialization"><span class="nav-text">2.Xavier Initialization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-He-Initialization"><span class="nav-text">3.He Initialization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、正则化"><span class="nav-text">二、正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-什么是过拟合-overfitting"><span class="nav-text">1.什么是过拟合(overfitting)?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-L2-Regularization"><span class="nav-text">2.L2 Regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Dropout"><span class="nav-text">3.Dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#mini-batch-实现"><span class="nav-text">mini_batch 实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-momentum动量"><span class="nav-text">2.momentum动量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#指数加权平均"><span class="nav-text">指数加权平均</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#偏差修正"><span class="nav-text">偏差修正</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#momentum公式"><span class="nav-text">momentum公式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-RMSprop-均方根"><span class="nav-text">3.RMSprop(均方根)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#公式"><span class="nav-text">公式:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#参数更新"><span class="nav-text">参数更新</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Adam"><span class="nav-text">4.Adam</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#公式-1"><span class="nav-text">公式</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        ﻿<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<center><div class="copyright">&copy; <span itemprop="copyrightYear">2018 - 2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Statusrank</span>

  
</div>








<script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>本站总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      次
    </span>
  

  
</div>








        
      </center></div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'QCHnVkYIrh5qoSOSSISETUBC-gzGzoHsz',
        appKey: 'nPH2aXy1ll77oLHYcbJr1xpX',
        placeholder: 'Just go go',
        avatar:'/images/tou.jpg',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.4"></script>



  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("QCHnVkYIrh5qoSOSSISETUBC-gzGzoHsz", "nPH2aXy1ll77oLHYcbJr1xpX");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  
  <script type="text/javascript">
  wpac_init = window.wpac_init || [];
  wpac_init.push({widget: 'Rating', id: 16027,
    el: 'wpac-rating',
    color: 'f79533'
  });
  (function() {
    if ('WIDGETPACK_LOADED' in window) return;
    WIDGETPACK_LOADED = true;
    var mc = document.createElement('script');
    mc.type = 'text/javascript';
    mc.async = true;
    mc.src = '//embed.widgetpack.com/widget.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
  })();
  </script>


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

<!-- ����鸴�ƹ��� -->
<script type="text/javascript" src="/js/src/clipboard.min.js"></script>  
<script type="text/javascript" src="/js/src/clipboard-use.js"></script>
</body>
<!-- ҳ����С���� --> 

</html>
<script type="text/javascript" src="/js/src/love.js"></script>
