<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Wannafly挑战赛23 游戏(sg函数)]]></title>
    <url>%2F2018%2F09%2F01%2F2018-9-1%2F</url>
    <content type="text"><![CDATA[题意链接：https://www.nowcoder.com/acm/contest/161/B来源：牛客网 小N和小O在玩游戏。他们面前放了n堆石子，第i堆石子一开始有ci颗石头。他们轮流从某堆石子中取石子，不能不取。最后无法操作的人就输了这个游戏。但他们觉得这样玩太无聊了，更新了一下规则。具体是这样的：对于一堆有恰好m颗石子的石头堆，假如一个人要从这堆石子中取石子，设他要取石子数为d，那么d必须是m的约数。最后还是无法操作者输。现在小N先手。他想知道他第一步有多少种不同的必胜策略。一个策略指的是，从哪堆石子中，取走多少颗石子。只要取的那一堆不同，或取的数目不同，都算不同的策略。输入描述:第一行一个整数n。接下来一行n个整数，分别代表每堆石子的石子数目。数据保证输入的所有数字都不超过105，均大于等于1，且为整数。输出描述:一行一个整数代表小$N$第一步必胜策略的数量。示例1输入复制1047 18 9 36 10 1 13 19 29 1输出复制7 思路:典型的sg函数问题,n堆可以看成先求每堆的sg然后异或,这里不同的就是因为只能取因子,所以sg的时候按照因子个数来取。对于有多少种取法,只需要对于每一堆我们看一下取因子个数后,是否还满足sg函数必胜态的条件:sg1^sg2^sg3… == 0.也就是把必败态扔给对手,然后统计即可 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include&lt;bits/stdc++.h&gt;using namespace std;const int maxn = 1e5 + 10;typedef long long ll;int a[maxn],vis[1000],sg[maxn];int n;int getsg(int x)&#123; if(sg[x] != -1) return sg[x]; memset(vis,0,sizeof vis); for(int j = 1;j * j &lt;= x;++j) &#123; if(x % j) continue; vis[getsg(x - j)] = 1; if(x / j != j) vis[getsg(x - x / j)] = 1; &#125; for(int i = 0;i &lt;= x;++i) if(!vis[i]) return sg[x] = i; return sg[x];&#125;int main()&#123; int res = 0; cin &gt;&gt; n; memset(sg,-1,sizeof sg); sg[0] = 0; for(int i = 1;i &lt; maxn;++i) sg[i] = getsg(i); for(int i = 1;i &lt;= n;++i) &#123; scanf("%d",&amp;a[i]); res ^= sg[a[i]]; &#125; int cnt = 0; for(int i = 1;i &lt;= n;++i) &#123; res ^= sg[a[i]]; for(int j = 1;j*j &lt;= a[i];++j) &#123; if(a[i]%j) continue; if( !(res ^ sg[a[i] - j])) cnt++; //这里需要注意的是:比较运算符优先级大于逻辑运算符。 if(a[i] / j != j &amp;&amp; !(res ^ sg[a[i] - a[i] / j])) cnt++; &#125; res ^= sg[a[i]]; &#125; cout &lt;&lt; cnt &lt;&lt; endl; return 0;&#125;]]></content>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统知识点总结]]></title>
    <url>%2F2018%2F08%2F30%2F1%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[可重定位分区分配 在连续分配方式中，必须把一个系统或用户程序装入一连续的内存空间。如果在系统中只有若干个小的分区，即使它们的容量总和大于要装入的程序，但由于这些分区不相邻接，也无法把该程序装入内存。这种不能被利用的小分区称为“零头”或“碎片”。当内存中出现几个互不相邻的小分区,他们单独的容量不能满足要求,但他们的总容量和大于作业的要求,这时就可以将内存中的所有作业进行移动,使他们全部相邻接,这样就把原来的小分区拼接成大分区来满足要求。 第一种方案是在某个分区回收时立即进行拼接，这样在内存中总是只有一个连续的空闲区。但由于拼接很费时间，拼接频率过高会使系统开销加大。 第二种方案是当找不到足够大的空闲区且空闲区的总容量可以满足作业要求时进行拼接。拼接的频率比第一种要小得多，但空闲区的管理稍微复杂一些。 （需要重定位寄存器） 进程和线程的区别进程是运行时的程序,是系统进行资源调度和分配的基本单位,实现了操作系统的并发。线程是CPU调度和分配的基本单位,一个进程至少有一个线程,线程是依赖于进程存在的。进程在执行过程中拥有独立的内存单元,而多个线程共享进程的内存。 进程间通信的方式进程通信,即进程间进行数据交换。 管道管道是一种供内存共享的外存文件,主要是用于连接一个写入进程和一个读出进程以实现他们之间的数据通信。管道分为无名管道和有名管道。1.互斥问题。管道文件应看成临界资源,对管道文件的访问应该互斥的进行。当一个进程对管道读/写时,另一个进程不可以访问他。2.同步先写后读,读完才能继续写. 消息队列消息队列是消息的链接表,具有写权限的进程可以按照一定的规则向消息队列中添加信息；对消息队列有读权限的进程可以从消息队列中读取信息。 共享内存多个进程可以访问同一块内存空间,不同进程可以及时看到对方进程中对共享内存数据的更新。这种方式需要依靠某种同步操作,如互斥锁和信号量等。 信号量进程之间及同一种进程的不同线程之间取得同步和互斥的手段 套接字可以用于网络中不同机器之间的进程间通信 线程同步的方式互斥量:采用互斥对象机制,只有拥有互斥对象的线程才有访问公共资源的权限，因为互斥对象只有一个,所以可以保证公共资源不会被多个线程同时访问。信号量 Semphare：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量事件(信号)，Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作 什么是死锁？死锁产生的条件？1)死锁的概念在两个或多个并发进程中,存在这样一组进程;每个进程都持有某种资源而又等待其他进程所占有的资源,在未改变状态前这些进程都不能前进,陷入了无休止的等待。2）产生死锁的原因、产生死锁的条件互斥条件。进程请求的资源属于临界资源,每次只能允许一个进程使用不剥夺条件。进程获得某个资源后就一直占有，直到使用完后才释放请求和等待条件。允许进程在保持已有资源不释放的情况下进一步请求新的资源,若请求的资源得不到满足将会阻塞,也不释放自己所持有的资源环路条件。若干进程之间形成了一种头尾相连的环形等待资源关系. 死锁的处理基本策略和常用方法解决死锁的方法有死锁预防、死锁避免、检测死锁、解除死锁等。 死锁预防基本思想只要确保死锁发生的四个必要条件至少有一个不成立即可1)打破“请求和保持条件”一次性分配方案:在进程创建开始时将整个运行期间所需的全部资源一次性分配到手,其运行过程中不许再追加申请。这种方法简单易行,但是会造成资源浪费,大部分时间里全部分配的资源是闲置不用的,这就造成了浪费。其次,很多情况下我们无法预知一个进程执行前所需的全部资源,因为进程是动态执行的,不可预知的；同时降低了程序的并发性。2)打破不可剥夺条件一个进程不必一次性地申请全部所需资源,允许采用动态申请的方式;但是当申请的资源得不到满足时就释放自己所占有的全部资源使得其他进程使用。实现起来复杂,使系统开销增加。3）打破环路条件实行资源的有序分配。对所有资源编号,所有进程对资源的请求必须严格按照资源序号递增提出,即占有了小号资源才能申请大号资源,这样就不产生回路，预防死锁的发生。 死锁避免基本思想:动态监测资源分配状态,以确保循环等待条件不成立,从而确保系统处于安全状态。安全状态:系统是否存在一个进程序列,使得系统按此序列执行时,所有进程都能到达终点。银行家算法在运行中实现动态测试,决定是否满足用户当前的需求，系统开销大。 死锁检测死锁定理:系统状态S为死锁状态的充要条件是,S状态的资源分配图是不可化简的。 死锁消除死锁消除的方法有:撤销进程法和资源剥夺法.1）撤销进程法撤销部分死锁的进程,用释放出来的资源救活其他死锁的进程。每撤销一个进程就采用死锁检测的方法检测死锁是否解除,若不解除继续撤销进程直到死锁解除为止。2）资源剥夺法将一个阻塞进程挂起后,剥夺该进程所有占有的资源,并保存他在挂起点前的状态,在以后该进程被激活时从以前的状态处继续执行. 进程有哪些状态？就绪态、运行态、阻塞态、挂起态、退出态 分页和分段有什么区别(内存管理)? 分页将内存空间划分为一些大小相同的存储块————“帧”进程划分为与帧同样大小的块称为—————“页面”，每一页面放入一帧中，&lt;页号，页内地址&gt;页表:为每一个进程建立的表,对应记录着每一页放在哪一帧中。 分段一个进程可以由很多段组成,各个段长度并不一定相同,所以为每个段分配等长的存储空间是不现实的,所以采用动态分区分配的机制,根据各个段的长度给予分配大小不等的一些存储块,并允许进程被放在不连续的空间中 两者的不同点1.分页不存在外碎片,但是会产生内碎片。分段会产生外碎片,没有内碎片2.大小:页的大小固定且由系统决定,而段的长度不固定,由其所完成的功能决定3.地址不同:段向用户提供二维地址空间,页向用户提供一维地址空间 操作系统中进程调度策略FCFS(先来先服务，队列实现，非抢占的)：先请求CPU的进程先分配到CPUSJF(最短作业优先调度算法)：平均等待时间最短，但难以知道下一个CPU区间长度优先级调度算法(可以是抢占的，也可以是非抢占的)：优先级越高越先分配到CPU，相同优先级先到先服务，存在的主要问题是：低优先级进程无穷等待CPU，会导致无穷阻塞或饥饿；解决方案：老化时间片轮转调度算法(可抢占的)：队列中没有进程被分配超过一个时间片的CPU时间，除非它是唯一可运行的进程。如果进程的CPU区间超过了一个时间片，那么该进程就被抢占并放回就绪队列。多级队列调度算法：将就绪队列分成多个独立的队列，每个队列都有自己的调度算法，队列之间采用固定优先级抢占调度。其中，一个进程根据自身属性被永久地分配到一个队列中。多级反馈队列调度算法：与多级队列调度算法相比，其允许进程在队列之间移动：若进程使用过多CPU时间，那么它会被转移到更低的优先级队列；在较低优先级队列等待时间过长的进程会被转移到更高优先级队列，以防止饥饿发生。 什么是虚拟内存?虚拟内存的基本思想:每个进程拥有独立的地址空间,这个空间被分为大小相等的多个块,称为页(page),每个页都是一段连续的地址,这些页被映射到物理内存,但并不是所有的页必须在内存中才能运行程序。当程序引用到一部分在物理内存中的地址空间时,由硬件立刻进行必要的映射;当程序引用到一部分不再物理内存中的地址空间时,由操作系统负责将缺失的部分装入物理内存。对于进程而言,逻辑上有很大的内存空间,实际上其中一部分对应物理内存上的帧,还有一些并没有加载到内存上而是保存在磁盘中由上图可以看出虚拟内存实际上比物理内存大。当访问虚拟内存时,会访问内存管理单元(MMU)，去匹配对应的物理地址。如果虚拟内存的页并不存在于物理内存中,会产生缺页中断,从磁盘中读出放入内存,若内存满还会使用相应的页面置换算法. 页面置换算法1.先进先出FIFO。2.最近最久未使用LRU,根据使用时间到现在的长短来判断3.最少使用次数LFU,根据使用次数来判断4.最优置换算法OPT，就是要保证置换出去的是不再被使用的页，或者是在实际内存中最晚使用的算法。5.clock算法设置一个访问位 A,A = 0 表明该页最近未访问,A = 1表明该页最近访问过。当需要置换页时: 旋转一周,找到一个A = 0 的将其置换出去,若A = 1该页不置换,但需要将A置成0,直到找到一个A=0的为止。6.改进的clock算法我们可以观察到,如果置换一个修改过的页比置换一个未修改的页麻烦。因为被修改的页需要重新写回外存。所以这里我们增加一个修改位M。A=0,M=0该页最近未被访问,未修改A=0，M=1该页最近未被访问，但被修改了A=1，M=1该页最近被访问了但是没修改A=1,M=1该页最近被访问了也被修改了。当需要置换页时:第一圈:找到一个A=0,M=0的并将其置换出去。若没有这样的页,则找到一个A=0，M=1的将其置换出去；在这个过程中需要将A=1的置为A=0.循环以上直到找到满足条件的页为止 局部性原理时间:最近被访问的页在不久的将来还会被访问空间:内存中被访问的页周围的页也很可能被访问 缓冲管理外部设备和CPU之间存在速度上的差异,为了缓解差异引入缓冲技术。 引入缓冲技术的优点1.减少磁盘的驱动次数2.可以缓解I/O对缺页置换策略的干扰3.缓解CPU与外设速度不匹配的矛盾,使数据处理速度提高 缓冲池技术缓冲池:一种可以被多用户,多任务共享的缓冲区,是系统提供的一种共享结构,不归进程所有。1.缓冲池由三个队列组成:空闲缓冲队列emq。挂有全部克用的空闲缓冲块。输入队列inq。挂有装满输入数据的缓冲块。输出队列outq。挂有装满输出数据的缓冲块。 缓冲区四种工作方式收容输入,提取输入,收容输出,提取输出]]></content>
      <tags>
        <tag>各种基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++知识点总结]]></title>
    <url>%2F2018%2F08%2F29%2F1C%2F</url>
    <content type="text"><![CDATA[1.c和c++的区别是什么？ 程序 = 数据结构+算法.c语言是面向过程的,面向过程就是分析出解决问题的步骤,然后用函数将其实现,使用时依次调用. 程序= 对象+消息.c++是面向对象的,c++面向对象是将构成问题的事物分解成各个对象，每个对象完成在解决整个问题中的行为.主要特点是类，封装，继承，多态等。面向对象的语言具有更好的可扩展性。 2.const有什么用途? （1）定义一个常量,即该数据只读,发生更改会报错 （2）修饰函数返回值和函数的参数,增加程序的健壮性 （3）修饰函数定义体.(也就是说这个函数为一个只读函数,该函数不会修改任何数据成员的值) 例如: void getDescription() const; 被const修饰过的函数能被const和非const对象调用,但是const对象只能调用被const修饰过的函数定义体. 3.指针和引用的区别是是什么？ （1）引用是变量的一个别名,引用与他所引用的变量实际上代表同一个存储单元 (2) 引用相比指针对内存的占用少,节省内存空间。 （3）引用是一个绑定的关系,绑定关系是永久的,而指针可以改变 （4）常引用的值不能被修改,主要用途是函数的形参以及函数的返回引用 4.malloc/free 和 new/delete的区别是什么？或者说为什么有了malloc/free 还需要new/delete？ (1)new 自动计算要分配的存储区的大小,而malloc需要给出所需要分配的存储区的大小.且new会自动返回正确的指针类型。 （2）new 可以对分配的存储空间进行初始化，也可以创建对象并对对象进行初始化（创建是调用构造函数,释放时调用析构函数）. （3）new/delete可以重载,重载一个与类相关的new/delete运算符. 5.数据封装、继承、多态 数据封装：实现了信息隐蔽性,将数据和数据有关的操作集合封装到一起,用户不必知道其实现细节,只需要知道功能即可。 继承: 解决了软件的可重用性和可扩展性 多态性:一个名字或符号具有多种含义,通过重载来实现的. 例如:c++中存在同名函数,根据参数不同自动匹配. 6.静态成员与静态成员函数静态成员属于类,而不属于对象。无论创建多少个该类的对象,都只有一个静态数据的存储空间,静态数据的初始化不会自动进行,必须在类外初始化.。 类型名 类名:: 静态数据成员[= 常量表达式] 静态成员函数也属于类而不是某个对象。在类外调用一个公有静态成员函数不需要指明对象或指向对象的指针。方式为: 类名:: 静态公有成员函数名(参数列表)7.构造函数、析构函数、拷贝构造函数、友元函数 一个类只能由一个析构函数。但却可以有多个构造函数,即可以重载构造函数。析构函数的调用顺序和构造函数的调用顺序相反 拷贝构造函数用于创建一个新对象并从已存在的对象中依据规则复制数据。缺省的拷贝构造函数采用内存拷贝的形式，将已经存在对象的内存一个字节一个字节的拷贝到新建对象的内存当中。 友元函数没有this指针,一个类的友元可以访问该类的所有成员,包括私有和受保护的,这是基于效率的考虑 8.继承和派生 派生:首先是继承基类的成员,再修改和增加派生类具有的自己特性的成员。通过派生,派生类在基类的基础上可以获得可重用性和可扩充性,既具备基类的特征又添加了自己的功能。构造函数和析构函数是不能被继承的。 多重继承:一个派生类有两个或两个以上的基类。继承的意义:从模块的观点,继承是一种关键的可重用和可扩充技术；从类型的观点,继承的关键是动态匹配。公有派生:(1)基类的保护成员在派生类中仍然是保护成员(2)基类的公有成员在派生类中仍然是公有成员(3)基类的私有成员在派生类中成为不可访问成员私有派生:(1)基类的保护成员在派生类中仍然是私有成员(2)基类的公有成员在派生类中仍然是私有成员(3)基类的私有成员和不可访问成员在派生类中成为不可访问成员保护派生基类所有公有段和保护段成员在保护派生中成为保护段。 保护段的作用: 保护派生使得基类的一些成员,他们可以被派生类访问,但不能被其他类访问(类内访问)。也就是说除了基类和他的派生类可以看到这部分成员外,对其他类如同私有段成员函数一样。 ***基类和派生类共享基类的static成员,要求访问静态成员比用"类名::成员"显示的访问*** 派生类的对象创建的时候一定会调用构造函数初始化该类对象,且一定会先调用基类的构造函数(首先基类,其次对象成员,最后派生类),执行析构函数的顺序是相反的。访问声明仅适用于私有派生,且不能说明任何类型。类D从基类B中私有派生,则D的派生类E不能访问基类B中的任何成员,为了满足需要可以采用访问声明,使得B的几个成员可以被E访问。即在类D中重新声明从基类B中继承的几个成员的类型:基类类名:基类保护段或公有段数据成员基类类名:基类保护段或公有段成员函数9.抽象类 抽象类指的是含有纯虚函数的类，该类不能建立对象，只能声明指针和引用，用于基础类的接口声明和运行时的多态 另外，如果抽象类的某个派生类在向继承体系的根回溯过程中，并不是所有的纯虚函数都实现了，该类也是抽象类，同样不能建立对象。 虚基类防止有多个间接基类的多个实例。虚基类构造函数的调用次序虚基类的构造函数在非虚基类之前调用若同一层次中包含多个虚基类,虚基类构造函数按他们的说明顺序调用若虚基类由非虚基类派生,则遵守先调用基类构造函数再调用派生类构造函数的规则 11.虚函数虚函数是一种在基类函数中定义为virtual的函数,并在一个或多个派生类中再定义的函数。虚函数的特点是，只要定义一个基类的指针，就可以指向派生类的对象。注：无虚函数时，遵循以下规则：C++规定，定义为基类的指针，也能作指向派生类的指针使用，并可以用这个指向派生类对象的指针访问继承来的基类成员；但不能用它访问派生类的成员。 使用虚函数实现运行时的多态性的关键在于：必须通过基类指针访问这些函数。 一旦一个函数定义为虚函数，无论它传下去多少层，一直保持为虚函数. 把虚函数的再定义称为过载（overriding）而不叫重载（overloading）。 **纯虚函数：**是定义在基类中的一种只给出函数原型，而没有任何与该基类有关的定义的函数。纯虚函数使得任何派生类都必须定义自己的函数版本。否则编译报错。 纯虚函数定义的一般形式： Virtual type func_name(参数列表)=0; 虚函数要求参数列表类型返回值完全一样! 重载一般函数式,函数的返回类型和参数列表可能是不同的,仅仅要求函数名相同。 重载虚函数时,要求函数名、返回类型、参量个数、参数类型和顺序是完全相同的,若仅仅是函数名相同,C++会认为这是一般的函数重载,那么此时虚函数特性就会消失. l 含有纯虚函数的基类称为抽象基类。抽象基类又一个重要特性：抽象类不能建立对象。但是抽象基类可以有指向自己的指针，以支持运行时的多态性。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &lt;iostream&gt;#define ECHO(str) std::cout &lt;&lt; str#define ECHOLN(str) std::cout &lt;&lt; str&lt;&lt; std::endl/* run this program using the consolepauser or add your own getch, system("pause") or input loop */class Base&#123; public: virtual void who()&#123; ECHOLN("我是基类！"); &#125;&#125;;class deriv_1:public Base&#123; public: voidwho()&#123; ECHOLN("我是子类deriv_1"); &#125; &#125;;class deriv_2:public Base&#123; public: void who()&#123; ECHOLN("我是子类deriv_2"); &#125;&#125;; int main(int argc, char** argv) &#123; classBase *b,b0; classderiv_1 d1; classderiv_2 d2; b= &amp;b0; b-&gt;who(); b= &amp;d1; b-&gt;who(); b= &amp;d2; b-&gt;who(); return0;&#125; 输出： 我是基类！ 我是子类deriv_1 我是子类deriv_2]]></content>
      <tags>
        <tag>各种基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛客练习赛25 定向(桥+tarjan)]]></title>
    <url>%2F2018%2F08%2F26%2F%E5%AE%9A%E5%90%91%2F</url>
    <content type="text"><![CDATA[题意给一张无向图，你需要将边定向，使得定向后的有向图强连通。 思路难点在于如何判断结果不存在。依据连通分量的知识,可以得出结论:如果给定的无向图中存在桥的话,结果一定不存在,否则我们一定可以找出答案,且这个答案直接dfs即可。如何判断是否有桥这里因为是否有桥的判定为,子节点v low[v] &gt; 父节点dfn[u]。若不存在这样的点说明所有的子节点都存在返祖边指向其祖先,而因为我们是dfs,祖先又有边指向子节点,可想而知此时就是一个连通图.所以在从起始点dfs一下分边即可。(可知,答案不唯一) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990#include&lt;bits/stdc++.h&gt;using namespace std;typedef long long ll;const int maxn = 1e6 + 5;int n,m,cnt,cont;int dfn[maxn],low[maxn],head[maxn],fa[maxn],vis[maxn];struct Edge&#123; int to,nxt,dir,used;&#125;e[maxn &lt;&lt; 2];void init()&#123; cnt = 0,cont = 1; for(int i = 1;i &lt;= n;++i) &#123; head[i] = -1,vis[i] = 0,fa[i] = i; &#125;&#125;void add(int a,int b)&#123; e[cnt].to = b; e[cnt].nxt = head[a]; e[cnt].used = 0; head[a] = cnt++;&#125;void tarjan(int cur,int par)&#123; dfn[cur] = low[cur] = cont++; fa[cur] = par; for(int i = head[cur];~i;i = e[i].nxt) &#123; int nxt = e[i].to; if(!dfn[nxt]) &#123; tarjan(nxt,cur); low[cur] = min(low[cur],low[nxt]); &#125; else if (nxt != par)&#123; //不通过父亲访问祖先 low[cur] = min(low[cur],dfn[nxt]); &#125; &#125; return;&#125;void dfs(int cur)&#123; for(int i = head[cur];~i;i = e[i].nxt) &#123; int nxt = e[i].to; if(e[i].used) continue; e[i].used = 1,e[i^1].used = 1; // i 和 i^1 是一对反向边 e[i].dir = 1,e[i^1].dir = 0; if(vis[nxt]) continue; vis[nxt] = 1; dfs(nxt); &#125;&#125;int main()&#123; scanf("%d %d",&amp;n,&amp;m); init(); for(int i = 0;i &lt; m;++i) &#123; int a,b; scanf("%d %d",&amp;a,&amp;b); add(a,b),add(b,a); &#125; tarjan(1,1); bool flag = true; for(int i = 1;i &lt;= n;++i) &#123; int par = fa[i]; if(low[i] &gt; dfn[par]) // 存在桥一定不可能 &#123; flag = false; break; &#125; &#125; if(!flag) puts("impossible"); else &#123; vis[1] = 1; dfs(1); for(int i = 0;i &lt; cnt;i += 2) printf("%d",e[i].dir); puts(""); &#125; return 0;&#125;]]></content>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[强连通分量以及tarjan算法]]></title>
    <url>%2F2018%2F08%2F26%2F%E5%BC%BA%E8%81%94%E9%80%9A%E5%88%86%E9%87%8F%2F</url>
    <content type="text"><![CDATA[参考这里还有这里 什么是强连通分量? 1.强连通分量是相对于有向图来说的,在有向图G中,若任意两个点都可达,则称图G是强连通图。如果存在两个顶点u和v,u不能到v或者v不能到u,则称图G为强非连通图2.如果G不是一个强连通图,它的子图G2是强连通图,V属于G2,任意包含V的强连通子图也是G2的子图,那么G2就是G的极大强连通子图,也叫做强连通分量。3.所谓强连通,就是图中有两点u和v,使得能够找到有向路径从u到v以及v到u。 什么是割点和桥(割边)？ 割点和桥的定义是相对于无向图来说的。 割点: 无向连通图中,去掉一个顶点及和它相邻的所有边,图中的连通分量数增加,则该顶点为割点。(也可以理解成使原来的连通部分变得不再连通) 桥(割边):无向连通图中,去掉一条边,图中的连通分量数增加,则这条边成为桥或者割边。 割点和桥的关系 有割点不一定有桥,但是有桥一定有割点。也就是说桥一定是与割点相连的边。 如图: 什么是tarjan算法? tarjan算法,其本质是基于dfs的思想,利用dfs遍历得到生成树的访问顺序从而找出在一个强连通分量中的关键节点,从而找到整个强连通分量。 dfn[] 和low[] 首先我们需要定义两个数组,这两个数组也是tarjan的核心思想。 dfn[] ，i表示顶点的编号,dfn[i]的值表示该顶点在dfs中的遍历顺序,每访问到一个未访问过的点该点的时间戳就+1.由此在dfs中我们可以得到:子节点的dfn一定大于父节点的dfn,而且在访问一个结点后其dfn便确定了,不会再改变。 low[]，下标表示顶点编号,low的值表示该点所能到达的点的时间戳的最小值。(这个当然是需要回溯维护的) 当然因为子节点的dfn一定大于父节点,所以low值主要维护的自己所能访问的祖先的时间戳的最小值。 我们也是通过它和dfn的关系来判断是否为关键节点。 具体例子请看上面两个博客,都很不错。 根据tarjan算法,当我们遍历完所有u所能到达的子节点时,若有low[u] == dfn[u] 那么这个点就是关键节点,同时也是进入强连通分量的起始节点(我们可以知道强连通分量一定有环的)。 代码只是用来做一个参考: 如何找到强连通分量中的所有点呢？ 我们知道当遍历完u所有能到达的点再回溯到u时,那么与u无关的点已经出栈；栈顶上这些的点一定是通过u dfs入栈的,我们又说过当满足条件是u是强连通分量的入口结点,所以此时栈顶一直到u出栈，之间这些点都是属于一个强连通分量的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657void dfs(int x)&#123; dfn[x] = low[x] = ++_lock; st.push(x); for(int i = 0;i &lt; vt[x].size();++i) &#123; int v = vt[x][i]; if(!dfn[v]) &#123; dfs(v); low[x] = min(low[x],low[v]); &#125; else if(!belong[v]) &#123; low[x] = min(low[x],dfn[v]); &#125; &#125; if(low[x] == dfn[x]) //这一部分是找到强连通分量中的所有点 &#123; ++cnt; while(!st.empty()) &#123; int w = st.top(); st.pop(); belong[w] = cnt; cost[cnt] += a[w]; if(w == x) break; &#125; &#125; return ;&#125;void solve() //缩点&#123; for(int i = 1;i &lt;= n;++i) &#123; for(int j = 0;j &lt; vt[i].size();++j) &#123; int v = vt[i][j]; if(belong[i] != belong[v]) &#123; af[belong[i]].pb(belong[v]); in[belong[i]]++; &#125; &#125; &#125; ll mi = 1e18; for(int i = 1;i &lt;= cnt;++i) &#123; if(in[i] == 0) &#123; mi = min(mi,cost[i]); &#125; &#125; cout &lt;&lt; mi &lt;&lt; endl; return ;&#125; 如何利用tarjan算法来求割点或桥？ 我们重新复习下上面我们定义的两个数组,dfn[]表示该顶点在dfs遍历中是第几个被访问的,也可以理解成是时间戳。low[]表示该点所能到达的祖先的最小顺序值(上面说过主要是对祖先起作用)。 那么我们可以得出如下结论:1.割点:判断u是否为割点,只需要拿u的dfn值和其所有子节点的low[v]比较,如果low[v]&gt;= dfn[u] 则u为割点。low[v] &gt;= dfn[u] 这说明v要访问u的祖先顶点必须经过u结点,若不是根据tarjan算法,low值肯定会比dfn[u]小(因为u的祖先一定比u的dfn小)。2.桥:判断是否有桥,我们上面说过桥一定是直接和割点相连的。所以我们得出条件: low[v] &gt; dfn[u]有桥。 这里没有等号的原因: 若low[v]==dfn[u] 说明从v可以回到u,也就是说即使删了u到v的边,还存在其他路径是u，v仍然连通。 具体例子请看here]]></content>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning.ai-note 结构化机器学习项目]]></title>
    <url>%2F2018%2F08%2F24%2F%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[前言本文是吴恩达deeplearning.ai 第三部分结构化机器学习项目学习笔记,主要是在做项目时的策略总结学习这一部分可以教你少走弯路,在何时应该选择什么样的策略使得自己的项目向着最好的方向发展。 项目策略在整个深度学习的训练过程中,为了提高精确度、拟合程度，要不断的调参、更改一些算法或增加数据等等,对此我们有很多的路径可以走,那么我们如何知道应该选择什么路径才是有效的呢？ 正交化 正交化就是将深度学习的整个过程的所有需要解决的问题独立开来。针对某个方面的问题有相应的解决方法,尽量不要用一个方法去尝试解决多个方面的问题。 ![](https://i.imgur.com/zdfDnKI.jpg) 如上图,主要有四个方面的问题:拟合训练集,拟合验证集,拟合测试集,在实际使用中能够很好的泛化。最好的是一个个解决问题,先拟合训练集,然后拟合验证集,测试集,在实际中应用。如果用early stopping之类的同时影响两个问题的方法就不是很好。 指标 dev set和指标很重要，先确定下来后再根据实际情况不断修改，让他们更加符合实际应用时的场景。 单一数字评估指标 查准率decision：就是精确度。例子：猫分类器判断一些图是猫，在这些图中真的是猫的图的比例就是查准率。 查全率recall:在所有猫图片中分类器正确判断出了多少 在有多个算法和模型时,需要判断哪个更好,就需要一个指标来衡量,建议不要使用多个指标,这样就很难迅速判断。 最好的方法是将几个学习模型的指标平均,**得出一个单一的评估标准用于以后的衡量**其中有一个比较好的就是调和平均数。 **单一指标的设定并不仅仅是一个简单的量或平均值,可以是复杂的加权结果,取决于实际应用的需要。** 优化指标、满足指标 如果有几个指标不能很好的融合成单一指标,就可以用优化指标,满足指标的方法。 假设有k个指标,就指定k-1个位满足指标,意思是只要这些指标达到一个阈值就可以了,不需要特别的好；一个作为优化指标,意思是这个指标是我关注的重点指标,这个指标越高越好。 训练集、验证集、测试集 数据的划分一般有两种: 较细致的一种是训练集、验证集和测试集;还有一种划分时训练集、测试集。 划分的目的都是为了得到精确度最高、泛化能力最佳的模型。 Training set 通过设置不同的超参数、不同的模型,利用大量的训练数据来获得几个比较好的模型 Dev set 验证集的目的是**用来瞄准设定的预期目标**,不断向预期目标靠近,如果dev数据与实际应用数据差距较大,则瞄准的目标就出现了偏移,做的都是无用功。 Test set 通过训练集和验证集得到最优模型后,用测试集进一步验证所得到的模型，从而进一步确定该模型是否有较强的泛化能力。 注意: 要使验证集、测试集和以后要预测的目标来自同一分布。而训练集因为需要大量的数据所以不一定要求绝对来自同一分布。 dev set 和 test set的数据要来自同一分布,不能让这两类数据分别从不同的地方取,并且最好的是能够取一些实际预测时的数据分别放入dev 和 test. 当然,如果三个都能和实际应用时的数据来自同一分布当然最好。如果没有那么多的数据,一定要保证**验证集和测试集与实际应用测试数据来自同一分布**。因为训练集需要大量数据,可以适当用一些有较多数据的其他分布。 训练集、验证集、测试集大小 偏差与方差 在训练模型后,想要提升指标就需要进行分析是否出现了偏差、方差等问题。对于不同的问题要找出相应的解决方案。 贝叶斯最佳误差 贝叶斯误差(最佳误差）计算机能达到的最优水平,一般与人类水平相似 PS：训练集的准确度不一定要非常好,因为存在贝叶斯误差,所以达到一定理想水平就可以了。 **我们关注贝叶斯误差和人类误差主要是为了比较我们对训练集验证集测试集的错误率,来实时判断我们是需要降低方差还是降低偏差还是其他的方面** 解决偏差的方法 当训练集误差和人类误差较大时:说明这时我们遇到了偏差问题 (1)可以训练更大的模型 (2)训练时间更长,或者选用更好的优化算法以获得更好的参数模型 (3)NN结构以及超参数搜索(改变超参数) 当训练集误差和验证集误差较大时:方差问题 (1)需要更多的数据 (2)正则化 L2 or Dropout etc. (3)NN结构以及超参数搜索(改变超参数) 当训练集来自不同的分布 当出现之前说的训练集与验证集来自不同分布的情况,如果迭代后的结果发现方差过大,原因有二: 过拟合 or 来自不同分布 method: 首先将一部分train set 和 dev set 组装成新的train-dev set。 对这个新数据的结果分析: train-dev set 大部分数据属于train set,和train set 是同分布,此时train set 与train-dev set之间的误差成为方差,train-dev set 与dev set之间成为数据不匹配 data mismatch,dev error 与test error 成为验证集的过拟合程度 degree of overfitting to dev set. 然后根据结果得出是因为方差、偏差、还是数据不匹配; 若是数据不匹配： 1.进行误差分析,分析dev set 和train set,找出两个分布差异的原因 2.尝试找一些与dev set 相似的数据,也可以采用人工数据合成的方法 误差分析 对**dev set进行误差分析**，取出一些错误的例子，人工统计分析不能拟合的原因。根据分析出来的几个问题选择重点问题寻找方法解决。 如猫分类器的模型，分析出也许有很大比例的误差都是因为猫狗识别错误，就要特别针对猫狗识别来处理；如果大多数误差都是因为图片模糊，则要特别针对图像清晰度处理。 **人工统计分析很重要，有必要花这个时间！！！** 如下图： 错误标签 对于**训练集**，由于训练集数据很多,所以如果只是偶然的随机错误，一般不值得花费时间去修改。 对于测试集和验证集，和误差分析一起统计，根据比例大小决定是否需要修正。 如果确定要修正，建议： 1.一定要同时修改测试集和验证集 2.同时统计模型判断错的和没有判断出来的 总结 快速搭建一个系统：1.快速找好训练集、验证集、测试集，设定好指标。尽快建立一个学习模型(输出层、代价函数不能忘)进行迭代（宁可之后再修改）。2.再进行偏差方差分析、误差分析，找出重点问题具体分析解决，在解决的过程中注意要正交化(这才是重点) 多种学习方式的策略 迁移学习迁移学习主要用于原对象有很多数据而迁移的对象没有大量数据的情况步骤:1.首先可以仅仅改变一下神经网络的输出层或者最后几层因为对于迁移学习来说,迁移对象两者之间是有关联的。正好神经网络的前几层是低层次的学习。比如猫狗识别与放射诊断的低层次特征是相似的,都是边缘检测、线条特征等等,这就是两个学习任务的共同知识,在迁移学习中就可以省略这些层数的重新训练。2.如果训练结果不好可以逐渐训练前面的层数 train set 和 dev set来自不同分布在某种程度上就是一种迁移学习的影子。例如猫分类器中,如果因为用户的低分辨率的图片数据较少,网络上的高分辨率的图片数据很多。低分辨率的图片其实是我们的理想数据,但是因为没有那么多数据,所以拿高分辨率的图片作为训练数据；这在某种程度上就可以理解为迁移学习,只不过这两者的关系及其紧密。 多任务学习用单个神经网络训练多个问题(问题之间有关联),比如在一张图同时识别车、行人、交通信号灯等。 多任务学习的要求:1.几个问题的低层次内容必须是一致的2.每个问题都必须有大量的数据3.深度学习模型要足够大如果模型足够大,一般多任务学习会比分开单独的学习模型要好。 多任务学习和迁移学习有点类似。但是最明显的区别是迁移学习的对象一个数据很多,另一个数据很少；而多任务学习要求每个问题的数据都很多。这就使得迁移学习的使用频率更高 端到端的深度学习端到端的学习简化了学习系统,直接输入值然后就输出值。多步骤的学习可以相对需要较少的数据去训练优点: 简化了学习系统,省略了中间件。让模型自己去寻找解决方案,在某些问题上能够学习的比人设置的中间件更好。缺点:1.要求有大量的数据来训练2.忽略了一些精心设计的非常好的中间件 是否使用端到端的深度学习1.数据是否足够多2.问题的复杂度(用端到端是否可行)]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow入门]]></title>
    <url>%2F2018%2F08%2F16%2FTensorFlow%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[本文部分转载自该大佬 张量(Tensor)Tensorflow内部的计算都是基于张量的,因此我们有必要先对张量有个认识。张量是在我们熟悉的标量、向量之上定义的,详细的定义很复杂,我们可以先简单的将其理解为多维数组。12343 # 这个 0 阶张量就是标量，shape=[][1., 2., 3.] # 这个 1 阶张量就是向量，shape=[3][[1., 2., 3.], [4., 5., 6.]] # 这个 2 阶张量就是二维数组，shape=[2, 3][[[1., 2., 3.]], [[7., 8., 9.]]] # 这个 3 阶张量就是三维数组，shape=[2, 1, 3] TensorFlow内部使用tf.Tensor类的实例来表示张量,每个tf.Tensor具有两个属性:dtype Tensor存储的数据类型,可以为tf.float32,tf.int32,tf.string。。。shape Tensor存储的多维数组中每个维度的数组元素的个数.我们现在可以敲几行代码看一下 Tensor 。在命令终端输入 python 或者 python3 启动一个 Python 会话，然后输入下面的代码：123456789101112131415161718192021# 引入 tensorflow 模块import tensorflow as tf# 创建一个整型常量，即 0 阶 Tensort0 = tf.constant(3, dtype=tf.int32)# 创建一个浮点数的一维数组，即 1 阶 Tensort1 = tf.constant([3., 4.1, 5.2], dtype=tf.float32)# 创建一个字符串的2x2数组，即 2 阶 Tensort2 = tf.constant([['Apple', 'Orange'], ['Potato', 'Tomato']], dtype=tf.string)# 创建一个 2x3x1 数组，即 3 阶张量，数据类型默认为整型t3 = tf.constant([[[5], [6], [7]], [[4], [3], [2]]])# 打印上面创建的几个 Tensorprint(t0)print(t1)print(t2)print(t3) 注意下面代码输出的shape类型:12345678&gt;&gt;&gt; print(t0)Tensor("Const:0", shape=(), dtype=int32)&gt;&gt;&gt; print(t1)Tensor("Const_1:0", shape=(3,), dtype=float32)&gt;&gt;&gt; print(t2)Tensor("Const_2:0", shape=(2, 2), dtype=string)&gt;&gt;&gt; print(t3)Tensor("Const_3:0", shape=(2, 3, 1), dtype=int32) print一个Tensor只能打印出它的属性定义,并不能打印出他的值,要想查看一个Tensor中的值还需要经过Session运行一下 1234567891011121314151617&gt;&gt;&gt; sess = tf.Session()&gt;&gt;&gt; print(sess.run(t0))3&gt;&gt;&gt; print(sess.run(t1))[ 3. 4.0999999 5.19999981]&gt;&gt;&gt; print(sess.run(t2))[[b'Apple' b'Orange'] [b'Potato' b'Tomato']]&gt;&gt;&gt; print(sess.run(t3))[[[5] [6] [7]] [[4] [3] [2]]]&gt;&gt;&gt; 数据流图(Dataflow Graph)数据流是一种常见的并行计算编程模型,数据流图是由结点(nodes)和线(edges)构成的有向图:节点(nodes) 表示计算单元，也可以是输入的起点或者输出的终点线(edges) 表示节点之间的输入/输出关系.在TensorFlow中每个结点都是用tf.Tensor的实例来表示的,即每个节点的输入、输出都是Tensor,如图,Tensor在Graph中流动,形象的展示TensorFlow的由来 Session我们在Python中需要做一些计算操作时一般会使用NumPy，NumPy在做矩阵操作等复杂的计算的时候会使用其他语言(C/C++)来实现这些计算逻辑，来保证计算的高效性。但是频繁的在多个编程语言间切换也会有一定的耗时，如果只是单机操作这些耗时可能会忽略不计，但是如果在分布式并行计算中，计算操作可能分布在不同的CPU、GPU甚至不同的机器中，这些耗时可能会比较严重。TensorFlow 底层是使用C++实现，这样可以保证计算效率，并使用 tf.Session类来连接客户端程序与C++运行时。上层的Python、Java等代码用来设计、定义模型，构建的Graph，最后通过tf.Session.run()方法传递给底层执行。 构建计算图Tensor 可以表示输入、输出的端点,还可以表示计算单元,如下的代码创建了对两个Tensor执行+操作:1234567891011import tensorflow as tf# 创建两个常量节点node1 = tf.constant(3.2)node2 = tf.constant(4.8)# 创建一个 adder 节点，对上面两个节点执行 + 操作adder = node1 + node2# 打印一下 adder 节点print(adder)# 打印 adder 运行后的结果sess = tf.Session()print(sess.run(adder)) 输出为:12Tensor("add:0", shape=(), dtype=float32)8.0 上面使用的tf.constant()创建的Tensor都是常量,一旦创建后其中的值就不能在改变了.但是有时我们还会需要从外部输入数据,这时可以用tf.placeholder创建占位的Tensor,占位Tensor的值可以在运行的时候输入。例如:1234567891011121314import tensorflow as tf# 创建两个占位 Tensor 节点a = tf.placeholder(tf.float32)b = tf.placeholder(tf.float32)# 创建一个 adder 节点，对上面两个节点执行 + 操作adder_node = a + b# 打印三个节点print(a)print(b)print(adder)# 运行一下，后面的 dict 参数是为占位 Tensor 提供输入数据sess = tf.Session()print(sess.run(adder, &#123;a: 3, b: 4.5&#125;))print(sess.run(adder, &#123;a: [1, 3], b: [2, 4]&#125;)) 运行结果:12345Tensor("Placeholder:0", dtype=float32)Tensor("Placeholder_1:0", dtype=float32)Tensor("add:0", dtype=float32)7.5[ 3. 7.] TensorFlow 应用实例下面通过一个例子来了解下TensorFlow。 建立模型如下为我们进行某项实验获得的一些实验数据： 输入 输出1 4.82 8.53 10.46 218 25.3我们将这些数据放到一个二维图上可以看的更直观一些，如下，这些数据在图中表现为一些离散的点：我们需要根据现有的这些数据归纳出一个通用模型，通过这个模型我们可以预测其他的输入值产生的输出值。假设我们现在选择一个线性模型来拟合这些数据。如果用 x 表示输入， y 表示输出，线性模型可以用下面的方程表示： $ y = Wx + b$即使我们选择了直线模型，可以选择的模型也会有很多，如下图的三条直线都像是一种比较合理的模型，只是W和b参数不同。这时我们需要设计一个损失模型(loss model)，来评估一下哪个模型更合理一些，并找到一个最准确的模型。 如下图每条黄线代表线性模型计算出来的值与实际输出值之间的差值:我们用y′表示实验得到的实际输出，用下面的方程表示我们的损失模型：$ loss = \sum_{n = 1}^N (y_n - y_n’)^2 $显然，损失模型里得到的loss越小，说明我们的线性模型越准确。 使用Tensorflow实现模型在我们设计的模型$ y = Wx + b$中 x是我们输入的值,所以可以使用tf.placeholder 来实现。输出y可以用线性模型的输出表示,我们需要不断的改变W和b的值,来找到一个使loss最小的值。这里W和b可以用变量tf.Variable()来表示。 123456789101112131415import tensorflow as tf# 创建变量 W 和 b 节点，并设置初始值W = tf.Variable([.1], dtype=tf.float32)b = tf.Variable([-.1], dtype=tf.float32)# 创建 x 节点，用来输入实验中的输入数据x = tf.placeholder(tf.float32)# 创建线性模型linear_model = W*x + b# 创建 y 节点，用来输入实验中得到的输出数据，用于损失模型计算y = tf.placeholder(tf.float32)# 创建损失模型loss = tf.reduce_sum(tf.square(linear_model - y))# 创建 Session 用来计算模型sess = tf.Session() 通过tf.Variable()创建变量Tensor时需要设置一个初始值,这个初始值不能立即使用必须通过init过程进行初始化。 123# 初始化变量init = tf.global_variables_initializer()sess.run(init) 这之后再使用print(sess.run(W))打印就可以看到我们之前赋的初始值1[0.1] 变量初始化完之后，我们可以先用上面对W和b设置的初始值0.1和-0.1运行一下我们的线性模型看看结果：1print(sess.run(linear_model, &#123;x: [1, 2, 3, 6, 8]&#125;)) 输出结果为：1[ 0. 0.1 0.20000002 0.5 0.69999999] 使用Tensorflow训练模型Tensorflow提供了很多优化算法来帮助我们训练模型,最简单的就是Gradient Descent.123456789101112#创建一个梯度下降优化器,学习率为0.001optimizer = tf.train.GradientDescentOptimizer(0.001)train = optimizer.minimize(loss)# 用两个数组保存训练数据x_train = [1, 2, 3, 6, 8]y_train = [4.8, 8.5, 10.4, 21.0, 25.3]# 训练10000次for i in range(10000): sess.run(train, &#123;x: x_train, y: y_train&#125;)# 打印一下训练后的结果print('W: %s b: %s loss: %s' % (sess.run(W), sess.run(b), sess.run(loss, &#123;x: x_train , y: y_train&#125;))) 打印出来的训练结果如下，可以看到损失值已经很小了：1W: [ 2.98236108] b: [ 2.07054377] loss: 2.12941]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning.ai 改善深度神经网络(正则化、优化、mini-batch等)]]></title>
    <url>%2F2018%2F08%2F15%2F%E6%94%B9%E5%96%84%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[作业地址可查看github 一、初始化 1.为什么神经网络的所有参数不能全部初始化为0>? 若w初始化为0 ,很可能导致模型失效,无法收敛。也就是说如果我们初始将所有的w初始化为0,那么进行前向传播时每一层得到的值都是一样,这样一来当我们使用反向传播时,传回的梯度也是一样的,这就导致了我们更新参数后w还是一样的,这就使得我们的NN不能各自学习到很好的特征了。[可以看这里](https://zhuanlan.zhihu.com/p/27190255) 2.Xavier Initialization Xavier Initialization 初始化的基本思想就是保持输入和输出的方差一致,这样就避免了所有的输出值趋向于0. 首先对于前向传播,我们需要确保所有层的激活值方差近似相等,因此每个训练样本传播经过网络的信息才能保持平滑的属性。同样对于反向传播,每层梯度保持近似的方差将允许信息平滑地反向流动以更新权重。近似方差的梯度同样确保误差数据能够反馈到所有层级,因此它是整个训练中的关键。 [这位大佬写的很不错](https://www.jiqizhixin.com/articles/2018-01-08-3) 将W的方差变为 $ \frac{1}{layersdims[l - 1]} $ 3.He Initialization He Initialization 在使用Relu作为非线性激活函数时具有更好的效果,因为Relu函数将所有的负数都变为0,所以W的整体的方差需要变为原来的二倍，$ \\frac{2}{layersdims[l - 1]} $ 二、正则化 1.什么是过拟合(overfitting)? 过拟合就是指在我们的训练数据中存在一些噪声,而我们的模型拟合的很好将这些噪声也都给拟合进去了,使我们的分类器过于严格。这时候我们得到的参数往往就对我们的train data表现的很好,对其他数据表现的很差。 2.L2 Regularization L2正则化,又叫L2范式。基本格式: $ \frac{\lambda}{2}\sum_i^{m} W_i^2 $ (除2是为了求导时抵消) 也就是把我们所有的参数,都加入一个平方的乘法项，因为加入了平方惩罚项,所以在进行拟合时我们使得所有得到的参数都比较小,我们一般认为参数小的模型都比较简单可以适应不同的数据集,从而一定程度上避免了过拟合。 但是同时这里也增加了一个超参数 $ \lambda $ 3.Dropout Dropout在深度学习中是一种广泛使用的技术。在每一层中,他会以一定的概率使一些神经元停止工作,这样使得各个神经元之间不相互依赖,从而提高模型的泛化能力。 每一个神经元有一个将其保留下来的概率,keep_prob(每一层的概率不一定相同)，那么当你在进行每一次迭代的时候对每一层的每个神经元你都有一定的概率(1 - keep_prob)，也就是将其激活值置为0,停止的神经元对后面的前向传播和反向传播不起作用。 注意在输入和输出层我们不需要使用dropout Dropout的实现: (1) 通过np.random.randn()初始化一个和A^L 一样的矩阵,并将此看为把每个神经元保留的概率 (2) D = (D]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[POJ3181 Dollar Dayz(完全背包+拆两个longlong模拟高精度)]]></title>
    <url>%2F2018%2F08%2F13%2F2018-8-13%2F</url>
    <content type="text"><![CDATA[[题意](https://vjudge.net/problem/POJ-3181) 大体就是一个整数划分问题,用1~k的数去正好凑出n,问你有几种求法。 思路:可以用完全背包来做,不过这里发现值很大,大约33位,所以可以拆成两个longlong,一个表示高位一个表示低位.dp[j] += dp[j - i]表示的就是在已经凑出的,dp[j-i]中添加i从而达到j状态。这样可以做到不重不漏. 12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;#include&lt;string.h&gt;using namespace std;typedef long long ll;const long long inf=1000000000000000000LL;const int maxn = 1e2 + 5;ll gao[10 * maxn];ll di[10 * maxn];int main()&#123; int n,k; while(scanf("%d %d",&amp;n,&amp;k) != EOF) &#123; memset(gao,0,sizeof gao); memset(di,0,sizeof di); di[0] = 1; for(int i = 1;i &lt;= k;++i) &#123; for(int j = i;j &lt;= n;++j) &#123; gao[j] = gao[j] + gao[j - i] + (di[j] + di[j - i]) / inf; di[j] = (di[j] + di[j - i]) % inf; &#125; &#125; if(gao[n] == 0) printf("%lld\n",di[n]); else printf("%lld%018lld\n",gao[n],di[n]); &#125; return 0;&#125;]]></content>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdu6375&&2018百度之星 度度熊学队列(模拟双向列表)]]></title>
    <url>%2F2018%2F08%2F12%2F2018-8-12-3%2F</url>
    <content type="text"><![CDATA[题意 思路大佬们都有很多思路,什么deque清内存,什么LCT的。我只会大力出奇迹,但是记得delete的时候因为没有将指向该结点的指针置位NULL,RE了一下午。。。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135#include&lt;bits/stdc++.h&gt;using namespace std;void read(int &amp;x)&#123; char ch = getchar();x = 0; for (; ch &lt; '0' || ch &gt; '9'; ch = getchar()); for (; ch &gt;='0' &amp;&amp; ch &lt;= '9'; ch = getchar()) x = x * 10 + ch - '0';&#125;const int maxn = 150000 + 1000;int n,q;struct List&#123; struct List *pre,*next; int val; struct List* head,*tail; List() &#123; pre = next = head = tail = NULL; &#125;&#125;s[maxn];int main()&#123; while(~scanf("%d %d",&amp;n,&amp;q)) &#123; for(int i = 1;i &lt; maxn;++i) s[i] = List(); int u,v,w,val,op; for(int i = 1;i &lt;= q;++i) &#123; read(op),read(u); //puts("++++"); //cout &lt;&lt; op &lt;&lt; ' ' &lt;&lt; u &lt;&lt; endl; if(op == 1) &#123; read(w),read(val); List *p = new List(); p -&gt; val = val; if(s[u].head == NULL || s[u].tail == NULL) &#123;s[u].head = p,s[u].tail = p;&#125; else&#123; if(w == 0) &#123; s[u].head -&gt; pre = p; p -&gt; next = s[u].head; s[u].head = p; &#125; else &#123; s[u].tail -&gt; next = p; p -&gt; pre = s[u].tail; s[u].tail = p; &#125; &#125; &#125; else if(op == 2) &#123; read(w); if(s[u].head == NULL || s[u].tail == NULL) puts("-1"); else &#123; if(w == 0)&#123; int x = s[u].head -&gt; val; if(s[u].head == s[u].tail) &#123; s[u].head = s[u].tail = NULL; &#125; else &#123; List *q = s[u].head; s[u].head = s[u].head -&gt; next; s[u].head -&gt; pre = NULL; delete q; &#125; printf("%d\n",x); &#125; else &#123; int x = s[u].tail -&gt; val; if(s[u].head == s[u].tail) s[u].tail = s[u].head = NULL; else &#123; List *q = s[u].tail; s[u].tail = s[u].tail -&gt; pre; s[u].tail -&gt; next = NULL; delete q; &#125; printf("%d\n",x); &#125; &#125; &#125; else &#123; read(v),read(w); List *vhead = s[v].head,* vtail = s[v].tail; if(vhead == NULL || vtail == NULL) continue; if(w == 0) &#123; if(s[u].tail == NULL || s[u].head == NULL )&#123; s[u].head = s[v].head,s[u].tail = s[v].tail; &#125; else &#123; s[u].tail -&gt; next = s[v].head; s[v].head -&gt; pre = s[u].tail; s[u].tail = s[v].tail; &#125; &#125; else &#123; while(vtail != NULL) &#123; swap(vtail -&gt; pre,vtail -&gt; next); //printf("%d\n",vtail -&gt; val); vtail = vtail -&gt; next; //puts("+++"); &#125; if(s[u].tail == NULL || s[u].head == NULL ) &#123; s[u].tail = s[v].head; s[u].head = s[v].tail; &#125; else &#123; s[u].tail -&gt; next = s[v].tail; s[v].tail -&gt; pre = s[u].tail; s[u].tail = s[v].head; &#125; &#125; s[v].head = s[v].tail = NULL; &#125; &#125; &#125; return 0;&#125;]]></content>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdu 6376&& 2018百度之星 度度熊剪纸条(思维)]]></title>
    <url>%2F2018%2F08%2F12%2F2018-8-12-2%2F</url>
    <content type="text"><![CDATA[[题意](http://acm.hdu.edu.cn/showproblem.php?pid=6376) 思路 首先想到要想1最多,我们肯定会优先切最多的连续的1,而一段1必须切两次,左一次右一次,记为[1,1].然而存在一些特殊情况,如果开头有连续的1,那么我们只需要切[0,1].如果末尾有连续的1,我们只需要切[1,0]次。那么我们就可以预处理出所有的连续1的段,按他们的个数排序然后贪心的来找就好了。但是存在几个情况需要考虑下,对于末尾的1,我们不要求它全是1,也就是说他的后面可以有0,也就是说我们可以把[x,1]的1修改一次,即让他构成末尾串，(注意这个修改要考虑开头串和中间串) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118#include&lt;bits/stdc++.h&gt;using namespace std;const int maxn = 1e4 + 5;struct node&#123; int num; int x,y;&#125;s[maxn];int cmp(node a,node b)&#123; if(a.num != b.num) return a.num &gt; b.num; else return a.x + a.y &lt; b.x + b.y;&#125;int k,n;int main()&#123; while(scanf("%d %d",&amp;n,&amp;k) != EOF) &#123; string str; cin &gt;&gt; str; int cnt = 0,num = 0; int beg = -1,sta_num = 0,en_num = 0; for(int i = 0;i &lt; str.size();++i) &#123; if(str[i] == '1') &#123; if(beg == -1) beg = i; num++; &#125; else &#123; if(str[i] == '0') &#123; if(num == 0) continue; else &#123; if(beg == 0) &#123; s[cnt].num = num; sta_num = num,num = 0; beg = -1; s[cnt].x = 0,s[cnt].y = 1; cnt++; &#125; else &#123; s[cnt].num = num,num = 0; beg = -1; s[cnt].x = s[cnt].y = 1; cnt++; &#125; &#125; &#125; &#125; // cout &lt;&lt; i &lt;&lt; ' ' &lt;&lt; beg &lt;&lt; endl; &#125; if(beg != -1) &#123; if(beg == 0) s[cnt].x = 0,s[cnt].y = 0; else s[cnt].x = 1,s[cnt].y = 0; s[cnt++].num = num,en_num = num,num = 0; &#125; //cout &lt;&lt; cnt &lt;&lt; endl; sort(s,s + cnt,cmp); int sum = 0,o = k; int cur = 0; while(s[cur].y == 0 &amp;&amp; cur &lt; cnt) cur++; s[cur].y = 0; //cout &lt;&lt; cur &lt;&lt; endl; //for(int i = 0;i &lt; cnt;++i) //cout &lt;&lt; s[i].num &lt;&lt; ' ' &lt;&lt; s[i].x &lt;&lt; ' ' &lt;&lt; s[i].y &lt;&lt; endl; for(int i = 0;i &lt; cnt;++i) &#123; if(k &gt;= s[i].x + s[i].y) &#123; sum += s[i].num; k -= (s[i].x + s[i].y); &#125; &#125; //printf("%d\n",sum); s[cur].y = 1; int ma = sum; for(int i = 0;i &lt; cnt;++i) &#123; if(s[i].x == 0 &amp;&amp; s[i].y == 1) &#123; s[i].y = 0; break; &#125; &#125; sum = 0; k = o; for(int i = 0;i &lt; cnt;++i) &#123; if(k &gt;= s[i].x + s[i].y) &#123; sum += s[i].num; k -= (s[i].x + s[i].y); &#125; &#125; //puts("----"); // for(int i = 0;i &lt; cnt;++i) //cout &lt;&lt; s[i].num &lt;&lt; ' ' &lt;&lt; s[i].x &lt;&lt; ' ' &lt;&lt; s[i].y &lt;&lt; endl; cout &lt;&lt; max(sum,ma) &lt;&lt; endl; &#125; return 0;&#125;/*12 1110111010111*/]]></content>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdu 6383&&2018百度之星 p1m2 二分]]></title>
    <url>%2F2018%2F08%2F12%2F2018-8-12-1%2F</url>
    <content type="text"><![CDATA[[题意](http://acm.hdu.edu.cn/showproblem.php?pid=6383) 思路 很水的一题啊…菜。最小问题最大化，立马想到二分,卡在了怎么验证，稍微一想就明白了,-2 + 1,每次都是会使整个数列和-1,可以证明给定一个数组（非零）,一定存在稳定数组。且答案具有单调性,当最小值为x的满足,则x-1也满足。对于验证:二分最小值x,对于$a_i &gt; x$ 则需要进行 $ \lfloor \frac{a_i - x}{2} \rfloor$ 次,$a_i &lt;= x$ 则需要加 $ x-a_i$次。如果减的次数大于等于加的次数则一定满足条件 具体为什么满足减&gt;加即可,这个也是可以被严格的证明的,但是我不太会,可以自己去网上了解一下,我只是大概一想是这样的,而且造不出反例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include&lt;bits/stdc++.h&gt;using namespace std;typedef long long ll;const int maxn = 3e5 + 10;int a[maxn],n;int check(int x)&#123; ll jian = 0,jia = 0; for(int i = 1;i &lt;= n;++i) &#123; if(a[i] &gt;= x) &#123; int cha = a[i] - x; if(cha % 2 == 0) jian += cha / 2; else jian += (cha - 1) / 2; &#125; else &#123; int cha = x - a[i]; jia += cha; &#125; &#125; return jian &gt;= jia;&#125;int main()&#123; int _; cin &gt;&gt; _; while(_--) &#123; scanf("%d",&amp;n); int l = 0,r = 0,mid; for(int i = 1;i &lt;= n;++i) scanf("%d",&amp;a[i]),r = max(r,a[i]); while(l &lt;= r) &#123; int mid = l + r &gt;&gt; 1; //cout &lt;&lt; mid &lt;&lt; endl; if(check(mid)) &#123; l = mid + 1; &#125; else r = mid - 1; &#125; printf("%d\n",r); &#125; return 0;&#125;]]></content>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络原理与实例]]></title>
    <url>%2F2018%2F08%2F06%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[本文学习自雪饼大佬,感谢!其余大部分内容学习自吴恩达深度学习课程 1.什么是神经网络？ 这里的神经网络也指人工神经网络（Artificial Neural Networks，简称ANNs），是一种模仿生物神经网络行为特征的算法数学模型，由神经元、节点与节点之间的连接（突触）所构成，如下图： 每个神经网络单元抽象出来的数学模型如下，也叫感知器，它接收多个输入（x1，x2，x3…），产生一个输出，这就好比是神经末梢感受各种外部环境的变化（外部刺激），然后产 生电信号，以便于转导到神经细胞（又叫神经元）。对应公式如下:$ h{W,b}(x) = f(W^Tx) = f(\sum{i = 1}^3 W_ix_i + b)$单个的感知器就构成了一个简单的模型，但在现实世界中，实际的决策模型则要复杂得多，往往是由多个感知器组成的多层网络，如下图所示，这也是经典的神经网络模型，由输入层、隐含层、输出层构成。人工神经网络可以映射任意复杂的非线性关系，具有很强的鲁棒性、记忆能力、自学习等能力，在分类、预测、模式识别等方面有着广泛的应用。 2.什么是卷积神经网络? 受Hubel和Wiesel对猫视觉皮层电生理研究启发，有人提出卷积神经网络（CNN），Yann Lecun 最早将CNN用于手写数字识别并一直保持了其在该问题的霸主地位。近年来卷积神经网络在多个方向持续发力，在语音识别、人脸识别、通用物体识别、运动分析、自然语言处理甚至脑电波分析方面均有突破。 卷积神经网络最主要的三层:卷积层、池化层、全连接神经网络 卷积神经网络的基本结构图: 卷积神经网络与普通的神经网络的区别在于,卷积神经网络包含了一个由卷积层和池化层构成的特征抽取器。在卷积神经网络的卷积层中通常包含若干个特征平面,每个特征平面由一些矩形排列的的神经元组成，同一特征平面的神经元共享权值，这里共享的权值就是卷积核。卷积核一般以随机小数矩阵的形式初始化,在网络的训练过程中卷积核将学习到合理的权值。通常采用卷积核的大小为3*3,也有5*5,7*7，但大多数为奇数.卷积核带来的好处是减少了网络各层之间的连接,参数减少，又降低了过拟合的风险。池化层也叫子采样,通常有max-pooling 和 average-pooling ,但通常我们使用max-pooling.卷积和池化大大简化了模型复杂度,减少了参数。 1)什么是卷积？ 当给定了一张新图时,CNN并不能准确的知道这些特征到底要匹配原图的哪些部分,所以它会在原图中把每一个可能的位置都进行尝试,相当于把这个特征(feature)变成了一个过滤器。这个用来匹配的过程就称为卷积操作。(特别注意,在卷积神经网络中进行的卷积操作其实本质上并不是严格的卷积,许多数学家把他们叫做互相关,但是在许多DL文献中我们把他看成卷积,真正的卷积是需要进行水平垂直的镜像) 卷积操作如图: 说白了要计算一个feature和其在原图上对应的某一小块的结果,只需要将两个小块对应位置的像素值进行乘法运算,然后将整个小块内的乘法运算的结果累加起来。(也可能还会有其余的操作,比如除以像素点总个数啊等等)。 如下是一个6x6的灰度图像，构造一个3x3的矩阵，在卷积神经网络中通常称之为filter，对这个6x6的图像进行卷积运算，以左上角的-5计算为例 其它的以此类推，让过滤器在图像上逐步滑动，对整个图像进行卷积计算得到一幅4x4的图像。 卷积步长 卷积步长是指过滤器在图像上滑动的距离，前两部分步长都默认为1，如果卷积步长为2，卷积运算过程为： 2)卷积是如何提取特征的？ 边界检测实例 假如你有一张如下的图像，你想让计算机搞清楚图像上有什么物体，你可以做的事情是检测图像的垂直边缘和水平边缘。 为什么这种卷积计算可以得到图像的边缘，下图0表示图像暗色区域，10为图像比较亮的区域，同样用一个3x3过滤器，对图像进行卷积，得到的图像中间亮，两边暗，亮色区域就对应图像边缘。 注意:这里由于我们图片大小仅仅是6x6所以可以发现中间的亮色区域是很宽的,当我们的图很大的时候是不会发生这种情况的。 通过以下的水平过滤器和垂直过滤器，可以实现图像水平和垂直边缘检测。 padding 在上部分中，通过一个3x3的过滤器来对6x6的图像进行卷积，得到了一幅4x4的图像，假设输入图像大小nxn，过滤器filter大小为fxf，步长为s,那么输出图像大小为 ($ \lfloor\frac{n - f}{s}\rfloor + 1 $) x ($\lfloor\frac{n - f}{s}\rfloor + 1 $) 这样做卷积运算的缺点是卷积图像的大小会不断缩小,另外图像的左上角元素只被一个输出所使用,而下图中红色阴影部分的像素却被多个输出使用了,所以图像的边缘像素在输出中采用较少,这也就意味着你会丢掉许多图像边缘信息,为了引入这两个问题我们引入padding操作,也就是在图像做卷积操作之前,沿着图像边缘用0进行填充对于3x3的过滤器，我们填充宽度为1时，就可以保证输出图像和输入图像一样大。如下图 padding的两种模式: valid: no padding same padding: 输入图像和输出图像大小一样大. 这里我们用p来表示我们填充的宽度,则使用padding后我们输出图像的大小关系为: ($ \lfloor\frac{n + 2p - f}{s} \rfloor + 1$) x ($ \lfloor\frac{n + 2p - f}{s} \rfloor + 1$) 3)彩色图像的卷积 以上我们所说的卷积都是灰度值图像的,也就是一维的。我们知道彩色图像在计算机中都是按照RGB来存的,所以如果我们想要在彩色图像上进行卷一那么过滤器的大小就不能使$3x3$ 而应该是$3x3x3$（RGB三通道,通道数）卷积生成图像中每个像素值为 $3*3*3$ 过滤器对应位置和图像对应位置相乘累加,过滤器依次再RGB图像上滑动,最终生成图像大小为$4x4$ 另外一个问题是，如果我们在不仅仅在图像总检测一种类型的特征，而是要同时检测垂直边缘、水平边缘、45度边缘等等，也就是多个过滤器的问题。如果有两个过滤器，最终生成图像为$4x4x2$的立方体，这里的2来源于我们采用了两个过滤器。如果有10个过滤器那么输出图像就是$4x4x10$的立方体。 4)池化层Pooling 为了有效减少计算,CNN使用另一个有效的工具被称为"池化"。池化就是将输入图像进行缩小,减少像素信息,保留重要的信息。 Max-pooling最大值池化 最大池化思想很简单，以下图为例，把4x4的图像分割成4个不同的区域，然后输出每个区域的最大值，这就是最大池化所做的事情。其实这里我们选择了2*2的过滤器，步长为2。在一幅真正的图像中提取最大值可能意味着提取了某些特定特征，比如垂直边缘、一只眼睛等等。 以下是一个过滤器大小为3*3，步长为1的池化过程，具体计算和上面相同，最大池化中输出图像的大小计算方式和卷积网络中计算方法一致，如果有多个通道需要做池化操作，那么就分通道计算池化操作。 最大池化（max-pooling）因为对于每一个filter,它其实就是一个特征提取器,专门用来提取某种特征,所以对于特定区域内,最后的结果越大表示匹配的越好。也就是说，它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。 average polling 平均池化和最大池化唯一的不同是，它计算的是区域内的平均值而最大池化计算的是最大值。在日常应用使用最多的还是最大池化。 池化的超参数:filter的大小,步长,池化的类型. 虽然这里池化层的参数我们也叫超参数,但是一般情况下她其实就是固定的,是不需要CNN进行学习的,一般我们采用最大值池化,过滤器大小为2,步长为2. 5)激活函数Relu 上面介绍卷积神经网络CNN结构时用的那个图大家可以看到,卷积层我们一般用CONV来表示,RELU就是我们激活函数的一种,POOL就是我们的池化层，FC就是全连接神经网络。这里需要注意的是池化层的多少没有很明确的规定,需要我们自己进行交叉验证或者调参来决定。而每一个卷积层后都是要跟着激活函数的RELU的。 常用的激活函数有sigmod,tanh,relu等，其中sigmod,tanh主要用于全连接fc层,而Relu主要用于卷积层。回顾一下我们前面讲的神经网络中,单个神经元在接受到和输入计算wx+b后,求和经过一个函数f,输出得到h(x).这里的f就是我们的激活函数。 激活函数的主要作用是加入非线性因素,把卷积层输出结果做非线性映射。这也是我们引入神经网络的目的,如果不加入激活函数那么对wx+b的计算还是线性的 在卷积神经网络中激活函数一般使用Relu(The Rectified Linear Unit，修正线性单元),他的主要特点是收敛速度快,求梯度简单,公式简单max(0,s)，即对于输入的负值输出全为0,正值输出其本身。 6)全连接层(Fully connected layers) 全连接层在整个神经网络中期到分类器的作用,即通过卷积、激活函数、池化等深度网络后,再经过全连接层对结果进行分类。这里我们还是要将得到的图像拉伸成一个一维向量,就像前面我们使用fc对图像(像素较小的)进行分类一样，由于神经网络是属于监督学习，在模型训练时，根据训练样本对模型进行训练，从而得到全连接层的权重(在每个分类的得分) 3.为什么我们需要CNN? 全连接神经网络参数过于庞大,例如:假设我们有一张1000x1000的图像,隐藏层我们有100万个神经元,由于每个隐藏层的神经元都要与图像的每个像素点连接,所以这里我们就有$1000000 * 1000000 = 10^12$ 个连接,也就需要$10^12$个参数,那么这个参数是过于庞大的。 卷积神经网络有两个神器可以用来降低参数,一个叫局部感受野,一叫交权值共享。 局部感受野 我们一般认为人对外界的认知是从局部到全局的,而图像的空间联系也是局部的像素联系较为紧密,而距离较远的像素相关性较弱。因而,每个神经元其实没必要对全局图像进行感知,只需要对局部进行感知,然后在更高层将局部的信息综合起来就得到了全局的信息。这种思想也是受启发与生物学里面的系统结构，视觉皮层的神经元就是局部接受信息的（即这些神经元只响应某些特定区域的刺激） 如下图所示：左图为全连接，右图为局部连接。 这个图就是描述上面的例子,这里如果我们采用局部感受野的话可以知道,每个像素不需要和所有神经元都连接,假设局部感受野的大小为10x10,隐藏层的神经元都只和每个感受野连接,此时我们的参数个数为 $1000000 * 10 * 10 = 10^8.$ 这样的话我们的参数个数就变为了原来的万分之一,但是还是很多啊,所以就出现了权值共享 权值共享 上面当中每个隐藏层神经元都连接不同的局部感受野,那么如果每个局部感受野的参数都相同,那么我的参数就变为 10*10 = 100.我们可以把这100参数对应的卷积操作看成是提取特征的一种方式,与位置无关。这是CNN的亮点之一,但是有一个问题就是说如果所有的参数都相同,也就是说我们只是提取了特定的某种特征,那么如果我们想要提取多种特征怎么办,这时候我们就需要多增加几个滤波器,每个filter对应提取不同的特征(参数不一样)。 4.一个简单实例 假设给定一张图（可能是字母X或者字母O），通过CNN即可识别出是X还是O，如下图所示，那怎么做到的呢 如果采用经典的神经网络模型，则需要读取整幅图像作为神经网络模型的输入（即全连接的方式），当图像的尺寸越大时，其连接的参数将变得很多，从而导致计算量非常大。 (1)提取特征 如果字母X、字母O是固定不变的，那么最简单的方式就是图像之间的像素一一比对就行，但在现实生活中，字体都有着各个形态上的变化（例如手写文字识别），例如平移、缩放、旋转、微变形等等，如下图所示： 我们的目标是对于各种形态变化的X和O，都能通过CNN准确地识别出来，这就涉及到应该如何有效地提取特征，作为识别的关键因子。 对于CNN来说，它是一小块一小块地来进行比对，在两幅图像中大致相同的位置找到一些粗糙的特征（小块图像）进行匹配，相比起传统的整幅图逐一比对的方式，CNN的这种小块匹配方式能够更好的比较两幅图像之间的相似性。如下图： 以字母X为例，可以提取出三个重要特征（两个交叉线、一个对角线），如下图所示： 假如以像素值"1"代表白色，像素值"-1"代表黑色，则字母X的三个重要特征如下： (2)计算卷积 在本案例中，要计算一个feature（特征）和其在原图上对应的某一小块的结果，只需将两个小块内对应位置的像素值进行乘法运算，然后将整个小块内乘法运算的结果累加起来，最后再除以小块内像素点总个数即可（注：也可不除以总个数的）。 如果两个像素点都是白色（值均为1），那么$1x1 = 1$，如果均为黑色，那么$(-1)*(-1) = 1$，也就是说，每一对能够匹配上的像素，其相乘结果为1。类似地，任何不匹配的像素相乘结果为-1。具体过程如下（第一个、第二个……、最后一个像素的匹配结果）： 根据卷积的计算方式，第一块特征匹配后的卷积计算如下，结果为1 对于其它位置的匹配，也是类似（例如中间部分的匹配） 计算之后的卷积如下： 以此类推，对三个特征图像不断地重复着上述过程，通过每一个feature（特征）的卷积操作，会得到一个新的二维数组，称之为feature map。其中的值，越接近1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。如下图所示： 可以看出，当图像尺寸增大时，其内部的加法、乘法和除法操作的次数会增加得很快，每一个filter的大小和filter的数目呈线性增长。由于有这么多因素的影响，很容易使得计算量变得相当庞大。 (3)池化 采用max-pooling 如下图： 池化区域往左，第二小块取大值max(0.11,0.33,-0.11,0.33)，作为池化后的结果，如下图： 其它区域也是类似，取区域内的最大值作为池化后的结果，最后经过池化后，结果如下： 对所有的feature map执行同样的操作，结果如下： 最大池化（max-pooling）保留了每一小块内的最大值，也就是相当于保留了这一块最佳的匹配结果（因为值越接近1表示匹配越好）。也就是说，它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。 通过加入池化层，图像缩小了，能很大程度上减少计算量，降低机器负载。 (4)Relu激活函数 第一个值，取max(0,0.77)，结果为0.77，如下图 以此类推，经过ReLU激活函数后，结果如下： 对所有的feature map执行ReLU激活函数操作，结果如下： (5)深度神经网络 通过将上面所提到的卷积、激活函数、池化组合在一起，就变成下图： 通过加大网络的深度，增加更多的层，就得到了深度神经网络，如下图： (6)全连接层 全连接层在整个卷积神经网络中起到“分类器”的作用，即通过卷积、激活函数、池化等深度网络后，再经过全连接层对结果进行识别分类。 首先将经过卷积、激活函数、池化的深度网络后的结果串起来，如下图所示： 由于神经网络是属于监督学习，在模型训练时，根据训练样本对模型进行训练，从而得到全连接层的权重（如预测字母X的所有连接的权重） 在利用该模型进行结果识别时，根据刚才提到的模型训练得出来的权重，以及经过前面的卷积、激活函数、池化等深度网络计算出来的结果，进行加权求和，得到各个结果的预测值，然后取值最大的作为识别的结果（如下图，最后计算出来字母X的识别值为0.92，字母O的识别值为0.51，则结果判定为X） 上述这个过程定义的操作为”全连接层“(Fully connected layers)，全连接层也可以有多个，如下图： (7)卷积神经网络 将以上所有结果串起来后，就形成了一个“卷积神经网络”（CNN）结构，如下图所示: 再回顾总结一下，卷积神经网络主要由两部分组成，一部分是特征提取（卷积、激活函数、池化），另一部分是分类识别（全连接层），下图便是著名的手写文字识别卷积神经网络结构图：]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018百度之星-调查问卷(状压dp)]]></title>
    <url>%2F2018%2F08%2F05%2F2018-8-5%2F</url>
    <content type="text"><![CDATA[题意Problem Description度度熊为了完成毕业论文，需要收集一些数据来支撑他的论据，于是设计了一份包含 m 个问题的调查问卷，每个问题只有 ‘A’ 和 ‘B’ 两种选项。 将问卷散发出去之后，度度熊收到了 n 份互不相同的问卷，在整理结果的时候，他发现可以只保留其中的一部分问题，使得这 n 份问卷仍然是互不相同的。这里认为两张问卷是不同的，当且仅当存在至少一个被保留的问题在这两份问卷中的回答不同。 现在度度熊想知道，存在多少个问题集合，使得这 n 份问卷在只保留这个集合的问题之后至少有 k 对问卷是不同的。Input第一行包含一个整数 T，表示有 T 组测试数据。 接下来依次描述 T 组测试数据。对于每组测试数据： 第一行包含三个整数 n，m 和 k，含义同题目描述。 接下来 n 行，每行包含一个长度为 m 的只包含 ‘A’ 和 ‘B’ 的字符串，表示这份问卷对每个问题的回答。 保证 1≤T≤100，1≤n≤103，1≤m≤10，1≤k≤106，给定的 n 份问卷互不相同。 Output对于每组测试数据，输出一行信息 “Case #x: y”（不含引号），其中 x 表示这是第 x 组测试数据，y 表示满足条件的问题集合的个数，行末不要有多余空格。 Sample InputCopy22 2 1AABB2 2 2AABBSample OutputCopyCase #1: 3Case #2: 0 思路观察到n比较大,m很小，而且是一个集合问题。所以我们可以想到这里可以利用二进制枚举集合的状态,$2^10 = 1024$。那么剩下的问题就是怎么去验证。dp[i][j]表示前i个物品中当集合状态为j时不同的问卷数目.注意观察,只要有一位不同就算不同,所以我们开一个数组,num[sta]前面1~i-1个物品中,当集合状态为j时,和i问卷答案相同(i的答案此时为sta)的问卷数,那么剩下的都不同,即有:dp[i][j] = dp[i - 1][j] + i - num[sta]. 1234567891011121314151617181920212223242526272829303132333435363738#include&lt;bits/stdc++.h&gt;using namespace std;typedef long long ll;const int maxn = 1e3+5;int dp[maxn][1025],num[1025];char str[maxn][12];int n,m,k;int main()&#123; int _,ca = 1; cin &gt;&gt; _; while(_--) &#123; scanf("%d%d%d",&amp;n,&amp;m,&amp;k); for(int i = 1;i &lt;= n;++i) scanf("%s",str[i]); memset(dp,0,sizeof dp); for(int sta = 0;sta &lt; (1 &lt;&lt; m);++sta) &#123; memset(num,0,sizeof num); for(int i = 1;i &lt;= n;++i) &#123; int tmp = 0; for(int j = 0;j &lt; m;++j) if(sta &amp; (1 &lt;&lt; j) &amp;&amp; str[i][j] == 'A') tmp |= (1 &lt;&lt; j); num[tmp]++; dp[i][sta] = dp[i - 1][sta] + i - num[tmp]; &#125; &#125; ll ans = 0; for(int sta = 0;sta &lt; (1 &lt;&lt; m);++sta) if(dp[n][sta] &gt;= k) ans++; printf("Case #%d: %I64d\n",ca++,ans); &#125; return 0;&#125;]]></content>
      <tags>
        <tag>Dp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[滑动窗口的最大值(单调队列)]]></title>
    <url>%2F2018%2F08%2F03%2F2018-8-3-1%2F</url>
    <content type="text"><![CDATA[题意 给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 思路: 典型的单调队列解决滑动窗口问题。 这里因为我们需要获得的是长度为size的最大值,所以我们可以维护一个单调的递减队列,这样队首即为最大值。但是这里我们需要用到双端队列(deque)因为我们还需要从队尾取,队列中存储的全部是下标(组这样好判断区间长度)很显然所有的都需要入队的,不过当入队时需要判断当前的数和队尾的大小,如果当前大那么队尾不可能成为答案的(这也是为了保证递减特性)。同时还需要从队首判断区间差是否超过了size,超过也需要pop出来。 这里需要注意使用的unsigned int，unsigned 出现负数是直接用补码表示出来的 123unsigned int v=1;unsigned int b=5;cout&lt;&lt;v-b&lt;&lt;endl; //输出4294967292，因为没有负数 123456789101112131415161718class Solution &#123;public: vector&lt;int&gt; maxInWindows(const vector&lt;int&gt;&amp; num, unsigned int size) &#123; if(num.size() == 0) return &#123;&#125;; vector&lt;int&gt;ans;ans.clear(); deque&lt;int&gt;q; //双端队列中存放下标 for(unsigned int i = 0;i &lt; num.size();++i) &#123; while(q.size() &amp;&amp; num[q.back()] &lt;= num[i]) q.pop_back(); while(q.size() &amp;&amp; i - q.front() + 1 &gt; size) q.pop_front(); q.push_back(i); if(size &amp;&amp; i + 1 &gt;= size) ans.push_back(num[q.front()]); &#125; return ans; &#125;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据流中的中位数]]></title>
    <url>%2F2018%2F08%2F02%2F2018-8-2-0%2F</url>
    <content type="text"><![CDATA[题意 如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。 思路这个题目我们可以维护两个堆,一个大顶堆一个小顶堆,小顶堆的最小的数大于等于大顶堆中的最大的数,同时保证二者之间元素的个数最多大顶堆比小顶堆多1,那么我们便可以知道中位数一定位于两个堆顶元素中.(要么是大顶堆中最大的,要么是二者的均值)1234567891011121314151617181920212223242526class Solution &#123;public: Solution() &#123; while(!p.empty()) p.pop(); while(!q.empty()) q.pop(); &#125; void Insert(int num) &#123; if(p.empty() || num &lt;= p.top()) p.push(num); else q.push(num); if(p.size() - q.size() == 2) q.push(p.top()),p.pop(); if(p.size() &lt; q.size()) p.push(q.top()),q.pop(); return ; &#125; double GetMedian() &#123; int cnt = p.size() + q.size(); if(cnt &amp; 1) return p.top(); else return (p.top() + q.top()) / 2.0; &#125;private: priority_queue&lt;int,vector&lt;int&gt;,less&lt;int&gt; &gt; p; //大顶堆 priority_queue&lt;int,vector&lt;int&gt;,greater&lt;int&gt; &gt; q; //小顶堆&#125;; 还有一个神仙做法是可以通过AVL数来做 思路：构建一棵”平衡二叉搜索树 “。每个结点左子树均是小于等于其value的值，右子树均大于等于value值。每个子树均按其 “结点数” 调节平衡。 这样根节点一定是中间值中的一个。若结点数为奇数，则返回根节点的值；若结点个数为偶数，则再从根结点左子数或右子数中个数较多的子树中选出最大或最小值既可。 按照节点数来调节平衡,很神奇的做法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183链接：https://www.nowcoder.com/questionTerminal/9be0172896bd43948f8a32fb954e1be1来源：牛客网struct myTreeNode&#123; int val; int count;//以此节点为根的树高 struct myTreeNode* left; struct myTreeNode* right; myTreeNode(int v) : val(v), count(1), left(NULL), right(NULL) &#123;&#125; &#125;; myTreeNode *root = NULL; class Solution&#123;public: /*计算以节点为根的树的高度 */ int totalCount(myTreeNode* node) &#123; if (node == NULL) return 0; else return node-&gt;count; &#125; //左左 void rotateLL(myTreeNode* &amp;t) &#123; myTreeNode* k = t-&gt;left; myTreeNode* tm = NULL; while (k-&gt;right != NULL) &#123; k-&gt;count--; tm = k; k = k-&gt;right; &#125; if (k != t-&gt;left) &#123; k-&gt;left = t-&gt;left; tm-&gt;right = NULL; &#125; t-&gt;left = NULL; k-&gt;right = t; t-&gt;count = totalCount(t-&gt;left) + totalCount(t-&gt;right) + 1; k-&gt;count = totalCount(k-&gt;left) + t-&gt;count + 1; t = k; &#125; //右右 void rotateRR(myTreeNode* &amp;t) &#123; myTreeNode* k = t-&gt;right; myTreeNode* tm = NULL; while (k-&gt;left != NULL) &#123; k-&gt;count--; tm = k; k = k-&gt;left; &#125; if (k != t-&gt;right) &#123; k-&gt;right = t-&gt;right; tm-&gt;left = NULL; &#125; t-&gt;right = NULL; k-&gt;left = t; t-&gt;count = totalCount(t-&gt;left) + 1; k-&gt;count = totalCount(k-&gt;right)+ t-&gt;count + 1; t = k; &#125; //左右 void rotateLR(myTreeNode* &amp;t) &#123; rotateRR(t-&gt;left); rotateLL(t); &#125; //右左 void rotateRL(myTreeNode* &amp;t) &#123; rotateLL(t-&gt;right); rotateRR(t); &#125; //插入 void insert(myTreeNode* &amp;root, int x) &#123; if (root == NULL) &#123; root = new myTreeNode(x); return; &#125; if (root-&gt;val &gt;= x) &#123; insert(root-&gt;left, x); root-&gt;count = totalCount(root-&gt;left)+ totalCount(root-&gt;right) + 1; if (2 == totalCount(root-&gt;left) - totalCount(root-&gt;right)) &#123; if (x &lt; root-&gt;left-&gt;val) &#123; rotateLL(root); &#125; else &#123; rotateLR(root); &#125; &#125; &#125; else &#123; insert(root-&gt;right, x); root-&gt;count = totalCount(root-&gt;left)+ totalCount(root-&gt;right) + 1; if (2 == totalCount(root-&gt;right) - totalCount(root-&gt;left)) &#123; if (x &gt; root-&gt;right-&gt;val) &#123; rotateRR(root); &#125; else &#123; rotateRL(root); &#125; &#125; &#125; &#125; void deleteTree(myTreeNode* root) &#123; if (root == NULL)return; deleteTree(root-&gt;left); deleteTree(root-&gt;right); delete root; root = NULL; &#125; void Insert(int num) &#123; insert(root, num); &#125; double GetMedian() &#123; int lc = totalCount(root-&gt;left), rc = totalCount(root-&gt;right); if ( lc == rc) return root-&gt;val; else &#123; bool isLeft = lc &gt; rc ; myTreeNode* tmp ; if (isLeft) &#123; tmp = root-&gt;left; while (tmp-&gt;right != NULL) &#123; tmp = tmp-&gt;right; &#125; &#125; else &#123; tmp = root-&gt;right; while (tmp-&gt;left != NULL) &#123; tmp = tmp-&gt;left; &#125; &#125; return (double)(root-&gt;val + tmp-&gt;val) / 2.0; &#125; &#125; &#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树的下一个结点]]></title>
    <url>%2F2018%2F07%2F31%2F2018-7-31-2%2F</url>
    <content type="text"><![CDATA[题意给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 思路题意描述的很不好,next指针就是指向父节点的那个。 一个最简单的方法就是跑一遍中序遍历,然后O(N)判断一下即可。 这里有个更好的方法对于中序遍历我们清楚的知道他的遍历顺序为:”左根右”。那么就可以分为一下三种情况:1)如果当前结点时NULL,则返回NULL2)如果当前结点存在右子树,那么它的下一节点就是从其右孩子开始一直找左孩子,知道找不到为止。3)如果当前结点没有右子树,但他是其父节点的左孩子,则返回父节点。否则就找其父亲的父亲,并判断其是否是父亲的左子树,直到满足这种条件为止,返回其父亲。若找不到则为NULL。(这里考虑到中序遍历的遍历顺序就很好理解了)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283/* 给定一个二叉树和其中的一个结点， 请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。*/#include&lt;bits/stdc++.h&gt;using namespace std;struct TreeLinkNode &#123; int val; struct TreeLinkNode *left; struct TreeLinkNode *right; struct TreeLinkNode *next; TreeLinkNode(int x) :val(x), left(NULL), right(NULL), next(NULL) &#123; &#125;&#125;;/*class Solution &#123;public: TreeLinkNode* GetNext(TreeLinkNode* pNode) &#123; if(pNode == NULL) return pNode; TreeLinkNode *root = pNode; while(root -&gt; next != NULL) root = root -&gt; next; gen = tmp = NULL; Inorder(root); while(gen != NULL) &#123; if(gen == pNode) return gen -&gt; next; gen = gen -&gt; next; &#125; return NULL; &#125;private: TreeLinkNode *gen,*tmp; void Inorder(TreeLinkNode *pHead) &#123; if(pHead == NULL) return ; Inorder(pHead -&gt; left); if(gen == NULL) gen = pHead; else tmp -&gt; next = pHead; tmp = pHead; tmp -&gt; next = NULL; Inorder(pHead -&gt; right); return ; &#125;&#125;;*/class Solution &#123;public: TreeLinkNode* GetNext(TreeLinkNode* pNode) &#123; if(pNode == NULL) return NULL; if(pNode -&gt; right != NULL) &#123; TreeLinkNode *p = pNode -&gt; right; while(p -&gt; left != NULL) p = p -&gt; left; return p; &#125; while(pNode -&gt; next != NULL) &#123; TreeLinkNode *root = pNode -&gt; next; if(root -&gt; left == pNode) return root; pNode = pNode -&gt; next; &#125; return NULL; &#125;&#125;;int main()&#123; return 0;&#125;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[删除链表中重复的结点]]></title>
    <url>%2F2018%2F07%2F31%2F2018-7-31-1%2F</url>
    <content type="text"><![CDATA[题意在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5 思路简单来讲就是要将所有重复的去掉,然后连接起来。这里通过p1,p2来实现.虽然三个while循环,但是复杂度应该是O(n) 因为每个结点都访问了一次. 12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123;public: ListNode* deleteDuplication(ListNode* pHead) &#123; if(pHead == NULL || pHead -&gt; next == NULL) return pHead; ListNode *head = NULL; ListNode *p = NULL; while(pHead != NULL) &#123; ListNode *p1 = pHead; while(p1 != NULL &amp;&amp; p1 -&gt; next != NULL &amp;&amp; p1 -&gt; val == p1 -&gt; next -&gt; val) &#123; ListNode *p2 = p1; while(p2 != NULL &amp;&amp; p2 -&gt; next != NULL &amp;&amp; p2 -&gt; val == p2 -&gt; next -&gt; val) &#123; //puts("+++"); //cout &lt;&lt; p2 -&gt; val &lt;&lt; '-' &lt;&lt; p2 -&gt; next -&gt; val &lt;&lt; endl; p2 = p2 -&gt; next; &#125; //puts("***"); p1 = p2 -&gt; next; //cout &lt;&lt; p1 -&gt; val &lt;&lt; endl; &#125; if(head == NULL) &#123; //puts("--="); head = p1; p = head; &#125; else &#123; p -&gt; next = p1; p = p1; &#125; if(p1 == NULL) break; pHead = p1 -&gt; next; &#125; return head; &#125;&#125;; 两一个简单易写的方法就是可以通过递归实现.123456789101112131415161718class Solution &#123;public: ListNode* deleteDuplication(ListNode* pHead) &#123; if(pHead == NULL || pHead -&gt; next == NULL) return pHead; if(pHead -&gt; val == pHead -&gt; next -&gt; val) &#123; ListNode *p = pHead -&gt; next; //循环跳过所有和当前值重复的结点,在递归处理下一个结点,最后结果是将所有有重复的结点都删除 while(p != NULL &amp;&amp; p -&gt; val == pHead -&gt; val) p = p -&gt; next; pHead = deleteDuplication(p); &#125; else pHead -&gt; next = deleteDuplication(pHead -&gt; next); return pHead; &#125;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链表中环的入口结点]]></title>
    <url>%2F2018%2F07%2F29%2F2018-7-29-2%2F</url>
    <content type="text"><![CDATA[题意: 给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 思路: 一个比较简单的方法就是map,或者set一下.第一个重复的就是入口结点,复杂度O(nlogn) 12345678910111213141516171819202122232425class Solution &#123;public: ListNode* EntryNodeOfLoop(ListNode* pHead) &#123; ListNode *Head = pHead; ListNode *ans = NULL; while(Head) &#123; if(mp[Head] != 0) &#123; ans = Head; break; &#125; mp[Head] = 1; Head = Head -&gt; next; &#125; return ans; &#125; Solution() &#123; mp.clear(); &#125;private: map&lt;ListNode*,int&gt;mp;&#125;; 这里还要介绍一个O(N)的做法 证明可以看这里 方法:这里可以开两个指针,一个是快指针fast,一个是慢指针slow.fast一次走两步,slow一次走一步，则存在定理如果单链表有环那么当二者相遇时一定在环内。则此时将一个指到链表头部,另一个不变，二者同时每次动一格,则当二者再次相遇时即为环的入口节点。如果fast走到null 则无环。 证明:假设环长度为n,进入环之前结点个数为x,slow在环内走了k个结点,fast绕环走了m圈,则有$ 2(x + k) = x + mn + k $ 可以得到x = mn - k。此时slow距入口结点还剩 n-k个结点,$x = (m - 1)n + n - k$，则可知当二者再次相遇时即为入口结点。 12345678910111213141516171819202122232425class Solution &#123;public: ListNode* EntryNodeOfLoop(ListNode* pHead) &#123; if(pHead == NULL || pHead -&gt; next == NULL || pHead -&gt; next -&gt; next == NULL) return NULL; ListNode *fast = pHead; ListNode *slow = pHead; while(fast != NULL) &#123; fast = fast -&gt; next -&gt; next; slow = slow -&gt; next; if(slow == fast) break; &#125; if(fast == NULL) return NULL; fast = pHead; while(fast != slow) &#123; fast = fast -&gt; next; slow = slow -&gt; next; &#125; return fast; &#125;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字符流中第一个不重复的字符(队列+思维)]]></title>
    <url>%2F2018%2F07%2F29%2F2018-7-29-1%2F</url>
    <content type="text"><![CDATA[题目描述: 请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。输出描述:如果当前字符流没有存在出现一次的字符，返回#字符。 思路:一个比较好的做法就是,去除插入Insert复杂度肯定为O(N)。求出现一次的第一个字符复杂度为O(C)，C是一个常数，这里我认为复杂度趋近于O(1),空间复杂度O(128)。 具体方法就是利用vis标记数组,标记每个字符出现的次数。对于每个第一次出现的字符将其放入队列中,每次需要找第一个只出现一次的字符就从队列头开始找,如果出现超过一次的直接pop就好了。这样我们可以知道,队列中最多128个字符,也最多pop 128次,其余的都是空的,每次取队列头即可。 1234567891011121314151617181920212223242526class Solution&#123;public: //Insert one char from stringstream void Insert(char ch) &#123; vis[ch - '\0']++; if(vis[ch - '\0'] == 1) Q.push(ch); return ; &#125; //return the first appearence once char in current stringstream char FirstAppearingOnce() &#123; while(!Q.empty() &amp;&amp; vis[Q.front() - '\0'] &gt; 1) Q.pop(); if(Q.empty()) return '#'; return Q.front(); &#125; Solution() &#123; memset(vis,0,sizeof vis); &#125;private: queue&lt;char&gt;Q; int vis[128];&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer 正则表达式匹配(递归)]]></title>
    <url>%2F2018%2F07%2F09%2F2018-7-9%2F</url>
    <content type="text"><![CDATA[题意 请实现一个函数用来匹配包括'.'和'*'的正则表达式。模式中的字符'.'表示任意一个字符，而'*'表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串"aaa"与模式"a.a"和"ab*ac*a"匹配，但是与"aa.a"和"ab*a"均不匹配 思路 这个题我们可以分以下几种情况来考虑: 1.若str 和 pattern 都是空串 那么匹配成功 2.若str不为空但是pattern为空 那么匹配失败 3.若str为空,但pattern不为空,此时不一定匹配失败。比如pattern 为 “aab“,它是可以代表一个空串的。 4.对于当前的pattern,我们要看它的下一个是否为”“: (1) 不为”“，那么他俩必须匹配,否则匹配失败(当然也要注意”.”也算匹配) (2)为”“,那么我们就需要判断”*”要匹配几次。 若匹配0次,则直接patern后移两位,str不变。 匹配一次,str后移,pattern不变。(这里是因为可以通过把匹配多次变成匹配一次,然后递归即可) 匹配多次可以转化成匹配一次,然后转到(2). 若当前str 和pattern 匹配,且pattern + 1 为 “*”，也可以不匹配!要两种情况考虑。 12345678910111213141516171819202122232425class Solution &#123;public: bool match(char* str, char* pattern) &#123; if(*str == '\0' &amp;&amp; *pattern == '\0') return true; if(*str != '\0' &amp;&amp; *pattern == '\0') return false; if(*(pattern + 1) != '*') &#123; if(*str == *pattern || (*str != 0 &amp;&amp; *pattern == '.')) return match(str + 1,pattern + 1); else return false; &#125; else &#123; if(*str == *pattern || (*str != '\0' &amp;&amp; *pattern == '.')) return match(str,pattern + 2) || match(str + 1,pattern); else return match(str,pattern + 2); &#125; &#125;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer 把字符串转换成整数(库函数)]]></title>
    <url>%2F2018%2F07%2F06%2F2018-7-6%2F</url>
    <content type="text"><![CDATA[题意将一个字符串转换成一个整数，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0 思路:此题其实不用库函数也很简单,这里只是记录下库函数。1.int/float to string/array: C语言提供了几个标准库函数，可以将任意类型(整型、长整型、浮点型等)的数字转换为字符串，下面列举了各函数的方法及其说明。● itoa()：将整型值转换为字符串。● ltoa()：将长整型值转换为字符串。● ultoa()：将无符号长整型值转换为字符串。● gcvt()：将浮点型数转换为字符串，取四舍五入。● ecvt()：将双精度浮点型值转换为字符串，转换结果中不包含十进制小数点。● fcvt()：指定位数为转换精度，其余同ecvt()。 除此外，还可以使用sprintf系列函数把数字转换成字符串，其比itoa()系列函数运行速度慢 2. string/array to int/floatC/C++语言提供了几个标准库函数，可以将字符串转换为任意类型(整型、长整型、浮点型等)。● atof()：将字符串转换为双精度浮点型值。● atoi()：将字符串转换为整型值。● atol()：将字符串转换为长整型值。● strtod()：将字符串转换为双精度浮点型值，并报告不能被转换的所有剩余数字。● strtol()：将字符串转换为长整值，并报告不能被转换的所有剩余数字。● strtoul()：将字符串转换为无符号长整型值，并报告不能被转换的所有剩余数字。 这里想强调一下的就是要进行字符串转化整数,必须要求字符串是合法的，否则如:”123654a4a” 转化为123654.另外 这些例如atoi,将字符串转化为整数的只能是char类型,string不可以。 123456789101112131415161718192021222324class Solution &#123;public: int StrToInt(string str) &#123; int res = 0; int flag = 1; for(int i = 0;i &lt; str.size();++i) &#123; if(!res &amp;&amp; str[i] == ' ') continue; if(str[i] == ' ') return 0; if(str[i] == '-' || str[i] == '+') &#123; if(i != 0) return 0; if(str[i] == '-') flag *= -1; if(str[i] == '+') flag = 1; &#125; else &#123; if(str[i] &lt; '0' || str[i] &gt; '9') return 0; res = res * 10 + str[i] - '0'; &#125; &#125; return res * flag; &#125;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性代数知识点总结]]></title>
    <url>%2F2018%2F07%2F06%2F%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[如何理解矩阵特征值以及特征向量？一篇很好的文章 可以把A看成是一个线性变换,那么这个定义可以看成对于向量x而言,在A的作用下保持方向不变(可能反向)，进行大小为 $ \lambda $的缩放。 特征向量所在的直线包含了所有特征向量. 矩阵乘以特征向量可以看成是矩阵在每个特征向量方向上的投影。通过求特征值和特征向量把矩阵数据投影在一个正交的空间,而且在各个方向的投影大小就是特征值。 最大特征值并不是说数据在所有方向的投影的最大值，而仅限于正交空间的某一方向。最大特征值的特征向量所对应的方向就是速度最大的方向。其实是一种数据的处理方法，可以简化数据。 什么是相似矩阵?有什么用? 线性变换例如:$ \vec{y} = A\vec{x} $（类似于一次函数 y = x）线性变换通过指定基下的矩阵A来表示 同一个线性变换,不同基下的矩阵称为相似矩阵.（任意向量在不同的基中有不同的表示） 线性变换也可以直观的理解为对n维空间中的一个立体(随便什么形状的)伸缩或者拉伸。其中这个n维立体中的每个点都经过这个线性变换,变成n维空间中的一个新的立体. 将一个矩阵相似对角化为另一个矩阵,可以简化坐标系,使看起来比较直观。另一方面可以很方便的矩阵的幂级数。例如: $ A^k = P\Lambda^kP^{-1} $ 行列式的本质是什么？行列式是线性变换的伸缩因子。(放大率)之所以这里行列式线性变换的伸缩因子或者放大率主要是因为 矩阵变换的本质其实是基的变换，而这种基的变换是通过一个矩阵来改变的,其值就是所对应的行列式的值,它代表了对图形的缩放。]]></content>
      <tags>
        <tag>各种基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer 不用加减乘除做加法]]></title>
    <url>%2F2018%2F07%2F05%2F2018-7-5-1%2F</url>
    <content type="text"><![CDATA[题意: 写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 思路 不让用+-*/。。。那肯定要考虑二进制了。 1.在不考虑进位的情况下两个数的加法相当于对应二进制下的异或。 2.进位就相当于两个数的与然后左移一位。 剩下的就是模拟了将1和2的结果相加,直到进位为0结束。 123456789class Solution &#123;public: int Add(int num1, int num2) &#123; if(num2 == 0) return num1; else return Add(num1^num2,(num1&amp;num2) &lt;&lt; 1); &#125;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之逻辑回归总结]]></title>
    <url>%2F2018%2F07%2F04%2F2018-7-4-2%2F</url>
    <content type="text"><![CDATA[逻辑回归此文章主要学习于吴恩达老师的Machine Learning 公开课。 逻辑回归主要用于二分类问题,eg 垃圾邮件识别，交易是否欺诈，是否恶性肿瘤等。 也就是说这里我们的lable只有两类,y == 0 || y == 1.很多时候，我们希望我们的预测值输出范围在0～1之间，可以加入一个sigmoid函数(可以称为逻辑函数)，类似神经网络的激活函数，输出范围就控制在了0～1之间。 sigmod函数: $ g(z) = \frac{1}{1 + e^{-z}}$ 从而假设函数如下: $ h_{\theta}(x) = \frac{1}{1 + e^{-\theta^Tx}}$ 假设函数的输出意义就是在输入X的情况下Y==1的概率。 决策边界 决策边界是假设函数的一个属性，有确定的参数值我们就可以直接得到决策边界。它不是数据集的属性只要概率大于等于0.5，那么就当作y=1 如果假设函数是 （三个参数分别等于-3，1，1），所以参数向量就是（-3，1，1），则：决策边界的方程就是 x1+x2 = 3. 损失函数 在之前线性回归的时候，我们使用的是差平方误差函数，我们能得到一个单弓形的凸函数，但是逻辑回归中，h函数是sigmoid函数，对应的J函数是左下角的形式，会形成很多局部最优解，用梯度下降的话就不能很好解决问题，因此需要换一个损失函数，希望它像右下角那样是单弓形的凸函数。我们选择的损失函数是对数函数： 上面这个图像告诉我们如果实际值y=1，而我们的预测值也为1的话,那么我们的代价函数值就很小,接近于0.而如果实际值y=1而我们预测值是0的话,我们的代价函数值就很大。 同理,当y=0而我们的预测值h(x)也为0我们的代价同样也是很小的；如果预测值为1我们的代价就是很大的。至此,我们的代价函数能很好的反应我们的参数$\theta$ 对数据的你和情况,我们就可以根据代价函数的值来寻找最优的参数$\theta$来拟合数据。 简化损失函数 梯度下降先来下梯度下降的推导:可以看这个博客推导 主要也是运用了链式法则，将下面的化简结果带入上式即可。、最终的梯度下降: 将逻辑回归应用到多分类中例如下面一个三分类问题,我们可以构造三个分类器$h_1,h_2,h_3$ 最后对于输入的x,我们预测它的分类只需要在所有的分类器中找最大值即可。 $ maxi h{\theta (x)$ 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485# -*- coding: utf-8 -*-import numpy as npimport pandas as pdfrom numpy.linalg import invimport matplotlib.pyplot as plt """构建Logistic Regression Function , h(x) = 1 / (1 + exp(-wx))，对是否为setosa进行分类Cost Function J(theta) is 1/m sigma(-ylog(h(x) - (1 - y)log(1 - h(x)))) """def sigmoid(x): return 1.0/(1 + np.exp(-x))#梯度下降def GradientDescent(X,Y,alpha,theta,maxIteraton,J): m,n = X.shape for i in range(maxIteraton): hypothesis = sigmoid(np.dot(X,theta)) #计算损失函数值 J[i] = -(1.0 / m)*np.sum(Y*np.log(hypothesis) + (1 - Y) * np.log(1 - hypothesis)) loss = hypothesis - Y gradient = np.dot(X.T,loss) / m theta = theta - alpha * gradient return theta,J"""牛顿迭代法,使收敛的速度更快"""#预测def predict(X,theta): m,n = X.shape Xtest = np.ones((m,n + 1)) Xtest[:,:-1] = X Ytest = sigmoid(np.dot(Xtest,theta)) Y_pre = [] for i in Ytest: if i &gt;= 0.5: Y_pre.append(1) else: Y_pre.append(0) return Y_preif __name__ == '__main__': datapath = r'C:\Users\shilongbao\assignment1\iris\iris.csv' #读取数据 iris = pd.read_csv(datapath) # 获得虚拟变量（哑变量）；也就是将当前所拥有的值扩充为一个矩阵,在自己对应的位置置1,其余的置0 dummy = pd.get_dummies(iris['Species']) # 将两个矩阵连接起来 iris = pd.concat([iris,dummy],axis = 1) #Y_trian = iris.iloc[113,'setosa'] #print(X_train) #print(Y_trian) #iloc 截取样本(按行按列) iris = iris.iloc[0:100,:] #print(iris) temp = pd.DataFrame(iris.iloc[:,1:]) #print(temp) X = temp.iloc[:,:4] X['x0'] = 1 #print(X) Y = np.reshape(iris['setosa'],len(iris),1) print(Y.shape) m,n = X.shape #print("%d %d"%(m,n)) theta = np.ones(n) #print(theta) alpha = 0.001 # 选择学习率 maxIteraton = 3000 J = pd.Series(np.arange(maxIteraton,dtype = float)) theta,J = GradientDescent(X,Y,alpha,theta,maxIteraton,J) print(theta) J.plot() X_train = [6.8,3.0,5.5,2.1] # 简单的测试 X_train = np.array(X_train).reshape(1,4) Y_train = np.array([0]).reshape(1) Y_pre = predict(X_train,theta) if Y_pre[0] == Y_train[0]: print("correct!") else: print("wrong!")]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer 和为S的连续正数序列]]></title>
    <url>%2F2018%2F07%2F04%2F2018-7-4-1%2F</url>
    <content type="text"><![CDATA[题意 小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? Good Luck!输出描述: 输出所有和为S的连续正数序列。序列内按照从小至大的顺序，序列间按照开始数字从小到大的顺序 思路: 经典尺取法,复杂度O(n)。手残写了好久。。。12345678910111213141516171819202122232425262728class Solution &#123;public: vector&lt;vector&lt;int&gt; &gt; FindContinuousSequence(int sum) &#123; vector&lt;vector&lt;int&gt; &gt;ans; vector&lt;int&gt;arr; if(sum == 1) return ans; int l = 1,r = 2,res = 1,tmp = sum / 2 + 1; while(l &lt; r) &#123; while(res &lt; sum &amp;&amp; r &lt;= tmp) &#123; res += r; r++; &#125; if(res == sum) &#123; arr.clear(); for(int i = l;i &lt; r;++i) arr.push_back(i); ans.push_back(arr); &#125; res -= l; l++; &#125; // cout &lt;&lt; ans.size() &lt;&lt; endl; return ans; &#125;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer 数组中只出现一次的两个数]]></title>
    <url>%2F2018%2F07%2F04%2F2018-7-4%2F</url>
    <content type="text"><![CDATA[题意: 一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。 思路: 我们先看看这个题的弱化版本,让你找出数组中出现一次的一个数,其余的数都只出现两次。简单直接的方法就是 直接异或,最后剩下的那个数就是数组中出现一次的那个数。 那么对于要我们找两个数,显然我们可以想到的是将数组分为两组,一组中包含一个只出现一次的数,另一组也包含一个只出现一次的数,其余的都出现两次。然后分别对这两组做异或即可得到两个只出现一次的数了。 现在问题是如何分组呢？我们知道这两个数一定不相同,也就是一定至少一位不一样。这里我们可以先对整个数组进行异或,得到我们要找的这两个数的异或值。最后从低位开始找到一位“1”,然后将原数组中该位为1的一组,为0的另一组,这样分组就完成了。每个分组里其余的数一定是出现两次,因为两个数相同则每一位都相同,他们一定是分到同一个组里的。 12345678910111213141516171819202122232425262728class Solution &#123;public: void FindNumsAppearOnce(vector&lt;int&gt; data,int* num1,int *num2) &#123; int ans = 0; for(int i = 0;i &lt; data.size();++i) ans ^= data[i]; int ans1 = 0,ans2 = 0; int index = 0; for(int i = 0;i &lt; 32;++i) &#123; if(ans &amp; (1 &lt;&lt; i)) &#123; index = i; break; &#125; &#125; for(int i = 0;i &lt; data.size();++i) &#123; if(data[i] &amp; (1 &lt;&lt; index)) ans1 ^= data[i]; else ans2 ^= data[i]; &#125; *num1 = ans1; *num2 = ans2; return ; &#125;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习之图像分类----多分类支持向量机（Multi-SVM）与softmax分类]]></title>
    <url>%2F2018%2F07%2F03%2Fsoftmax%2F</url>
    <content type="text"><![CDATA[Multi-SVM本文学习自该大V 概述: 由于KNN算法的局限性,我们需要实现更强大的方法来实现图像分类，一般情况下该方法包含两个函数，一是评分函数(score function)，它是原始图像到每个分类的分值映射，二是损失函数(loss function)，它是用来量化预测分类标签的得分与真实标签的一致性的。该方法可以转化为一个最优化问题,在最优化过程中,将通过更新评分函数的参数来最小化损失函数的值,从而使得我们找到一个更好的评分函数(参数W)。 从图像到标签分值的参数化映射 评分函数将图像的像素值映射为各个类别的得分,得分越高说明越有可能属于该类别。 评分函数:我们定义一个简单的函数映射:$ f(x_i,W,b) = Wx_i + b $ $参数W被称为权重（weights）。b被称为偏差向量（bias vector）$，这是因为它影响输出数值，但是并不和原始数据$x_i$产生关联。每个输入图像x_i经过该评分映射后会得到一个[K × 1 ]的矩阵,每一个表示该图像在该分类中的得分,即可能是该分类的可能性 需要注意的几点：1.该方法的一个优势是训练数据是用来学习到参数W和b的，一旦训练完成，训练数据就可以丢弃，留下学习到的参数即可。这是因为一个测试图像可以简单地输入函数，并基于计算出的分类分值来进行分类。2.输入数据$（x_i,y_i）$是给定不变的,我们的目的是通过设置权重W和偏差值b，使得计算出来的分类分值情况和训练集中图像的数据真实类别标签相符合 一个将图像映射到分类分值的例子。为了便于可视化，假设图像只有4个像素（都是黑白像素，这里不考虑RGB通道），有3个分类（红色代表猫，绿色代表狗，蓝色代表船，注意，这里的红、绿和蓝3种颜色仅代表分类，和RGB通道没有关系）。首先将图像像素拉伸为一个列向量，与W进行矩阵乘，然后得到各个分类的分值。需要注意的是，这个W一点也不好：猫分类的分值非常低。从上图来看，算法倒是觉得这个图像是一只狗。 偏差和权重的合并技巧:在进一步学习前，要提一下这个经常使用的技巧。它能够将我们常用的参数W和b合二为一。回忆一下，分类评分函数定义为：$f(x_i,W,b) = Wx_i + b $分开处理这两个参数（权重参数W和偏差参数b）有点笨拙，一般常用的方法是把两个参数放到同一个矩阵中，同时x_i向量就要增加一个维度，这个维度的数值是常量1，这就是默认的偏差维度。这样新的公式就简化成下面这样：$f(x_i,W) = Wx_i$还是以CIFAR-10为例，那么x_i的大小就变成[3073x1]，而不是[3072x1]了，多出了包含常量1的1个维度）。W大小就是[10x3073]了。W中多出来的这一列对应的就是偏差值b，具体见下图： 偏差技巧的示意图。左边是先做矩阵乘法然后做加法，右边是将所有输入向量的维度增加1个含常量1的维度，并且在权重矩阵中增加一个偏差列，最后做一个矩阵乘法即可。左右是等价的。通过右边这样做，我们就只需要学习一个权重矩阵，而不用去学习两个分别装着权重和偏差的矩阵了. 损失函数假设对于输入的一个图片,我们通过评分函数计算出它对于每一个分类的得分,那么怎么去评价他的好坏呢？这时候我们需要使用损失函数来衡量我们对该评分的满意程度。我们可以通过调整参数W来使得评分函数的结果(最高分所在类别)与训练集真实标签是一致的。一般来说，评分函数输出结果与真实结果差别越大，损失函数值越大，反之越小。 多分类支持向量机(multi-SVM)的损失函数 我们定义其损失函数为 对于第i个输入数据 $ Li = \sum{j \ne yi} max(0,s_j - s{yi} + \Delta)$ 我们这里定义每一类的得分为s,$s_j$即为对于第i个输入数据经过评分函数后在第j个分类的得分,$s{y_i}$表示对于第i个输入数据经过评分函数，在正确分类处的得分。$\Delta$ 这里是一个边界值,具体的意思就是,Multi-SVM 我不关心正确的分类得分,我关心的是对于我在正确分类处所得的分数,是否比我在错误分类处所得的分数高,而且要出一定边界值$\Delta $，如果高于$\Delta$ 那么我便不管它,否则我就需要计算我的损失举例:用一个例子演示公式是如何计算的。假设有3个分类，并且得到了分值s=[13,-7,11]。其中第一个类别是正确类别，即$y_i = 0$同时设$\Delta = 10$ 上面的公式是将所有不正确分类$（j\not=y_i）$加起来，所以我们得到两个部分：$L_i = max(0,-7-13+10) + max(0,11-13+10)$ 可以看到第一个部分结果是0，这是因为[-7-13+10]得到的是负数，经过max(0,-)函数处理后得到0。这一对类别分数和标签的损失值是0，这是因为正确分类的得分13与错误分类的得分-7的差为20，高于边界值10。而SVM只关心差距至少要大于10，更大的差值还是算作损失值为0。第二个部分计算[11-13+10]得到8。虽然正确分类的得分比不正确分类的得分要高（13&gt;11），但是比10的边界值还是小了，分差只有2，这就是为什么损失值等于8。简而言之，SVM的损失函数想要正确分类类别$y_i$的分数比不正确类别分数高，而且至少要高$\Delta$。如果不满足这点，就开始计算损失值。 还必须提一下的属于是关于0的阀值：$max(0,-)$函数，它常被称为折叶损失（hinge loss）。有时候会听到人们使用平方折叶损失SVM（即L2-SVM），它使用的是$max(0,-)^2$，将更强烈（平方地而不是线性地）地惩罚过界的边界值。不使用平方是更标准的版本，但是在某些数据集中，平方折叶损失会工作得更好。可以通过交叉验证来决定到底使用哪个。 梯度推导: 这里需要矩阵求导公式: 代码实现: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465""" 不加入正则化的多分类SVM的损失函数的实现,采用三种方法,一是两层循环,二是一层循环,三是不用循环"""import numpy as npdef L_i(x,y,W,reg): """ unvectorized version. Compute the multiclass svm loss for a single example (x,y) - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10) with an appended bias dimension in the 3073-rd position (i.e. bias trick) - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10) - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10) """ delta = 1.0 #delta 为1.0 一般是安全的 # f(xi,W) = Wx,x 为 3073 * 1,W = 10 * 3073 scores = np.dot(W,x) correct_class_score = scores[y] D = W.shape[0] loss_i = 0.0 for j in xrange(D): if j == y: # 对所有错误的分类进行Iterate,j == y 为正确classification,so skip it. continue """ multi SVM 只关注正确评分的损失,即要求正确分类所获得的分数要大于错误分类所获得分数,且至少要大delta, 否则就计算loss. """ loos_i += max(0,scores[j] - correct_class_score[y] + delta) return loss_i def L_i_vectorized(x,y,W): """ A faster half-vectorized implementation. half-vectorized refers to the fact that for a single example the implementation contains no for loops, but there is still one loop over the examples (outside this function) 需要一个循环对每个 test 进行call L_i_vectorized. """ delta = 1.0 scores = np.dot(W,x) # W = 10 * 3073 , x = 3073 * 1 margins = np.maximum(0,scores - scores[y] + delta) # 矩阵运算 margins[y] = 0 loss_i = np.sum(margins) return loss_idef L(X,y,W): """ fully-vectorized implementation : - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10) - y is array of integers specifying correct class (e.g. 50,000-D array) - W are weights (e.g. 10 x 3073) 不用循环实现,这里可以利用numpy的broadcasting机制. """ delta = 1.0 scores = np.dot(W,X)# 10 * 50000 num_train = X.shape[1] num_class = W.shape[0] scores_correct = scores[y,np.arange(num_train)] # 1 * 50000 scores_correct = np.reshape(scores_correct,(1,num_train)) # 1 * 50000 # scores is 10 * 50000, broadcasting makes scores_correct to 10 * 50000 margins = scores - scores_correct + delta margins = np.maximum(margins,0) margins[y,np.arange(num_train)] = 0 # 正确分类不计算 loss_i = np.sum(margins) return loss_i 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195"""我们有评分函数为 F = WX. 怎么求W呢？还是使用gradient descent.初始给W随机比较小的值,如: &#123; 生成一个很小的SVM随机权重矩阵 真的很小，先标准正态随机然后乘0.0001 &#125; W = np.random.randn(3073, 10) * 0.0001 一边计算损失函数,一遍计算W的梯度dW，对于损失函数loss关于W的偏导数,我们这里还是使用了矩阵求导公式"""def svm_loss_naive(W,X,y,lamda): """ 使用循环实现的SVM loss 函数。 输入维数为D，有C类，我们使用N个样本作为一批输入。 输入: -W: 一个numpy array，形状为 (D, C) ，存储权重。 -X: 一个numpy array, 形状为 (N, D)，存储一个小批数据。 -y: 一个numpy array，形状为 (N,)， 存储训练标签。y[i]=c 表示 x[i]的标签为ｃ，其中 0 &lt;= c &lt;= C 。 -reg: float, 正则化强度。 输出一个tuple: - 一个存储为float的loss - 权重W的梯度，和W大小相同的array """ # delta 设为1.0一般比较安全 delta = 1.0 # 梯度初始化 dW = np.zeros(W.shape) #计算损失和梯度 num_class = W.shape[1] num_trian = X.shape[0] loss = 0. for i in range(num_trian): scores = np.dot(X[i],W) score_correct = scores[y[i]] for j in range(num_class): if j == y[i]: continue margin = scores[j] - score_correct + delta if margin &gt; 0: loss += margin dW[:,y[i]] += -X[i,:].T dW[:,j] += X[i,:].T dW /= num_train loss /= num_trian # 获得损失函数的均值 dW += lamda * W # 梯度要加入正则化部分 return loss,dW"""偶尔会出现梯度验证时候，某个维度不一致，导致不一致的原因是什么呢？这是我们要考虑的一个因素么？梯度验证失败的简单例子是？ 提示：SVM 损失函数没有被严格证明是可导的． 可以看上面的输出结果，turn on reg前有一个是3.954297e-03的loss明显变大．参考答案 解析解和数值解的区别，数值解是用前后2个很小的随机尺度(比如0.00001)进行计算，当Loss不可导的，两者会出现差异。比如SyiSyi刚好比SjSj大１．"""def svm_vectorized(W,X,y,lamda): """ 使用向量来实现 """ delta = 1.0 loss = 0.0 dW = np.zeros(W.shape) scores = np.dot(X,W) # num_train * num_class num_class = W.shape[1] num_train = X.shape[0] scores_correct = scores[np.arange(num_train),y] # 1 * num_train scores_correct = np.reshape(scores_correct,(num_train,-1)) # num_train * 1 margins = scores - scores_correct + delta margins = np.maximum(0,margis) margins[np.arange(num_train),y] = 0 loss += np.sum(margins) / num_train loss += 0.5 * lamda * np.sum(W * W) """ 使用向量计算SVM损失函数的梯度,把结果保存在dW """ margins[margins &gt; 0] = 1 row_sum = np.sum(margins,axis = 1) margins[np.arange(num_train),y] = -row_sum # 1 by N dW += np.dot(X.T,margins) / num_train + lamda * W return loss , dW"""上面的代码都是来计算损失函数的,现在我们采用SGD(随机梯度下降)来最小化损失函数"""def train_SGD(self,X,y,alpha = 1e-3,lamda = 1e-5,numIterations = 1000,batch_size = 200,verbose = False): """Train this linear classifier using stochastic gradient descent.使用随机梯度下降来训练这个分类器。输入:-X:一个 numpy array，形状为 (N, D), 存储训练数据。 共N个训练数据， 每个训练数据是N维的。-Y:一个 numpy array， 形状为(N,), 存储训练数据的标签。y[i]=c 表示 x[i]的标签为ｃ，其中 0 &lt;= c &lt;= C 。-learning rate： float， 优化的学习率。-reg：float， 正则化强度。-num_iters: integer， 优化时训练的步数。-batch_size: integer, 每一步使用的训练样本数。-verbose: boolean，若为真，优化时打印过程。输出：一个存储每次训练的损失函数值的list。""" num_train ,dim = X.shape num_class = np.max(y) + 1# 假设y的值是0...K-1，其中K是类别数量 if self.W is None: # 简易初始化,给W初始化比较小的值 self.W = 0.001 * np.random.randn(dim,num_class)# 使用随机梯度下降SGD,优化W loss_history = [] for it in range(numIterations): x_batch = None y_batch = None """ 从训练集中采样batch_size个样本和对应的标签，在这一轮梯度下降中使用。 把数据存储在 X_batch 中，把对应的标签存储在 y_batch 中。 """ batch_index = np.random.choice(num_train,batch_size) x_batch = X[batch_index,:] y_batch = y[batch_index] # 用随机产生的样本,求损失函数,以及梯度。 loss,gradient = self.svm_vectorized(X,y,W,lamda) loss_history.append(loss) # GradientDescent 更新W self.W = self.W - alpha * gradient return loss_history"""使用验证集去调整超参数（正则化强度lamda,和学习率alpha)# 使用验证集去调整超参数（正则化强度和学习率），你要尝试各种不同的学习率# 和正则化强度，如果你认真做，将会在验证集上得到一个分类准确度大约是0.4的结果。# 设置学习率和正则化强度，多设几个靠谱的，可能会好一点。# 可以尝试先用较大的步长搜索，再微调。"""learning_rates = [2e-7, 0.75e-7,1.5e-7, 1.25e-7, 0.75e-7]regularization_strengths = [3e4, 3.25e4, 3.5e4, 3.75e4, 4e4,4.25e4, 4.5e4,4.75e4, 5e4]# 结果是一个词典，将形式为(learning_rate, regularization_strength) 的tuples 和形式为 (training_accuracy, validation_accuracy)的tuples 对应上。准确率就简单地定义为数据集中点被正确分类的比例。results = &#123;&#125;best_val = -1 # 出现的正确率最大值best_svm = None # 达到正确率最大值的svm对象################################################################################# 任务: ## 写下你的code ,通过验证集选择最佳超参数。对于每一个超参数的组合，# 在训练集训练一个线性svm，在训练集和测试集上计算它的准确度，然后# 在字典里存储这些值。另外，在 best_val 中存储最好的验证集准确度，# 在best_svm中存储达到这个最佳值的svm对象。## 提示：当你编写你的验证代码时，你应该使用较小的num_iters。这样SVM的训练模型# 并不会花费太多的时间去训练。当你确认验证code可以正常运行之后，再用较大的# num_iters 重跑验证代码。################################################################################for rate in learning_rates: for regular in regularization_strengths: svm = LinearSVM() svm.train(X_train, y_train, learning_rate=rate, reg=regular, num_iters=1000) y_train_pred = svm.predict(X_train) accuracy_train = np.mean(y_train == y_train_pred) y_val_pred = svm.predict(X_val) accuracy_val = np.mean(y_val == y_val_pred) results[(rate, regular)]=(accuracy_train, accuracy_val) if (best_val &lt; accuracy_val): best_val = accuracy_val best_svm = svm################################################################################# 结束 #################################################################################for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print ('lr %e reg %e train accuracy: %f val accuracy: %f' % (lr, reg, train_accuracy, val_accuracy))print ('best validation accuracy achieved during cross-validation: %f' % best_val)"""随堂练习 2:描述你的SVM可视化图像，给出一个简单的解释 参考答案: 将学习到的权重可视化，从图像可以看出，权重是用于对原图像进行特征提取的工具，与原图像关系很大。很朴素的思想，在分类器权重向量上投影最大的向量得分应该最高，训练样本得到的权重向量，最好的结果就是训练样本提取出来的共性的方向，类似于一种模板或者过滤器。""" SoftMax 上面我们有提到强大的图像分类函数都有一个评分函数以及一个损失函数。Softmax的评分函数与多分类SVM的评分函数相同，不同的是在于Loss Function.SoftMax可以理解为是逻辑回归分类器面对多个分类的一般化归纳。与SVM不同,softmax的输出（归一化的分类概率）更加直观，是从概率上来解释的。评分函数$f(xi,W) = Wx_i $ 不变,将这些评分值视为未归一化的对数概率，将折叶损失（hinge loss）变为交叉熵损失（cross-entropy loss）公式如下: $L_i = -log\frac{e^{f{yi}}}{\sum{j} e^{fj}} $ 或者: $ L_i = -f{yi} + log(\sum{j} e^{fj}) $ 在上式中，使用$f_j$来表示分类评分向量f中的第j个元素。整个数据集的损失值是数据集中所有样本数据的损失值$L_i$的均值与正则化损失R(W)之和。其中函数 $f_j(z) = \frac{e^{z_j}}{\sum{k} e^{z_k}} $被称作softmax 函数.其输入值是一个向量，向量中元素为任意实数的评分值（z中的），函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。所以，包含softmax函数的完整交叉熵损失看起唬人，实际上还是比较容易理解的。 概率论解释：先看下面的公式： 可以解释为是给定图像数据$x_i$，以W为参数，分配给正确分类标签y_i的归一化概率。为了理解这点，请回忆一下Softmax分类器将输出向量f中的评分值解释为没有归一化的对数概率。那么以这些数值做指数函数的幂就得到了没有归一化的概率，而除法操作则对数据进行了归一化处理，使得这些概率的和为1。从概率论的角度来理解，我们就是在最小化正确分类的负对数概率，这可以看做是在进行最大似然估计（MLE）。该解释的另一个好处是，损失函数中的正则化部分R(W)可以被看做是权重矩阵W的高斯先验，这里进行的是最大后验估计（MAP）而不是最大似然估计。 让人迷惑的命名规则：精确地说，SVM分类器使用的是折叶损失（hinge loss），有时候又被称为最大边界损失（max-margin loss）。Softmax分类器使用的是交叉熵损失（corss-entropy loss）。Softmax分类器的命名是从softmax函数那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。 SVM和Softmax的比较 下图有助于区分这 Softmax和SVM这两种分类器： 针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量f（本节中是通过矩阵乘来实现）。不同之处在于对f中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的对数概率，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。 在实际使用中，SVM和Softmax经常是相似的：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加“局部目标化（local objective）”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。那么一个SVM（\Delta =1）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。 对于softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。 梯度推导： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788"""f = WX + bL_i = -log(np.exp(e^f_i) / np.sum(np.exp(e^f_j))) (for j = 1 ~ n)主要用到交叉熵等含义"""import numpy as npfrom random import shuffledef softmax_loss_naive(W,X,y,lamda): """ Softmax loss function, naive implementation (with loops) Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - lamda: (float) regularization strength 正则化强度 Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ loss = 0.0 dW = np.zeros(W.shape) ############################################################################# # TODO: Compute the softmax loss and its gradient using explicit loops. # # Store the loss in loss and the gradient in dW. If you are not careful # # here, it is easy to run into numeric instability. Don't forget the # # regularization! # ############################################################################# num_train = X.shape[0] num_calss = W.shape[1] for i in range(num_train): scores = X[i].dot(W) scores -= np.maximum(scores) # 防止指数太大而导致结果溢出,这里将所有值都缩小的接近0. correct_socres = scores[y[i]] exp_sum = np.sum(np.exp(scores)) loss += np.log(exp_sum) - correct_socres dW[:,y[i]] += (np.exp(scores[y[i]]) / exp_sum) * X[i].T - X[i].T for j in range(num_calss): if j == y[i]: continue dW[:,j] += np.exp(scores[j]) / exp_sum * X[i].T loss /= num_train loss += 0.5 * lamda * np.sum(W * W) dW /= num_train dW += lamda * W return loss,dWdef softmax_loss_vectorized(W,X,y,lamda): """ calculate the loss and dW without any loops """ loss = 0.0 dW = np.zeros(W.shape) num_train = X.shape[0] num_calss = W.shape[1] scores = np.dot(X,W) score_max = np.reshape(np.max(scores,axis = 1),(num_train,1)) #(N,1) scores -= scores_max exp_scores = np.exp(scores) sum_exp_socres = np.sum(scores,axis = 1) # 得到每个样本的 exp 和 ，N correct_socres = scores[np.arange(num_train),y] # 每个正确分类得到的预测分数 correct_socres = np.reshape(correct_socres,(num_train,-1)) loss += np.sum(np.log(sum_exp_socres)) - np.sum(correct_socres) exp_socres = exp_scores / sum_exp_socres[:,np.newaxis] # broadcasting 梯度 要/ exp_sum #for i in range(num_train): # dW += exp_scores[i] * X[i].T[:,np.newaxis] # newaxis 增加一个维度 #dW[:,y[i]] -= X[i].T # 不用循环 y_mid = np.zeros(scores.shape)# num_train * 3073 y_mid[np.arange(num_train),y] = 1.0 dW += np.dot(X.T,exp_scores - y_mid) loss /= num_train loss += 0.5 * lamda * np.sum(W * W) dW /= num_train dW += lamda * W return loss,dW]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer 数组中的逆序对]]></title>
    <url>%2F2018%2F07%2F03%2F2018-7-3%2F</url>
    <content type="text"><![CDATA[题意 在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007。 题目保证输入的数组中没有的相同的数字 数据范围： 对于%50的数据,size]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer 整数中1出现的次数（从1到n整数中1出现的次数）]]></title>
    <url>%2F2018%2F07%2F02%2F2018-7-2-2%2F</url>
    <content type="text"><![CDATA[题意 求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数。 思路 比较好想的一种解题方法就是我们可以发现一定的规律 1~9 个位有1个1 10~19 十位有10个1 100~199 百位有100个1 … 所以我们这里可以个位，十位，百位…这样分开计算。比如当n = 123时,对于个位的1,除了有123 / 10 = 12个,还有他自己的个位1个1,一共13个。其他位置同理。但是有一个需要注意，假设n = 13,这时候你讨论十位1的个数，就不能直接 + 10（10~19）,而是需要+4.所以我们在计算时需要考虑这一条件。 123456789101112131415161718192021class Solution &#123;public: int NumberOf1Between1AndN_Solution(int n) &#123; int sum = 0; int t1,t2,k1 = 1,k2 = 10,flag = 0; while(!flag) &#123; t1 = n / k2; t2 = n % k2; if(t1 == 0) flag = 1; sum += t1 * k1; if(t2 &gt; 2 * k1 - 1) sum += k1; else if(t2 &gt;= k1) sum += t2 - k1 + 1; k1 = k2; k2 *= 10; &#125; return sum; &#125;&#125;; 另一种比较好的方法就是数位dp,我们用dp[i][j]表示从1~以i开头的长度为j位的数中1的个数。那么可以得到如下转移方程: 当i == 1: dp[i][j] = dp[9][j-1]*2 + pow(10,i - 1); 否则: dp[i][j] = dp[9][j-1] + dp[i-1][j]; 我们预处理出以上的,即可以按照如下方式计算: 假设n = 2345， 那么结果为: 以1开头中1的个数 + 以2打头的长度为3的 + 以3打头长度为2 … 注意以1打头时,还应加上n % pow(i - 1) + 1个数。（末尾的数+ 0）&lt;/p&gt;1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include&lt;bits/stdc++.h&gt;using namespace std;class Solution &#123;public: int NumberOf1Between1AndN_Solution(int n) &#123; int dp[10][20]; for(int i = 1;i &lt;= 9;++i) dp[i][1] = 1; long long tmp = 10; for(int j = 2;j &lt;= 10;++j) &#123; for(int i = 1;i &lt;= 9;++i) &#123; if(i == 1) dp[i][j] = dp[9][j - 1] * 2 + tmp; else dp[i][j] = dp[9][j - 1] + dp[i - 1][j]; &#125; tmp *= 10; &#125; int bit[13],cnt = 0,x = n; while(n) &#123; bit[++cnt] = n % 10; n /= 10; &#125; int sum = 0; for(int i = cnt;i &gt;= 1;--i) &#123; if(bit[i] == 0) continue; if(bit[i] == 1) sum += dp[9][i - 1] + x % (int)pow(10,i - 1) + 1; else sum += dp[bit[i] - 1][i]; tmp *= 10; &#125; return sum; &#125;&#125;;int main()&#123; Solution w; int n; while(cin &gt;&gt; n) &#123; cout &lt;&lt; w.NumberOf1Between1AndN_Solution(n) &lt;&lt; endl; &#125; return 0;&#125;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer 最小的k个数]]></title>
    <url>%2F2018%2F07%2F02%2F2018-7-2%2F</url>
    <content type="text"><![CDATA[题意 输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 思路 找最小的k个数,我们首先想到的最简单的方法就是对n个数排序,然后找出最小的k个即可。复杂度O(nlogn)。也可以维护一个大小为k的堆,复杂度为O(nlogk)。 这里我们讲一个O(n)的方法。采用快排的划分(Partition)思想，利用哨兵每次把数组分成两部分，左边比哨兵小，右边都比他大。同时比较哨兵的位置(实质是看元素的个数)，即若左面都比k小,那么满足条件的一定在左边,否则右面也有部分满足条件。最后前k个就是我们需要的。注意这里前k个不一定有序,只是相对较小的k个 上述方法也可以用在O(n)寻找数组最小的第k个数.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/*输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。*/#include&lt;bits/stdc++.h&gt;using namespace std;class Solution &#123;public: vector&lt;int&gt; GetLeastNumbers_Solution(vector&lt;int&gt; input, int k) &#123; int len = input.size(); if(len == 0 || k &gt; len) return vector&lt;int&gt;(); if(len == k) return input; int beg = 0,en = input.size() - 1; int index = Partition(input,beg,en); while(index != (k - 1) &amp;&amp; k &gt; 0) &#123; if(index &gt; k - 1) &#123; en = index - 1; index = Partition(input,beg,en); &#125; else &#123; beg = index + 1; index = Partition(input,beg,en); &#125; &#125; vector&lt;int&gt; res(input.begin(),input.begin() + k); return res; &#125;private: int Partition(vector&lt;int&gt; &amp;input,int low,int height) &#123; int beg = low,ed = height; int val = input[beg]; while(beg &lt; ed) &#123; while(beg &lt; ed &amp;&amp; val &lt;= input[ed]) --ed; input[beg] = input[ed]; while(beg &lt; ed &amp;&amp; val &gt;= input[beg]) ++beg; input[ed] = input[beg]; &#125; input[beg] = val; return beg; &#125;&#125;;int main()&#123; return 0;&#125;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer 复杂链表的复制]]></title>
    <url>%2F2018%2F06%2F20%2F2018-6-20-3%2F</url>
    <content type="text"><![CDATA[题目描述输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） 思路:这题一开始思想有点混乱啊,差点忘记了,复制的结点也要指向复制的结点。这个题我们有两种方法,一种是哈希,一种是先复制,再指向,再拆分。 哈希先说下哈希的方法,对于这个复杂链表的复制,next结点我们比较好处理,创建完成后直接指向即可。主要不好处理的是关于random的结点,所以这里我们可以采用一下哈希。第一遍先把所有的复制结点通过next连接起来,这个过程中哈希记录一下该点的复制结点,（也就是哈希的值就是该结点的复制结点）。然后再从头遍历一遍存一下random即可。123456789101112131415161718192021222324252627282930313233class Solution&#123;public: RandomListNode* Clone(RandomListNode* pHead) &#123; mp.clear(); RandomListNode *cloneHead = NULL; RandomListNode *pNode = pHead; RandomListNode *clonepNode = NULL; while(pNode) &#123; RandomListNode *newNode = new RandomListNode(pNode -&gt; label); if(cloneHead == NULL) cloneHead = newNode; else clonepNode -&gt; next = newNode; clonepNode = newNode; mp[pNode] = newNode; pNode = pNode -&gt; next; &#125; pNode = pHead,clonepNode = cloneHead; while(pNode) &#123; clonepNode -&gt; random = mp[pNode -&gt; random]; clonepNode = clonepNode -&gt; next; pNode = pNode -&gt; next; &#125; return cloneHead; &#125;private: map&lt;RandomListNode*,RandomListNode*&gt;mp;&#125;; 第二种方法偷个图: 也就是先复制,再处理random,最后再拆分为两个链表12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Solution &#123;public: RandomListNode* Clone(RandomListNode* pHead) &#123; CloneNode(pHead); ClonePoint(pHead); return RcreateList(pHead); &#125;private: void CloneNode(RandomListNode * pHead) &#123; RandomListNode *pNode = pHead; while(pNode) &#123; RandomListNode *newNode = new RandomListNode(pNode -&gt; label); newNode -&gt; next = pNode -&gt; next; pNode -&gt; next = newNode; pNode = newNode -&gt; next; &#125; &#125; void ClonePoint(RandomListNode *pHead) &#123; RandomListNode *pNode = pHead; while(pNode) &#123; RandomListNode *nxt = pNode -&gt; next; if(pNode -&gt; random) nxt -&gt; random = pNode -&gt; random -&gt; next; pNode = nxt -&gt; next; &#125; &#125; RandomListNode *RcreateList(RandomListNode *pHead) &#123; RandomListNode *pNode = pHead; RandomListNode *pCloneHead = NULL; RandomListNode *pCloneNode = NULL; if(pNode != NULL) &#123; pCloneHead = pCloneNode = pNode -&gt; next; pNode -&gt; next = pCloneHead -&gt; next; pNode = pNode -&gt; next; &#125; while(pNode) &#123; pCloneNode -&gt; next = pNode -&gt; next; pCloneNode = pCloneNode -&gt; next; pNode -&gt; next = pCloneNode -&gt; next; pNode = pNode -&gt; next; &#125; return pCloneHead; &#125;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer 二叉搜索树的后序遍历序列]]></title>
    <url>%2F2018%2F06%2F20%2F2018-6-20-2%2F</url>
    <content type="text"><![CDATA[题目描述 输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 思路我们知道后序遍历的顺序为”左右根”，也就是说序列的最后一个一定为根节点.又因为为BST,所以可以知道他的左子树的值一定都比他小,右子树一定都比他大。所以我们可以根据这个性质来划分一下左右子树,然后递归下去直到不满足条件为止,或全部都满足条件。这样就可以判断出来了。123456789101112131415161718class Solution &#123;public: bool VerifySquenceOfBST(vector&lt;int&gt; sequence) &#123; if(sequence.size() == 0) return false; return check(sequence,0,sequence.size() - 1); &#125;private: bool check(vector&lt;int&gt; arr,int l,int r) &#123; if(l &gt;= r) return true; int i = l; while(i &lt;= r &amp;&amp; arr[i] &lt; arr[r]) ++i; int j = i; while(j &lt;= r &amp;&amp; arr[j] &gt; arr[r]) ++j; if(j &lt; r) return false; return check(arr,l,i - 1) &amp;&amp; check(arr,i,r - 1); &#125;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer 包含min函数的栈]]></title>
    <url>%2F2018%2F06%2F20%2F2018-6-20-1%2F</url>
    <content type="text"><![CDATA[题目描述 定义栈的数据结构，请在该类型中实现一个能够得到栈最小元素的min函数。(这里我们要求时间min函数的时间复杂度为O(1)) 思路: 这里我们可以采用两个栈sta1，sta2。sta1用来实现真正的入栈出栈,sta2用来实现min函数. 关于min函数的实现方法,其实本质上是按照入栈顺序维护一个递减的栈。即如果很早入栈的有比当前要入栈的这个数的值小的话,那么要入栈的这个值对最小值min是没有影响的,所以没必要保存在sta2中。当然sta2中的每个数所能影响的范围是一段区间,这个区间伴随着它出栈也就结束了.也就是出栈的时候要判断sta1的栈顶和sta2栈顶一致则sta2也出栈,最小值变为下一个。。以此类推。123456789101112131415161718192021class Solution &#123;public: void push(int value) &#123; sta1.push(value); if(sta2.empty() || sta2.top() &gt;= value) sta2.push(value); &#125; void pop() &#123; if(sta1.top() == sta2.top()) sta2.pop(); sta1.pop(); &#125; int top() &#123; return sta1.top(); &#125; int min() &#123; return sta2.top(); &#125;private: stack&lt;int&gt;sta1,sta2;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构知识点总结]]></title>
    <url>%2F2018%2F06%2F19%2F1%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[线索二叉树 利用二叉链表中的空指针域，存放指向结点在某种遍历次序下的前驱和后继结点的指针（这种附加的指针称为”线索”）。因此，提出了一种方法，利用原来的空链域存放指针，指向树中其他结点。这种指针称为线索。 记ptr指向二叉链表中的一个结点，以下是建立线索的规则： （1）如果ptr-&gt;lchild为空，则存放指向中序遍历序列中该结点的前驱结点。这个结点称为ptr的中序前驱； （2）如果ptr-&gt;rchild为空，则存放指向中序遍历序列中该结点的后继结点。这个结点称为ptr的中序后继； 显然，在决定lchild是指向左孩子还是前驱，rchild是指向右孩子还是后继，需要一个区分标志的。因此，我们在每个结点再增设两个标志域ltag和rtag，注意ltag和rtag只是区分0或1数字的布尔型变量，其占用内存空间要小于像lchild和rchild的指针变量。结点结构如下所示。 其中： （1）ltag为0时指向该结点的左孩子，为1时指向该结点的前驱； （2）rtag为0时指向该结点的右孩子，为1时指向该结点的后继； 十字链表 十字链表是有向图的一种链式存储结构。 十字链表是为了便于求得图中顶点的度（出度和入度）而提出来的。它是综合邻接表和逆邻接表形式的一种链式存储结构。 在十字链表存储结构中，有向图中的顶点的结构如下所示： 其中data表示顶点的具体数据信息，而firstIn则表示指向以该顶点为弧头的第一个弧节点。而firstOut则表示指向以该顶点为弧尾的第一个弧节点。为了表示有向图中所有的顶点，采用一个顶点数组存储每一个结点 另外，在十字链表存储结构中，有向图中的每一条弧都有一个弧结点与之对应，具体的弧结点结构如下所示： 其中的tailVex表示该弧的弧尾顶点在顶点数组xList中的位置，headVex表示该弧的弧头顶点在顶点数组中的位置。hLink则表示指向弧头相同的下一条弧，tLink则表示指向弧尾相同的下一条弧。 从十字链表的数据结构来看，每一个顶点对应两个链表：以该顶点为弧尾的弧结点所组成的链表以及以该顶点为弧头的弧结点所组成的链表。 如下图所示的一个有向图： 其对应的顶点以及弧结点如下所示。拿结点A说明，该结点对应两个链表（绿色和黄色标记的）。绿色链表表示以结点Ａ为弧头的弧组成的链表。黄色链表表示以结点Ａ为弧尾的弧组成的链表。 哈夫曼编码 1）树的路径长度 树的路径长度是从树根到树中每一结点的路径长度之和。在结点数目相同的二叉树中，完全二叉树的路径长度最短. （2）树的带权路径长度(Weighted Path Length of Tree，简记为WPL) 结点的带权路径长度：结点到树根之间的路径长度与该结点上权的乘积。 树的带权路径长度(Weighted Path Length of Tree)：定义为树中所有叶结点的带权路径长度之和 $ WPL = \sum_{i = 1}^n w_il_i $ 其中： n表示叶子结点的数目 wi和li分别表示叶结点ki的权值和根到结点ki之间的路径长度。 树的带权路径长度亦称为树的代价。 我们称判定过程最优的二叉树为哈夫曼树，又称最优二叉树 [参考这个博客](https://blog.csdn.net/shuangde800/article/details/7341289) B-树 B树，也就是英文中的B-Tree，一个 m 阶的B树满足以下条件：(1)树中每个结点至多有m棵子树(2)若根节点不是叶子结点,则至少有两棵子树(3)除根之外的所有非终端结点至少有ceil(m/2)棵子树(4)至少有 ceil(m/2) - 1个关键字,最多有m - 1个关键字.(5)所有叶子结点在同一层 如一棵四阶B-树，其深度为4.B-树的深度 $ log_{ceil(m/2)} (\frac{N + 1}{2}) + 1$ N为关键字个数 B-树主要应用在文件系统 为了将大型数据库文件存储在硬盘上以减少访问硬盘次数为目的 在此提出了一种平衡多路查找树——B-树结构 由其性能分析可知它的检索效率是相当高的]]></content>
      <tags>
        <tag>各种基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法知识点总结]]></title>
    <url>%2F2018%2F06%2F18%2F1%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[NP完全问题 P类问题： 所有可以在多项式时间内求解的判定问题构成P类问题.此类问题的复杂度是此类问题的一个实例的规模n的多项式函数。比如排序问题，求最短路径问题等。 P类问题是NP问题的子集，原因是P类问题既然能在多项式时间内求解，也必然能在多项式时间在验证它的解，满足NP类问题的定义。 NP类问题： 所有的非确定性多项式时间可解的判定问题构成NP类问题. 有些问题很难找到多项式时间的解法（也许根本就不存在），但是如果给出了该问题的一个解，我们可以在多项式时间内判断这个解是否正确，比如对于哈密尔顿回路问题，如果给出一个任意的回路，我们可以很容易的判断出该回路是否是哈密尔顿回路. 著名的NP难问题 最长路径问题:最长路径问题是指在给定的图中找出长度最长的道路.]]></content>
      <tags>
        <tag>各种基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer-树的子结构]]></title>
    <url>%2F2018%2F06%2F18%2F2018-6-18-3%2F</url>
    <content type="text"><![CDATA[题目: 输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 思路: 这题坑了我好久。原本觉得这题和我做过的一个题比较像，但是我忘记了一种树的遍历方法是没有办法确定唯一的一棵树的,只有先序遍历+中序,或者中序+后序,但是我如果把树中的空的叶节点用一个树中没出现过的字符代替,(e.g. ‘#’)其实是可以对比两个树是否一样的。 但是我又入了另一个坑,题目中要求的是B是否是A的子结构,也就是说B并不是A一个完整的子树,所以这种方法也行不通了。 最后我们只能从A的根节点开始找,当结点和B的根节点相同时,在以此为根节点递归判断是否是A的一个子结构。123456789101112131415161718192021222324252627282930struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;class Solution &#123;public: bool HasSubtree(TreeNode* pRoot1, TreeNode* pRoot2) &#123; if(pRoot2 == NULL || pRoot1 == NULL) return false; bool flag = false; if(pRoot1 -&gt; val == pRoot2 -&gt; val) flag |= isSubtree(pRoot1,pRoot2); flag |= HasSubtree(pRoot1 -&gt; left,pRoot2); flag |= HasSubtree(pRoot1 -&gt; right,pRoot2); return flag; &#125;private: bool isSubtree(TreeNode *pRoot1,TreeNode* pRoot2) &#123; if(pRoot2 == NULL) return true; if(pRoot1 == NULL) return false; if(pRoot1 -&gt; val != pRoot2 -&gt; val) return false; return isSubtree(pRoot1 -&gt; left,pRoot2 -&gt; left) &amp;&amp; isSubtree(pRoot1 -&gt; right,pRoot2 -&gt; right); &#125;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计蒜客-贝壳找房计数比赛]]></title>
    <url>%2F2018%2F06%2F18%2F2018-6-18-2%2F</url>
    <content type="text"><![CDATA[题目链接 思路: 让我们找出s中所有去重全排列的t出现的次数.因为全排列了,所以说白了s中的字符就是重组罢了。 这里我们可以每次固定t在s中出现的位置,那么剩下的就是一个组合数了,假设现在还剩下n个位置可放字符串,其中a个’a’,b个’b’….z个’z’. 那么很容易得到公式: $\frac{n!}{a!b!c!d!…z!}$，这里取模需要求个逆元，而且这只是针对t在s中的一个位置,最后求个和即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include&lt;bits/stdc++.h&gt;using namespace std;const int maxn = 1e5+5;typedef long long ll;const ll mod = 1e9+7;ll qmod(ll a,ll b)&#123; ll res = 1; while(b) &#123; if(b &amp; 1) res = (res * a) % mod; a = a * a % mod; b &gt;&gt;= 1; &#125; return res;&#125;ll fac[maxn],inv[maxn];int boo[30];void init()&#123; fac[0] = inv[0] = 1; for(int i = 1;i &lt; maxn;++i) &#123; fac[i] = fac[i - 1] * i % mod; inv[i] = qmod(fac[i],mod - 2) % mod; &#125; return ;&#125;string s,t;int main()&#123; init(); int _; cin &gt;&gt; _; while(_--) &#123; cin &gt;&gt; s &gt;&gt; t; memset(boo,0,sizeof boo); for(int i = 0;i &lt; s.size();++i) boo[s[i] - 'a']++; for(int i = 0;i &lt; t.size();++i) boo[t[i] - 'a']--; int len = t.size(),n = s.size(); ll ans = fac[n - len],tmp = 0,res = n - len; for(int i = 0;i &lt; 26;++i) &#123; if(boo[i] != 0) &#123; //cout &lt;&lt; i &lt;&lt; ' ' &lt;&lt; boo[i] &lt;&lt; endl; ans = ans * inv[boo[i]] % mod; &#125; &#125; // cout &lt;&lt; ans &lt;&lt; endl; tmp = ans * (n - len + 1) % mod; cout &lt;&lt; tmp &lt;&lt; endl; &#125; return 0;&#125;/*aabbab*/ PS: 这里我还在想会不会有重复的情况,就像上面的样例.t在[1,2]的时候[3,4]位置可以是ab或者ba,在[3,4]的时候[1,2]位置可能是ab和ba,这就有两个abab,这好像不是s的去重全排列啊,其实不然,我们在对[1,2]的ab计数的时候是没有计数[3,4]位置的,[3,4]也是。这样就比较显然了。]]></content>
      <tags>
        <tag>数学思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计蒜客-贝壳找房函数最值(重载sort)]]></title>
    <url>%2F2018%2F06%2F18%2F2018-6-18-1%2F</url>
    <content type="text"><![CDATA[题目链接 思路: 比较水的一道题目,但是当时有点二虎啊。 我们很容易找到如下关系: 若: $ a_2(a_1x+b_1) + b_2 &gt; a_1(a_2x+b_2) + b_1 $ 整理可以得到: $a_2b_1 + b_2 &gt; a_1b_2 + b_1 $.也就是说我们要想使得最后的值最大,就需要把这个关系小的放在内部。所以每次按照$ a_2b_1 + b_2 &gt; a_1b_2 + b_1 $排序即可1234567891011121314151617181920212223242526272829303132#include&lt;bits/stdc++.h&gt;using namespace std;const int maxn = 1e4+5;struct node&#123; int a,b;&#125;s[maxn];bool cmp(node x,node y)&#123; return x.a * y.b + x.b &lt; y.a * x.b + y.b;&#125;int main()&#123; int _,n,x; cin &gt;&gt; _; while(_--) &#123; cin &gt;&gt; n &gt;&gt; x; for(int i = 1;i &lt;= n;++i) scanf("%d",&amp;s[i].a); for(int i = 1;i &lt;= n;++i) scanf("%d",&amp;s[i].b); sort(s + 1,s + 1 + n,cmp); for(int i = 1;i &lt;= n;++i) &#123; x = s[i].a * x + s[i].b; x %= 10; &#125; cout &lt;&lt; x &lt;&lt; endl; &#125; return 0;&#125;]]></content>
      <tags>
        <tag>数学思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树镜像]]></title>
    <url>%2F2018%2F06%2F16%2F%E4%BA%8C%E5%8F%89%E6%A0%91%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[问题: 操作给定的二叉树，将其变换为源二叉树的镜像。123456789101112二叉树的镜像定义：源二叉树 8 / \ 6 10 / \ / \ 5 7 9 11 镜像二叉树 8 / \ 10 6 / \ / \ 11 9 7 5 解答: 关于这道题我们可以充分利用递归的思想,既然是要将二叉树镜像,那么显然是需要将左子树变为相应的右子树,右子树反之。这里我们就可以将左子树指向右子树,然后递归处理,右子树同理。 123456789101112131415161718struct TreeNode &#123; int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123; &#125;&#125;;class Solution &#123;public: TreeNode* Mirror(TreeNode *pRoot) &#123; if(pRoot == NULL) return NULL; TreeNode *mid = pRoot -&gt; left; pRoot -&gt; left = Mirror(pRoot -&gt; right); pRoot -&gt; right = Mirror(mid); return pRoot; &#125;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链表反转]]></title>
    <url>%2F2018%2F06%2F15%2F%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[问题: 输入一个链表，反转链表后，输出链表的所有元素。 解答: 这里我们要求不另外开辟空间将链表反转。 我们只需要开辟一个变量保存下当前结点的上一结点,每次将该点的next修改为指向上一个结点即可,注意提前保存该点的next的值,否则整个链表将失去指向性。 12345678910111213141516class Solution &#123;public: ListNode* ReverseList(ListNode* pHead) &#123; //ListNode* rHead = NULL; ListNode* pre = NULL; ListNode* nxt = NULL; while(pHead != NULL) &#123; nxt = pHead -&gt; next; pHead -&gt; next = pre; pre = pHead; pHead = nxt; &#125; return pre; &#125;&#125;;]]></content>
      <tags>
        <tag>剑指offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习之图像分类-----------K最邻近算法（KNN）]]></title>
    <url>%2F2018%2F06%2F14%2FKNN%2F</url>
    <content type="text"><![CDATA[邻近算法,或者说是K最邻近算法,是一个相对简单的多分类算法，其基本工作原理为: 首先我们存在一个训练集,训练集中的每个图片都存在标签（已知图片属于哪一类）.对于我们输入的没有标签的数据，我们将新数据中的每个特征与样本集合中的数据的对应特征进行比较,计算出二者之间的距离，然后记录下与新数据距离最近的K个样本，最后选择K个数据当中类别最多的那一类作为新数据的类别。 下面通过一个简单的例子说明一下：如下图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。 ![该图片也说明了KNN算法的结果很大程度取决于K的选择](https://img-blog.csdn.net/20180609213238608?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hvd2FyZEVtaWx5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) 上面说到要计算二者之间的距离,那么距离怎么计算呢？这里距离一般使用欧式距离，或者曼哈顿距离。 欧氏距离: $d(x,y) = \sqrt{ \sum_{k=1}^n (x_k - y_k)^2 } $ 曼哈顿距离: $ d(x,y) = \sum_{i=1}^n |x_i - y_i| $ 那么在进行图像分类的时候,两个图片之间的距离就是每个像素点距离的和。 KNN算法的流程 (1)计算测试数据与训练数据之间的距离(2)按照距离的递增关系进行排序(3)找出其中距离最近的K个数据(4)确定K个点哪个类别出现的次数最多(概率最大)(5)返回这K个点中类别出现频率最高的作为预测数据的类别 说明: 1)KNN算法不存在训练数据的问题，他是一种懒惰学习,仅仅是把所有的数据保存下来，训练时间开销为02)KNN算法的优点: 简单、有效。计算时间和空间线性于训练集的规模（在一些场合不算太大）3)缺点:计算复杂度高，空间复杂度高。存储开销大,需要把所有数据都保存起来当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import numpy as npclass KNN(object): def __init__(self): pass def train(self,X,Y): """ Train the classifier .For k_nearest_neighbor this is just memorizing the train data. Inputs: - X: A numpy array of shape (num_train, D) containing the training data consisting of num_train samples each of dimension D. - y: A numpy array of shape (N,) containing the training labels, where y[i] is the label for X[i]. """ self.X_train = X self.Y_train = Y def predict(self,X,k = 1,num_loops = 0): """ Predict labels for test data using this classifier. 基于该分类器，预测测试数据的标签分类。 Inputs: - X: A numpy array of shape (num_test, D) containing test data consisting of num_test samples each of dimension D.测试数据集 - k: The number of nearest neighbors that vote for the predicted labels. - num_loops: Determines which implementation to use to compute distances between training points and testing points.选择距离算法的实现方法 Returns: - y: A numpy array of shape (num_test,) containing predicted labels for the test data, where y[i] is the predicted label for the test point X[i]. """ if num_loops == 0: dis = self.compute_no_loop(X) elif num_loops == 1: dis = self.compute_one_loop(X) elif num_loops == 2: dis = self.compute_two_loops(X) else: raise ValueError("Invalid value %d for num_loops" %num_loops) return self.predict_label(dis,k=k) def compute_two_loops(self,X): """ Compute the distance between each test point in X and each training point in self.X_train using a nested loop over both trainging and the test data.(两层for 计算距离,这里计算欧几里得距离) """ num_test = X.shape[0] num_train = self.X_train.shape[0] # dis[i,j] is the Euclidean distance between the ith test data and jth training data. dis = np.zeros((num_test,num_train)) for i in range(num_test): for j in range(num_train): test_row = X[i,:] # choose this test row. train_row = self.X_train[j,:] dis[i,j] = np.sqrt(np.sum((test_row - train_row)**2)) return dis def compute_one_loop(self,X): """ Compute the distance between each test point in X and each trainging point in X_train using a single loop over the test data. dis: the same as compute_two_loops """ num_test = X.shape[0] num_train = self.X_train[0] dis = np.zeros((num_test,num_train)) for i in range(num_test): test_row = X[i,:] # numpy 的broadcasting 机制 dis[i,:] = np.sqrt(np.sum(np.square(test_row - self.X_train),axis = 1))#axis=1 计算每一行的和,返回为一个列表,正好存在第i行 return dis def compute_no_loop(self,X): """ Compute the distance between each test point in X and each training point in X_train without using any loops. """ num_test = X.shape[0] num_train = self.X_train.shape[0] dis = np.zeros((num_test,num_train)) X_square = np.square(X).sum(axis = 1) # 1 * 500 X_train_square = np.square(self.X_train).sum(axis = 1) # 1 * 5000 dis = np.sqrt(-2*np.dot(X,self.X_train.T) + np.matrix(X_square).T + X_train_square) # 500 * 5000 dis = np.array(dis) return dis def predict_label(self,dis,k = 1): """ Given a matrix of distances between test points and training points, predict a label for each test point. """ num_test = dis.shape[0] # y_pred Return the label of ith test data. y_pred = np.zeros(num_test) for i in range(num_test): closest_y = [] closest_y = self.Y_train[np.argsort(dis[i,:])[:k]] # np.argsort 函数返回的是数组值从小到大的索引值(按从小到大的顺序返回每个索引) y_pred[i] = np.argmax(np.bincount(closest_y)) # np.argmax 返回最大值,np.bincount()统计每一个元素出现次数的 return y_pred PS:上面计算两个图片之间的距离用到了三种方法,一是两层循环，一个是一层循环,还有一个是不用循环。最后两个主要是用到了numpy的广播机制。 numpy的广播机制广播(broadcasting)指的是不同形状的数组之间的算数运算的执行方式。1.数组与标量值的乘法： 1234import numpy as nparr = np.arange(5)arr #-&gt; array([0, 1, 2, 3, 4])arr * 4 #-&gt; array([ 0, 4, 8, 12, 16]) 在这个乘法运算中，标量值4被广播到了其他所有元素上。2.通过减去列平均值的方式对数组每一列进行距平化处理 123456arr = np.random.randn(4,3)arr #-&gt; array([[ 1.83518156, 0.86096695, 0.18681254], # [ 1.32276051, 0.97987486, 0.27828887], # [ 0.65269467, 0.91924574, -0.71780692], # [-0.05431312, 0.58711748, -1.21710134]])arr.mean(axis=0) #-&gt; array([ 0.93908091, 0.83680126, -0.36745171]) 关于mean中的axis参数，个人是这么理解的：在numpy中，axis = 0为行轴(竖直方向),axis = 1为列轴（水平方向），指定axis表示该操作沿axis进行，得到结果将是一个shape为除去该axis的array。在上例中,arr.mean(axis=0)表示对arr沿着轴0(竖直方向)求均值，即求列均值。而arr含有3列,所以结果含有3个元素,这与上面的结论相符。 12345678demeaned = arr - arr.mean(axis=0)demeaned&gt; array([[ 0.89610065, 0.02416569, 0.55426426], [ 0.3836796 , 0.1430736 , 0.64574058], [-0.28638623, 0.08244448, -0.35035521], [-0.99339402, -0.24968378, -0.84964963]])demeaned.mean(axis=0)&gt; array([ -5.55111512e-17, -5.55111512e-17, 0.00000000e+00]) 广播原则: 如果两个数组的后缘维度(从末尾开始算起的维度)的轴长度相符或其中一方的长度为1，则认为它们是广播兼容的。广播会在缺失维度和(或)轴长度为1的维度上进行。&lt;/font&gt;3.各行减去行均值 12345678910111213row_means = arr.mean(axis=1)row_means.shape&gt; (4,)arr - row_means&gt; --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-10-3d1314c7e700&gt; in &lt;module&gt;() ----&gt; 1 arr - row_means ValueError: operands could not be broadcast together with shapes (4,3) (4,) 直接相减，报错，无法进行广播。回顾上面的原则，要么满足后缘维度轴长度相等，要么满足其中一方长度为1。在这个例子中，两者均不满足，所以报错。根据广播原则,较小数组的广播维必须为1。解决方案是为较小的数组添加一个长度为1的新轴。numpy提供了一种通过索引机制插入轴的特殊语法。通过特殊的np.newaxis属性以及“全”切片来插入新轴。上面的例子中，我们希望实现二维数组各行减去行均值，我们需要你将行均值沿着水平方向进行广播，广播轴为axis=1，对arr.mean(1)添加一个新轴axis=1 1234567row_means[:,np.newaxis].shape&gt; (4, 1)arr - row_means[:,np.newaxis]&gt; array([[ 0.87419454, -0.10002007, -0.77417447], [ 0.46245243, 0.11956678, -0.58201921], [ 0.36798351, 0.63453458, -1.00251808], [ 0.17378588, 0.81521647, -0.98900235]])]]></content>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每天三道奥数题]]></title>
    <url>%2F2018%2F06%2F14%2F1%2F</url>
    <content type="text"><![CDATA[这是一道逻辑推理的题目 问题: 老师在班上30名同学头上各戴了一顶带颜色帽子,帽子颜色只有黑白两种,而且白帽子个数比黑帽子的个数多。每个同学都只能看到别人的帽子，但看不到自己的帽子。老师连问三次：“请问你们谁知道自己的帽子颜色？”都没有一个人说话。老师第四次问：“请问你们谁知道自己的帽子颜色？”终于有一些同学举起了手。如果班上的同学都很聪明，请问白帽子的同学有多少名？ 解答: 对于这种带询问的问题,一般每一次询问我们都可以看成是一轮推理。首先第一次问学生都没有回答,说明所有的同学都看到了黑帽子,否则如果有人没看到黑帽子那么他们肯定知道自己就是黑帽子了,由此我们推理的黑帽子个数至少为2.同理第二次问的时候说明每个人都看到了至少两顶黑帽子（否则第一轮同学们就知道最少两顶黑帽子,此时自己看到一顶那么自己肯定也是黑帽子了）。由此我们推断至少三顶黑帽子,同理第三轮每人至少看到三顶,也就是至少四顶.第四轮有人回答了,说明黑帽子个数确实是四顶,因为此时有人只看到了三顶他就知道自己是黑的了,此时举手的也都是看到三顶黑帽子的,人数为4.由此白帽子有26顶。 约瑟夫环问题 问题 1.某学校有2018个学生围成一个大圆圈做游戏，这些学生的编号按顺时针依次为1、2、3、…、2018。从1号学生开始不断顺时针报数，报数是偶数的学生就退出游戏。最后剩下一个学生的编号是多少？ 解答: 我们可以发现人数为1,2,4,8,16…剩下的人数刚好是第一个报数的人,也就是1号。那么我们可以想最接近2018的 2^n 的数是1024,什么时候剩下1024个人呢？我们可以求得（2018-1024）*2 = 1988,那么下一个报数的人是1989,所以最后剩下的人是1989. 问题 2.某学校有2018个学生围成一个大圆圈做游戏，这些学生的编号按顺时针依次为1、2、3、…、2018。从1号学生开始不断顺时针报数，报数是奇数的学生就退出游戏。最后剩下一个学生的编号是多少？ 解答: 这里可以发现人数为2,4,8,16…最后一个人总是最后离开的。还是同样的道理,什么时候剩下1024个人呢？依据上面可以得到下一次会从1989开始报数,而1988编程最后一个,所以最后离开的是1988. 循环小数问题 问题： 小明在将分数n/37化为小数时，四舍五入后的计算结果是1.684856。老师说小明的结果中所有数字都正确，小数点以前都没有问题，就是小数点后的数字顺序错了。请问自然数n是多少？ 解答: 先思考第一个问题， n/37化为小数是几位一循环的？由于1/37=0.027027……，因此n/37化为小数是3位一循环。再思考第二个问题，正确的结果中小数最后1位是多少？小明的结果是1.684856，小数点后的数字只是顺序错了，小数点后6位中，6和8都出现2次，4和5各出现1次，而根据步骤1的结论，小数以三位为一循环，只能是在四舍五入中发生了五入，因此最后1位数字是5。综合上述两个问题，正确结果可能是1.684685或1.864865，对此进行讨论：(1)如果结果是1.684685,则n &lt; 1.737=62.345，n &gt; 1.6837=62.16,这样的自然数n不存在。(2) 如果结果是1.864865,则n &lt; 1.8737=69.19，n &gt; 1.8637=68.82,这样的自然数n只能是69，代入检验69/37=1.864864……。所以n=69。 数列规律问题 问题某校5年级所有学生站成一排，先从1到10循环报数，又从1到12循环报数，结果恰好有10名同学既报了2又报了4。请问5年级最多有多少名同学？ 解答:首先报数只有两种情况先报2后报4,和先报4后报2.我们先来考虑第一种。报2的人是第10m+2个人,也是报4的第12n+4个人。所以有10m+2=12n+4;n = 5(m - n) - 1.符合条件的n=4,9,14,19,24…,此时m=5,11,17,23,29…。带入发现,此时数列规律为相邻元素差为60. 先报4后报2的同学序号有什么规律？在从1到10循环报数中报4，说明该同学序号是10m+4,其中m是自然数；在从1到12循环报数中报2，说明该同学序号是12n+2,其中n是自然数。故10m+4=12n+2，化简即6n-5m=1,即n=5(m-n)+1,符合条件的n=1,6,11,16,21…,此时m=1,7,13,19,25…。代入序号10m+4中因此序号是14,74,134,194,254…,规律是数列相差60。 综合上述两个问题，有10名同学既报了2又报了4，说明序号必须包含的数字是14,74,134,194,254和52,112,172,232,292。此后下一个既报2又报4的是314号，所以最多有313名同学。 问题老师组织班上的同学站成一排报数，从1到8循环报数，第一次从左往右报，第二次从右向左报。结果发现有10个人既报2又报3。请问班上有多少名同学？ 解答:先考虑先报2后报3。和上面一样,正着报数序数为8m+2，倒着为8n+3.我们可以知道正着的序数+倒着的序数=k + 1(k为总人数)。所以有 8m+2 + 8n+3 -1 = k.班里的总人数8（n+m)+4.所以可以得到班里的总人数%8余数一定为4.即为8x+4. 我们观察发现从左到右报2的人,回来报3.那么从左到右报3的回来一定报2.x的取值可以为0 ~ x,一共x+1个数,又报2又报3的一共有2(x+1)个人。2x+2 = 10 ，x = 4.所以班上一共有8*4+4 = 36名同学。]]></content>
      <tags>
        <tag>数学思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c语言中移位运算符]]></title>
    <url>%2F2018%2F06%2F14%2Fnn%2F</url>
    <content type="text"><![CDATA[本文说一下c语言中的移位运算符有如下代码: 123456789#include&lt;bits/stdc++.h&gt;using namespace std;int main()&#123; int a = 1,b = 32; printf("%d %d\n",a &lt;&lt; b,1 &lt;&lt; 32);&#125; 输出结果: 1 0 我们可以发现变量的移位运算和常量的移位运算是不一样的,常量的左移的运算后面补0,所以超过31为整个32位全是0,最后结果也是0.而对于变量的移位如果b超过了其size(int 这里32位),就先会用b = b % size ，然后在进行移位,所以是1.]]></content>
      <tags>
        <tag>各种基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二进制中1的个数]]></title>
    <url>%2F2018%2F06%2F14%2F%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0%2F</url>
    <content type="text"><![CDATA[问题描述: 输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 思路: 这个题目卡住我的原因就是我不知道怎么求补码了,其实负数在计算机中存储本来就是按照补码的形式,只是输出的时候又转化成我们对应的数.另外这个题有一个比较基础的知识,也是比较容易陷入坑的一个点.对于c语言的移位运算符”&lt;&lt;”和”&gt;&gt;”对于无符号数他们都是直接补0即可;对于有符号数来说,左移运算还是按照逻辑左移一样直接补0,对于右移的话是在前面补符号位的,所以当n为负数的时候你直接每次右移来算是会死循环的!解决办法我们可以记录一下位数,最多32位。 123456789101112131415class Solution &#123;public: int NumberOf1(int n) &#123; int cnt = 0; int bit = 0; while(bit &lt;= 31) &#123; if(n &amp; 1) cnt++; n &gt;&gt;= 1; bit++; &#125; return cnt; &#125;&#125;; 还可以利用int向unsigned int 的转化，这里的转化是指:当一个负数向unsigned转化时会直接将其转化为一个补码。这时候就可以按照正数来进行正常的右移了. 123456789101112131415class Solution&#123;public: int NumberOf1(int n) &#123; int cnt = 0; unsigned int nn = n; while(nn) &#123; if(nn &amp; 1) cnt++; nn = nn &gt;&gt; 1; &#125; return cnt; &#125;&#125;; 还有一种更优化的方法就是: n = (n - 1) &amp; n,这一步使得n中有多少个1就循环多少次,更优化。 class Solution { public: int NumberOf1(int n) { int cnt = 0; while(n) { cnt++; n = (n - 1) &amp; n; } return cnt; } };]]></content>
      <tags>
        <tag>各种基础知识</tag>
      </tags>
  </entry>
</search>
