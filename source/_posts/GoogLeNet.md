---
title: GoogLeNet(从Inception v1到v4的演进)
copyright: true
mathjax: true
tags: Deep Learning
categories: 深度学习
abbrlink: '74255015'
date: 2018-10-09 21:34:31
updated:
---
[学习自雪饼大佬!](https://my.oschina.net/u/876354/blog/1637819)
{% asset_img 1.png %}
<!--more-->
##为什么有GoogLeNet?
2014年，GoogLeNet和VGG是当年ImageNet挑战赛(ILSVRC14)的双雄，GoogLeNet获得了第一名、VGG获得了第二名，这两类模型结构的共同特点是层次更深了。VGG继承了LeNet以及AlexNet的一些框架结构，而GoogLeNet则做了更加大胆的网络结构尝试，虽然深度只有22层，但大小却比AlexNet和VGG小很多，GoogleNet参数为500万个，AlexNet参数个数是GoogleNet的12倍，VGGNet参数又是AlexNet的3倍，因此在内存或计算资源有限时，GoogleNet是比较好的选择；从模型结果来看，GoogLeNet的性能却更加优越
**小知识**：GoogLeNet是谷歌（Google）研究出来的深度网络结构，为什么不叫“GoogleNet”，而叫“GoogLeNet”，据说是为了向“LeNet”致敬，因此取名为“GoogLeNet”
###GoogLeNet是如何提升性能的?
一般来说,提升网络性能最直接的办法就是增加网络的深度和宽度,**深度指网络层次数量、宽度指神经元数量**。但这种方式存在以下问题:
(1)参数太多,如果训练数据集有限,很容易产生过拟合
(2)网络越大、参数越多，计算复杂度越大，难以应用
(3)网络越深,很容易出现梯度弥散问题(梯度越往后传越容易消失)，难以优化模型
** 解决这些问题的方法**当然就是在增加网络深度和宽度的同时减少参数，为了减少参数，自然就想到将全连接变成稀疏连接。但是在实现上全连接变成稀疏连接后实际计算量并不会有质的提升,因为大部分硬件是针对密集矩阵计算优化的,稀疏矩阵虽然数据量少，但是计算所消耗的时间却难以减少。
那么有没有一种方法既能保持网络结构的稀疏性,又能利用密集矩阵的高计算性能。大量的文献表明可以将稀疏矩阵聚类为较为密集的子矩阵来提高计算性能，就如人类的大脑是可以看做是神经元的重复堆积，因此，GoogLeNet团队提出了Inception网络结构，就是构造一种“基础神经元”结构，来搭建一个稀疏性、高计算性能的网络结构。
##Inception
Inception经历的V1、V2、V3、V4等多个版本的发展。
###Inception V1
通过设计一个稀疏的网络结构,但是能够产生稠密的数据,既能增加神经网络的表现,又能保证计算资源的使用效率。谷歌提出了最原始Inception的基本结构:
{% asset_img 2.png %}
该结构将CNN中常用的卷积($1 \times 1 ,3\times 3,5 \times5$)、池化操作堆叠在一起(卷积、池化后尺寸相同，通道增加),一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性。
网络卷积层中的网络能够提取输入的每一个细节信息,同时$5\times5$的滤波器也能够覆盖大部分接收层的输入。还可以进行一个池化操作,以减少空间大小,降低过度你和。在这些层之上,在每一个卷积层后都要做一个ReLu操作,以增加为网络的非线性特征。
###问题
然而这个原始的Inception版本,所有的卷积核都在上一层的所有输出上来做,这时候$5\times5$卷积所做的计算量就太大了,早成了特征图的厚度很大,为了避免这种情况在$3\times3 5 \times 5$,max-pooling前面分别加上了$1\times1$的卷积核,以起到了降低特征图厚度的作用,如下图所示:
{% asset_img 3.png %}
###$1\times1$的卷积有什么作用呢?
$ 1\times1$卷积核的主要作用就是为了减少维度，还用于修正线性激活ReLU.
{% asset_img 4.png %}
{% asset_img 5.png %}
比如对于上面两张图,单纯的使用$5\times5$的卷积核与中间加一个$1\times1$的卷积核来降低维度,参数少了接近10倍。
**对于这种$1\times1$卷积核我们也叫做bottleneck(瓶颈),存在证明通过引入$1\times1$卷积核对最终的结果并无太大影响**
###GoogLeNet结构图
基于Inception构建了GoogLeNet的网络结构如下（共22层）：
{%asset_img 6.jpg %}
** 说明:**
1.GoogLeNet采用了模块化结构(inception 结构),方便添加和修改
2.网络最后采用了average-pooling(均值池化)来代替全连接层,该想法来自NIN(Network in Network),事实证明这样可以将准确率提高0.6%。但是实际最后还是加了一个全连接层,主要是为了方便对输出进行灵活调整；
3.虽然移除了全连接,但是网络中依然使用了Dropout
4.为了避免梯度消失,网络额外增加了两个辅助的sotfmax用于向前传导梯度(辅助分类器)。辅助分类器是将中间某一层的输出用作分类,并按照一个较少的权重(0.3)加到最终分类结果中,这样相当于做了模型融合,同时也给网络增加了反向传播的梯度信号,也提供了额外的正则化,对于整个网络的训练大有裨益。而在实际测试中,这两个额外的softmax会被去掉。
{%asset_img 7.png %}
####**GoogLeNet网络结构明细表解析如下**:
0、输入
原始输入图像为224x224x3，且都进行了零均值化的预处理操作（图像每个像素减去均值）。
1、第一层（卷积层）
使用7x7的卷积核（滑动步长2，padding为3），64通道，输出为112x112x64，卷积后进行ReLU操作
经过3x3的max pooling（步长为2），输出为((112 - 3+1)/2)+1=56，即56x56x64，再进行ReLU操作
2、第二层（卷积层）
使用3x3的卷积核（滑动步长为1，padding为1），192通道，输出为56x56x192，卷积后进行ReLU操作
经过3x3的max pooling（步长为2），输出为((56 - 3+1)/2)+1=28，即28x28x192，再进行ReLU操作
3a、第三层（Inception 3a层）
分为四个分支，采用不同尺度的卷积核来进行处理
（1）64个1x1的卷积核，然后RuLU，输出28x28x64
（2）96个1x1的卷积核，作为3x3卷积核之前的降维，变成28x28x96，然后进行ReLU计算，再进行128个3x3的卷积（padding为1），输出28x28x128
（3）16个1x1的卷积核，作为5x5卷积核之前的降维，变成28x28x16，进行ReLU计算后，再进行32个5x5的卷积（padding为2），输出28x28x32
（4）pool层，使用3x3的核（padding为1），输出28x28x192，然后进行32个1x1的卷积，输出28x28x32。
将四个结果进行连接，对这四部分输出结果的第三维并联，即64+128+32+32=256，最终输出28x28x256
3b、第三层（Inception 3b层）
（1）128个1x1的卷积核，然后RuLU，输出28x28x128
（2）128个1x1的卷积核，作为3x3卷积核之前的降维，变成28x28x128，进行ReLU，再进行192个3x3的卷积（padding为1），输出28x28x192
（3）32个1x1的卷积核，作为5x5卷积核之前的降维，变成28x28x32，进行ReLU计算后，再进行96个5x5的卷积（padding为2），输出28x28x96
（4）pool层，使用3x3的核（padding为1），输出28x28x256，然后进行64个1x1的卷积，输出28x28x64。
将四个结果进行连接，对这四部分输出结果的第三维并联，即128+192+96+64=480，最终输出输出为28x28x480

第四层（4a,4b,4c,4d,4e）、第五层（5a,5b）……，与3a、3b类似，在此就不再重复。

从GoogLeNet的实验结果来看，效果很明显，差错率比MSRA、VGG等模型都要低，对比结果如下表所示：
{% asset_img 8.png %}
###Inception V2
GoogLeNet凭借其优秀的表现，得到了很多研究人员的学习和使用，因此GoogLeNet团队又对其进行了进一步地发掘改进，产生了升级版本的GoogLeNet。
GoogLeNet设计的初衷就是要又准又快，而如果只是单纯的堆叠网络虽然可以提高准确率，但是会导致计算效率有明显的下降，所以如何在不增加过多计算量的同时提高网络的表达能力就成为了一个问题。
Inception V2版本的解决方案就是修改Inception的内部计算逻辑，提出了比较**特殊的“卷积”计算结构**.
####卷积分解(Factorizing Convolutions )
大尺寸的卷积核可以带来更大的感受野，但也意味着会产生更多的参数,比如$5\times5$的卷积核参数有25个，$3\times3$的卷积核参数有9个,前者是后者的2.78倍。因此GoogLeNet团队提出可以用2个连续的$3\times3$卷积层组成的小网络来代替单个的$5\times5$卷积层，即在保持感受野范围的同时又减少了参数量。如下图：
{%asset_img 9.png %}
** 那么这种替代方案会造成表达能力的下降吗？通过大量实验表明，并不会造成表达缺失**
可以看出，大卷积核完全可以由一系列的3x3卷积核来替代，那能不能再分解得更小一点呢？GoogLeNet团队考虑了nx1的卷积核，如下图所示，用3个$3\times1$ 取代$3\times3$
{%asset_img 10.png %}
<font color = "red">因此,任意$n\times n$的卷积都可以通过$1\times n$后接$n\times1$的卷积来替代。</font>GoogLeNet团队发现在网络的前期使用这种分解效果并不好，在中度大小的特征图（feature map）上使用效果才会更好（特征图大小建议在12到20之间）。
{%asset_img 11.png %}
####降低特征图的大小
一般情况下，如果想让图像缩小，可以有如下两种方式：
{%asset_img 12.png %}
**先做池化再做inception卷积,或者先做Inception卷积再做池化**。但是方法一（左图）先作pooling（池化）会导致特征表示遇到瓶颈（特征缺失），方法二（右图）是正常的缩小，但计算量很大。为了同时保持特征表示且降低计算量，将网络结构改为下图，使用两个并行化的模块来降低计算量（卷积、池化并行执行，再进行合并）
{%asset_img 13.png %}
使用Inception V2作改进版的GoogLeNet，网络结构图如下：
{%asset_img 14.png %}
注：上表中的Figure 5指没有进化的Inception，Figure 6是指小卷积版的Inception（用$3\times 3$卷积核代替$5\times5$卷积核），Figure 7是指不对称版的Inception（用$1\times n$、$n\times1$卷积核代替$n\times n$卷积核）。

经实验，模型结果与旧的GoogleNet相比有较大提升，如下表所示：{%asset_img 15.png %}
###Inception V3
Inception V3一个最重要的改进是分解,将$7 \times 7$ 分解成两个一维的卷积($ 1 \times 7，7 \times 1$),$3 \times 3$ 也是一样的($1 \times 3,  3 \times 1$).这样的好处既可以加速计算,又可以将1个卷积拆成2个卷积,使得网络深度进一步增加,增加了网络的非线性(** 每一层都要进行ReLU**)
另外网络输入从$224 \times 224$ 变为 $ 299 \times 299$
###Inception V4
Inception V4研究了Inception模块与残差连接的结合。ResNet结构大大加深了网络速度,还极大提升了训练速度,同时性能也有提升([关于ResNet](https://statusrank.xyz/2018/09/03/ResNet/))
Inception v4 主要利用残差连接(Residual Connection)来改进V3结构,得到Inception-ResNet-v1,Inception-ResNet-v2，Inception-v4网络
ResNet的残差结构如下:
{% asset_img 16.png %}
将该结构与Inception结构结合,得到如下图:
{% asset_img 17.png %}
通过20个类似的模块组合，Inception-ResNet构建如下：{% asset_img 18.png %}