---
title: 深度学习——--残差网络(ResNet)
copyright: true
mathjax: true
tags: Deep Learning
abbrlink: e8d78fff
date: 2018-09-03 22:30:57
updated:
top:
categories: 深度学习
---
[本文学习自雪饼大佬](https://my.oschina.net/u/876354/blog/1622896)感谢!
##为什么会有残差网络?
###网络越深准确率越高吗？
一说起深度学习,自然也就想到了它非常显著的特点"深"，通过很深层次的网络实现准确率非常高的图像识别、语音识别等。因此,我们大家很自然就想到:深的网络肯定比浅的网络效果好,如果要进一步提升模型的准确率,最直接的方法就是把网络设计的更深,这样模型的准确率也会越来越准确。
那现实是这样吗？我们先来看几个经典的图像识别深度学习模型
<!--more-->
{% asset_img 1.png %}
这几个模型都是在世界顶级比赛中获奖的著名模型,然而这些网络的层次也并没有那么深,最多的也就22层而已,这种算是深度学习吗？为什么不把网络层次加到成百上千层呢？
带着这个问题,我们先来看一个实验,对于常规网络(plain netword，平原网络)直接堆叠很多层次,经对图像识别结果进行检验,训练集、测试集的误差结果如下图:
{% asset_img 2.png%}

从上面两个图可以看出,在网络很深的时候模型的效果越来越差了(误差率越来越大),并不是我们所想的网络越深越好。
通过实验可以发现:<font color = "red">随着网络层级的不断增加,模型精度不断得到提升,而当网络层级增加到一定的数目以后,训练精度和测试精度迅速下降,这说明当网络变得很深以后,深度网络就变得更加难以训练了</font>
### 为什么越深反而效果越差了呢？
下图是一个简单神经网络图,由输入层、隐含层、输出层构成:
{% asset_img 4.png %}
回想一下神经网络反向传播的原理，先通过正向传播计算出结果output，然后与样本比较得出误差值Etotal(损失函数)
{% asset_img 5.png %}
根据我们得到的损失函数值,再用“链式法则”进行反向传播,使误差反向传播从而调整权重w的梯度。
下图是输出结果到隐含层的反向传播过程（隐含层到输入层的反向传播过程也是类似）：
{% asset_img 6.png %}
<font color = "red">通过不断的迭代,对参数矩阵进行不断调整后,使得输出结果的误差值更小,使输出结果与事实更接近。</font>
从上面的过程可以看出,神经网络在反向传播过程中要不断地传播梯度,而当网络层数加深时,梯度在传播过程中会逐渐消失,导致无法对前面网络层的权重进行有效的调整。
那么如何又能加深网络层次,又能解决梯度消失问题,又能提升模型精度呢？
## 深度残差网络(Deep Residual Netword ,DRN)
前面我们描述了,在不断增加神经网络的深度时,模型的准确率会先上升 然后达到饱和,再持续增加梯度时则会导致准确率下降,示意图如下:
{% asset_img 7.png %}

** 那么我们做这样一个假设:**<font color = "red">假设现在有一个比较浅的网络已经达到了饱和准确率,这时在它后面再加上几个恒等映射层(identity mapping,也就是y=x,输入=输出),这样就增加了网络深度,并且起码误差不会增加,也即更深的网络不应该带来训练集误差的上升。而这里提到的使用恒等映射直接将前一层输出传到后面的思想,便是著名的ResNet的灵感来源。</font>
ResNet引入了残差网络结构(Residual Network)，通过这种残差网络结构,可以把网络层弄的很深（据说目前可以达到1000多层）,并且最终的分类效果也很好,残差网络的基本结构如下图:
{% asset_img 8.png %}
很明显，该网络是带有跳跃式结构的，残差网络借鉴了高速网络的跨层链接思想,但对其进行改进(残差项原本是带权值的，但是ResNet采用恒等映射代替)
假定某段神经网络的输入时x,期望输出是H(x),即H(x)是期望的复杂潜在映射,如果要是学习这样的模型,则训练的难度会比较大。回想我们前面的假设,如果学习到较饱和的准确率(或者发现下层的误差变大时),那么接下来的学习目标就会转变为恒等学习,也就是使输入x近似于输出H(x)，以保持在后面的层次中不会造成精度下降。
在上图的残差网络结构中,通过“shortcut connections(捷径连接)”的方式,直接把输入传到输出作为初始结果,使输出结果为H(x) = F(x) + x,当F(x) = 0,那么H(x) = x,也就是上面的恒等映射。于是,ResNet相当于将学习目标改变了,不再是学习一个完整的输出,而是目标值H(x)和x的差值,也就是所谓的残差**F(x) = H(x) - x**,因此后面的目标就是将残差结果逼近于0,使随着网络加深,准确率不下降。
这种残差跳跃式结构,打破了传统的神经网络n-1层的输出只能给n层做输入的惯例,使某一层的输出可以直接跨过几层作为后面某一层的输入，其意义在于为叠加多层网络而使得整个学习模型的错误率不降反升的难题提供了新的方向。
至此，神经网络的层数可以超越之前的约束，达到几十层、上百层甚至千层，为高级语义特征提取和分类提供了可行性。
###ResNet结构图
{% asset_img 9.png %}
### Channel不一样怎么办？
<font color = "red">从图可以看出，怎么有一些“shortcut connections（捷径连接）”是实线，有一些是虚线，有什么区别呢？</font>
{% asset_img 10.png %}
因为经过shortcur连接后, H(x) = F(x) + x,如果F(x) 和 x的通道相同,则可以直接相加，那么通道不同怎么相加呢。
** 实线部分:** 表示通道相同,如上图的第一粉色和第三个粉色矩形,都是进行 $ 3 \times 3 \times 64$的卷积,他们的通道数相同,所以可以直接相加 H(x) = F(x) + x
**虚线部分:** 表示通道不同，如上图的第一个绿色矩形和第三个绿色矩形，分别是$3\times 3\times 64$和$3\times3\times128$的特征图，通道不同，采用的计算方式为H(x)=F(x)+Wx，其中W是卷积操作，用来调整x维度的。
PS:若通道数不同则可以通过$1\times1$的卷积操作来使通道数保持一致,若feature size不一致可以通过调整stride来确定.
除了上面提到的两层残差学习单元，还有三层的残差学习单元，如下图所示：
{% asset_img 11.png %}
###其他结构
两种结构分别是针对ResNet34(左图)和ResNet50/101/152(右)，一般称整个结构为一个”building block“。其中右图又称为"bottleneck design "目的主要是为了降低参数的数目。第一个$1\times1$的卷积把256d降维到64,然后在通过$1\times1$卷积恢复为256d，此时需要的参数为:$1\times1\times256\times64 + 3\times3\times64\times64 + 1\times1\times64\times256 = 69632$.若不采用此结构，就是两个$ 3\times3\times256$的卷积,参数数目为$3\times3\times256\times256\times2 = 1179648$,差了16.94倍。
**<font color = "red">对于常规ResNet，可以用于34层或者更少的网络中，对于Bottleneck Design的ResNet通常用于更深的如101这样的网络中，目的是减少计算和参数量（实用目的）。</font>**
经检验，深度残差网络的确解决了退化问题，如下图所示，左图为平原网络（plain network）网络层次越深（34层）比网络层次浅的（18层）的误差率更高；右图为残差网络ResNet的网络层次越深（34层）比网络层次浅的（18层）的误差率更低。
{% asset_img 12.png %}
##结语
ResNet在ILSVRC2015竞赛中惊艳亮相，一下子将网络深度提升到152层，将错误率降到了3.57，在图像识别错误率和网络深度方面，比往届比赛有了非常大的提升，ResNet毫无悬念地夺得了ILSVRC2015的第一名。如下图所示：
{% asset_img 14.png %}
在ResNet的作者的第二篇相关论文《Identity Mappings in Deep Residual Networks》中，提出了ResNet V2。ResNet V2 和 ResNet V1 的主要区别在于，作者通过研究 ResNet 残差学习单元的传播公式，发现前馈和反馈信号可以直接传输，因此“shortcut connection”（捷径连接）的非线性激活函数（如ReLU）替换为 Identity Mappings。同时，ResNet V2 在每一层中都使用了 Batch Normalization。这样处理后，新的残差学习单元比以前更容易训练且泛化性更强。
 