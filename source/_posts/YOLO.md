---
title: YOLO(You Look Only Once)详解
copyright: true
mathjax: true
tags: YOLO
categories: 深度学习
abbrlink: 4859fe41
date: 2018-10-13 20:35:13
updated:
---
YOLO是目前比较流行的object detection算法,速度快且结构简单。
[转自知乎](https://zhuanlan.zhihu.com/p/32525231)
##前言
当我们谈起计算机视觉时，首先想到的就是图像分类，没错，图像分类是计算机视觉最基本的任务之一，但是在图像分类的基础上，还有更复杂和有意思的任务，如目标检测，物体定位，图像分割等，见图1所示。其中目标检测是一件比较实际的且具有挑战性的计算机视觉任务，其可以看成图像分类与定位的结合，给定一张图片，目标检测系统要能够识别出图片的目标并给出其位置，由于图片中目标数是不定的，且要给出目标的精确位置，目标检测相比分类任务更复杂。目标检测的一个实际应用场景就是无人驾驶，如果能够在无人车上装载一个有效的目标检测系统，那么无人车将和人一样有了眼睛，可以快速地检测出前面的行人与车辆，从而作出实时决策。
<!--more-->
{% asset_img 1.jpg %}
近几年来，目标检测算法取得了很大的突破。比较流行的算法可以分为两类，一类是基于Region Proposal的R-CNN系算法（R-CNN，Fast R-CNN, Faster R-CNN），它们是two-stage的，需要先使用启发式方法（selective search）或者CNN网络（RPN）产生Region Proposal，然后再在Region Proposal上做分类与回归。而另一类是Yolo，SSD这类one-stage算法，其仅仅使用一个CNN网络直接预测不同目标的类别与位置。第一类方法是准确度高一些，但是速度慢，但是第二类算法是速度快，但是准确性要低一些。
本文介绍的是Yolo算法，其全称是You Only Look Once: Unified, Real-Time Object Detection，其实个人觉得这个题目取得非常好，基本上把Yolo算法的特点概括全了：**You Only Look Once说的是只需要一次CNN运算，Unified指的是这是一个统一的框架，提供end-to-end的预测，而Real-Time体现是Yolo算法速度快。**这里我们谈的是Yolo-v1版本算法，其性能是差于后来的SSD算法的，但是Yolo后来也继续进行改进，产生了Yolo9000算法。本文主要讲述Yolo-v1算法的原理，特别是算法的训练与预测中详细细节，最后将给出如何使用TensorFlow实现Yolo算法。
{% asset_img 2.jpg %}
##滑动窗口与CNN
在介绍Yolo算法之前，首先先介绍一下滑动窗口技术，这对我们理解Yolo算法是有帮助的。采用滑动窗口的目标检测算法思路非常简单，它将检测问题转化为了图像分类问题。其基本原理就是采用不同大小和比例（宽高比）的窗口在整张图片上以一定的步长进行滑动，然后对这些窗口对应的区域做图像分类，这样就可以实现对整张图片的检测了，如下图3所示，如DPM就是采用这种思路。但是这个方法有致命的缺点，就是你并不知道要检测的目标大小是什么规模，所以你要设置不同大小和比例的窗口去滑动，而且还要选取合适的步长。但是这样会产生很多的子区域，并且都要经过分类器去做预测，这需要很大的计算量，所以你的分类器不能太复杂，因为要保证速度。解决思路之一就是减少要分类的子区域，这就是R-CNN的一个改进策略，其采用了selective search方法来找到最有可能包含目标的子区域（Region Proposal），其实可以看成采用启发式方法过滤掉很多子区域，这会提升效率。
{% asset_img 3.jpg %}
如果你使用的是CNN分类器，那么滑动窗口是非常耗时的。但是结合卷积运算的特点，我们可以使用CNN实现更高效的滑动窗口方法。这里要介绍的是一种全卷积的方法，简单来说就是网络中用卷积层代替了全连接层，如图4所示。输入图片大小是16x16，经过一系列卷积操作，提取了2x2的特征图，但是这个2x2的图上每个元素都是和原图是一一对应的，如图上蓝色的格子对应蓝色的区域，这不就是相当于在原图上做大小为14x14的窗口滑动，且步长为2，共产生4个字区域。最终输出的通道数为4，可以看成4个类别的预测概率值，这样一次CNN计算就可以实现窗口滑动的所有子区域的分类预测。这其实是overfeat算法的思路。之所可以CNN可以实现这样的效果是因为卷积操作的特性，就是图片的空间位置信息的不变性，尽管卷积过程中图片大小减少，但是位置对应关系还是保存的。说点题外话，这个思路也被R-CNN借鉴，从而诞生了Fast R-cNN算法。
{% asset_img 4.jpg %}
**上面尽管说可以减少滑动窗口的计算量,但是只是针对一个固定大小与步长的窗口,这是远远不够的。YOLO算法很好的解决了这个问题,它不再是窗口滑动了,而是直接将原始图片分割成互不重合的小方块,然后通过卷积最后生产这样大小的特征图。基于上面的分析,可以认为特征图的每个元素也是对应原始图片的一个小方块,然后用每个元素来可以预测那些中心点在该小方格内的目标,这就是YOLO算法的朴素思想。**
###设计理念
整体来看,YOLO算法采用一个单独的CNN模型实现end-to-end的目标检测,整个系统如图所示:首先将输入图片resize到$448 \times 448$ 然后送入CNN网络,最后处理网络预测结果得到检测的目标。相比R-CNN算法,其实一个统一的框架,速度更快,而且YOlO的训练过程也是end-to-end的。
{%asset_img 5.jpg %}
具体来说,Yolo的CNN网络将输入的图片分割成$S\times S$网格,然后**每个单元格负责去检测那些中心点落在该格子内的目标**,如下图所示，可以看到狗这个目标的重心落在左下角一个单元格内，那么该单元格负责预测这个狗。每个单元格会预测$B$个边界框（bounding box）以及边界框的置信度（confidence score).所谓置信度其实包含两个方面,一是**这个边界框含有目标的可能性大小**，二是**这个边界框的准确度**。前者记为$Pr(object)$，当该边界框是北京时,此时$P=0$。而当边界框包含目标时,$Pr(object) = 1$。边界框的准确度可以使用预测框与实际狂的IOU(intersection over union 交并比)来表征.因此置信度可以定义为$ Pr(object) \times IOU $.很多人可能将yolo的置信度看成边界框是否含有目标的概率,但是其实它是两个因此的乘积,预测框的准确度也反应在里面。边界框的大小位置可以用4个值来表征$ (x,y,w,h)$，其中(x,y)是边界框的中心坐标,w和h是边界框的宽和高。 还有一点要注意，中心坐标的预测值 (x,y) 是相对于每个单元格左上角坐标点的偏移值，并且单位是相对于单元格大小的，单元格的坐标定义如图所示。而边界框的 w 和 h 预测值是相对于整个图片的宽与高的比例，这样理论上4个元素的大小应该在 [0,1] 范围。这样，每个边界框的预测值实际上包含5个元素： (x,y,w,h,c) ，其中前4个表征边界框的大小与位置，而最后一个值是置信度。{% asset_img 6.jpg %}
还有分类问题,对于每一个单元格其还要给出预测出C个类别概率值,其表征的是由该单元格负责预测的边界框其目标属于各个类别的概率。但是这些概率值其实是在各个边界框置信度下的条件概率,即$Pr(calss_i|object)$。值得注意的是,不管一个单元格预测多少个边界框,其只预测一组类别概率值,这是yolo算法的一个缺点,在后来的改进版本中，Yolo9000是把类别概率预测值与边界框是绑定在一起的。同时，我们可以计算出各个边界框类别置信度（class-specific confidence scores）: 
{% asset_img 1.png %}
** 边界框类别置信度表征的是该边界框中目标属于各个类别的可能性大小以及边界框匹配目标的好坏。一般会根据类别置信度来过滤网络的预测框**
**总结一下**,每个单元格需要预测$(B \times 5 + C)$个值，如果将输入图片划分为$ S\times S$网格,那么最终预测值为$S\times S \times (B \times 5 + C)$大小的张量。整个模型的预测值结构如下图所示。。对于PASCAL VOC数据，其共有20个类别，如果使用 S=7,B=2 ，那么最终的预测结果就是 $7\times 7\times 30$ 大小的张量。在下面的网络结构中我们会详细讲述每个单元格的预测值的分布位置。{% asset_img 7.jpg %}
###网络设计
Yolo采用卷积网络来提取特征，然后使用全连接层来得到预测值。网络结构参考GooLeNet模型，包含24个卷积层和2个全连接层，如图所示。对于卷积层，主要使用$1\times 1$卷积来做channle reduction，然后紧跟$3\times 3$卷积。对于卷积层和全连接层，采用Leaky ReLU激活函数： $max(x, 0.1x)$ 。但是最后一层却采用线性激活函数。
{% asset_img 8.jpg %}
可以看到网络的最后输出为 $7\times 7\times 30$ 大小的张量。这和前面的讨论是一致的。这个张量所代表的具体含义如图9所示。对于每一个单元格，前20个元素是类别概率值，然后2个元素是边界框置信度，两者相乘可以得到类别置信度，最后8个元素是边界框的 $(x, y,w,h)$ 。大家可能会感到奇怪，对于边界框为什么把置信度 c 和$ (x, y,w,h)$ 都分开排列，而不是按照 
$(x, y,w,h,c)$ 这样排列，其实纯粹是为了计算方便，因为实际上这30个元素都是对应一个单元格，其排列是可以任意的。但是分离排布，可以方便地提取每一个部分。这里来解释一下，首先网络的预测值是一个二维张量 P ，其shape为 $[batch, 7\times 7\times 30] $。
{%asset_img 2.png %}
{%asset_img 9.jpg  %}
##网络训练
在训练之前，先在ImageNet上进行了预训练，其预训练的分类模型采用图8中前20个卷积层，然后添加一个average-pool层和全连接层。预训练之后，在预训练得到的20层卷积层之上加上随机初始化的4个卷积层和2个全连接层。由于检测任务一般需要更高清的图片，所以将网络的输入从224x224增加到了448x448。整个网络的流程如下图所示：{% asset_img 10.jpg %}
[具体的可以去看github](https://github.com/enggen/Deep-Learning-Coursera/blob/master/Convolutional%20Neural%20Networks/Week3/Car%20detection%20for%20Autonomous%20Driving/Autonomous%20driving%20application%20-%20Car%20detection%20-%20v1.ipynb)
