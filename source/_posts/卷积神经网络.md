---
title: 卷积神经网络原理与实例
tags: Deep Learning
mathjax: true
copyright: true
abbrlink: a0a72429
date: 2018-08-06 21:48:24
updated:
---
[本文学习自雪饼大佬,感谢!](https://my.oschina.net/u/876354/blog/1620906)
[其余大部分内容学习自吴恩达深度学习课程](https://www.coursera.org/specializations/deep-learning)
<!--more-->
<h1>1.什么是神经网络？</h1>

这里的神经网络也指人工神经网络（Artificial Neural Networks，简称ANNs），是一种模仿生物神经网络行为特征的算法数学模型，由神经元、节点与节点之间的连接（突触）所构成，如下图：
{% asset_img 1.png %}

每个神经网络单元抽象出来的数学模型如下，也叫感知器，它接收多个输入（x1，x2，x3...），产生一个输出，这就好比是神经末梢感受各种外部环境的变化（外部刺激），然后产	生电信号，以便于转导到神经细胞（又叫神经元）。
{% asset_img 2.png %}
对应公式如下:
$ h_{W,b}(x) = f(W^Tx) = f(\sum_{i = 1}^3 W_ix_i + b)$
单个的感知器就构成了一个简单的模型，但在现实世界中，实际的决策模型则要复杂得多，往往是由多个感知器组成的多层网络，如下图所示，这也是经典的神经网络模型，由输入层、隐含层、输出层构成。
{% asset_img 3.png %}
人工神经网络可以映射任意复杂的非线性关系，具有很强的鲁棒性、记忆能力、自学习等能力，在分类、预测、模式识别等方面有着广泛的应用。

<h1>2.什么是卷积神经网络?</h1>
  受Hubel和Wiesel对猫视觉皮层电生理研究启发，有人提出卷积神经网络（CNN），Yann Lecun 最早将CNN用于手写数字识别并一直保持了其在该问题的霸主地位。近年来卷积神经网络在多个方向持续发力，在语音识别、人脸识别、通用物体识别、运动分析、自然语言处理甚至脑电波分析方面均有突破。

<font color = "red">卷积神经网络最主要的三层:卷积层、池化层、全连接神经网络</font>
卷积神经网络的基本结构图:
	{% asset_img 18.png %}
卷积神经网络与普通的神经网络的区别在于,卷积神经网络包含了一个由卷积层和池化层构成的特征抽取器。在卷积神经网络的卷积层中通常包含若干个特征平面,每个特征平面由一些矩形排列的的神经元组成，同一特征平面的神经元共享权值，这里共享的权值就是卷积核。卷积核一般以随机小数矩阵的形式初始化,在网络的训练过程中卷积核将学习到合理的权值。通常采用卷积核的大小为3*3,也有5*5,7*7，但大多数为奇数.卷积核带来的好处是减少了网络各层之间的连接,参数减少，又降低了过拟合的风险。池化层也叫子采样,通常有max-pooling 和 average-pooling ,但通常我们使用max-pooling.卷积和池化大大简化了模型复杂度,减少了参数。

<h2>1)什么是卷积？</h2>
当给定了一张新图时,CNN并不能准确的知道这些特征到底要匹配原图的哪些部分,所以它会在原图中把每一个可能的位置都进行尝试,相当于把这个特征(feature)变成了一个过滤器。这个用来匹配的过程就称为卷积操作。(特别注意,在卷积神经网络中进行的卷积操作其实本质上并不是严格的卷积,许多数学家把他们叫做互相关,但是在许多DL文献中我们把他看成卷积,真正的卷积是需要进行水平垂直的镜像)
卷积操作如图:
	{% asset_img 4.gif %}
说白了要计算一个feature和其在原图上对应的某一小块的结果,只需要将两个小块对应位置的像素值进行乘法运算,然后将整个小块内的乘法运算的结果累加起来。(也可能还会有其余的操作,比如除以像素点总个数啊等等)。

如下是一个6x6的灰度图像，构造一个3x3的矩阵，在卷积神经网络中通常称之为filter，对这个6x6的图像进行卷积运算，以左上角的-5计算为例 
其它的以此类推，让过滤器在图像上逐步滑动，对整个图像进行卷积计算得到一幅4x4的图像。
{% asset_img 5.png %}

<h3>卷积步长</h3>

卷积步长是指过滤器在图像上滑动的距离，前两部分步长都默认为1，如果卷积步长为2，卷积运算过程为： 

{% asset_img 10.png %}

{% asset_img 11.png %}

{% asset_img 12.png %}

<h2>2)卷积是如何提取特征的？</h2>
<h3>边界检测实例</h3>

假如你有一张如下的图像，你想让计算机搞清楚图像上有什么物体，你可以做的事情是检测图像的垂直边缘和水平边缘。 
{% asset_img 6.png %}

为什么这种卷积计算可以得到图像的边缘，下图0表示图像暗色区域，10为图像比较亮的区域，同样用一个3x3过滤器，对图像进行卷积，得到的图像中间亮，两边暗，亮色区域就对应图像边缘。 

{% asset_img 7.png %}

<font color = "red">注意:这里由于我们图片大小仅仅是6x6所以可以发现中间的亮色区域是很宽的,当我们的图很大的时候是不会发生这种情况的。</font>
通过以下的水平过滤器和垂直过滤器，可以实现图像水平和垂直边缘检测。 

{% asset_img 8.png %}
<h2>padding</h2>
在上部分中，通过一个3x3的过滤器来对6x6的图像进行卷积，得到了一幅4x4的图像，假设输入图像大小nxn，过滤器filter大小为fxf，步长为s,那么输出图像大小为 ($ \lfloor\frac{n - f}{s}\rfloor + 1 $) x ($\lfloor\frac{n - f}{s}\rfloor + 1  $)
<font color = "blue">这样做卷积运算的缺点是卷积图像的大小会不断缩小,另外图像的左上角元素只被一个输出所使用,而下图中红色阴影部分的像素却被多个输出使用了,所以图像的边缘像素在输出中采用较少,这也就意味着你会丢掉许多图像边缘信息,为了引入这两个问题我们引入padding操作,也就是在图像做卷积操作之前,沿着图像边缘用0进行填充对于3x3的过滤器，我们填充宽度为1时，就可以保证输出图像和输入图像一样大。如下图</font>
{% asset_img 9.png %}
padding的两种模式:
	valid: no padding
	same padding: 输入图像和输出图像大小一样大.
这里我们用p来表示我们填充的宽度,则使用padding后我们输出图像的大小关系为:
	($ \lfloor\frac{n + 2p - f}{s} \rfloor + 1$) x ($ \lfloor\frac{n + 2p - f}{s} \rfloor + 1$)

<h2>3)彩色图像的卷积</h2>
以上我们所说的卷积都是灰度值图像的,也就是一维的。我们知道彩色图像在计算机中都是按照RGB来存的,所以如果我们想要在彩色图像上进行卷一那么过滤器的大小就不能使$3x3$ 而应该是$3x3x3$（RGB三通道,通道数）卷积生成图像中每个像素值为 $3*3*3$ 过滤器对应位置和图像对应位置相乘累加,过滤器依次再RGB图像上滑动,最终生成图像大小为$4x4$
{% asset_img 13.png %}
另外一个问题是，如果我们在不仅仅在图像总检测一种类型的特征，而是要同时检测垂直边缘、水平边缘、45度边缘等等，也就是多个过滤器的问题。如果有两个过滤器，最终生成图像为$4x4x2$的立方体，这里的2来源于我们采用了两个过滤器。如果有10个过滤器那么输出图像就是$4x4x10$的立方体。
{% asset_img 14.png %}

<h2>4)池化层Pooling</h2>
为了有效减少计算,CNN使用另一个有效的工具被称为"池化"。池化就是将输入图像进行缩小,减少像素信息,保留重要的信息。
<h3>Max-pooling最大值池化</h3>
最大池化思想很简单，以下图为例，把4x4的图像分割成4个不同的区域，然后输出每个区域的最大值，这就是最大池化所做的事情。其实这里我们选择了2*2的过滤器，步长为2。在一幅真正的图像中提取最大值可能意味着提取了某些特定特征，比如垂直边缘、一只眼睛等等。 
{% asset_img 15.png %}

以下是一个过滤器大小为3*3，步长为1的池化过程，具体计算和上面相同，最大池化中输出图像的大小计算方式和卷积网络中计算方法一致，如果有多个通道需要做池化操作，那么就分通道计算池化操作。 
{% asset_img 16.png %}
最大池化（max-pooling）因为对于每一个filter,它其实就是一个特征提取器,专门用来提取某种特征,所以对于特定区域内,最后的结果越大表示匹配的越好。也就是说，它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。
<h3>average polling</h3>
平均池化和最大池化唯一的不同是，它计算的是区域内的平均值而最大池化计算的是最大值。在日常应用使用最多的还是最大池化。 
{% asset_img 17.png %}

<font color = "red">池化的超参数:filter的大小,步长,池化的类型.</font>
<font color = "red">虽然这里池化层的参数我们也叫超参数,但是一般情况下她其实就是固定的,是不需要CNN进行学习的,一般我们采用最大值池化,过滤器大小为2,步长为2.</font>

<h2>5)激活函数Relu</h2>
上面介绍卷积神经网络CNN结构时用的那个图大家可以看到,卷积层我们一般用CONV来表示,RELU就是我们激活函数的一种,POOL就是我们的池化层，FC就是全连接神经网络。这里需要注意的是池化层的多少没有很明确的规定,需要我们自己进行交叉验证或者调参来决定。而每一个卷积层后都是要跟着激活函数的RELU的。
常用的激活函数有sigmod,tanh,relu等，其中sigmod,tanh主要用于全连接fc层,而Relu主要用于卷积层。回顾一下我们前面讲的神经网络中,单个神经元在接受到和输入计算wx+b后,求和经过一个函数f,输出得到h(x).这里的f就是我们的激活函数。
<font color = "red">激活函数的主要作用是加入非线性因素,把卷积层输出结果做非线性映射。这也是我们引入神经网络的目的,如果不加入激活函数那么对wx+b的计算还是线性的</font>
{% asset_img 19.png %}

在卷积神经网络中激活函数一般使用Relu(The Rectified Linear Unit，修正线性单元),他的主要特点是收敛速度快,求梯度简单,公式简单max(0,s)，即对于输入的负值输出全为0,正值输出其本身。

<h2>6)全连接层(Fully connected layers)</h2>
全连接层在整个神经网络中期到分类器的作用,即通过卷积、激活函数、池化等深度网络后,再经过全连接层对结果进行分类。这里我们还是要将得到的图像拉伸成一个一维向量,就像前面我们使用fc对图像(像素较小的)进行分类一样，由于神经网络是属于监督学习，在模型训练时，根据训练样本对模型进行训练，从而得到全连接层的权重(在每个分类的得分)
{% asset_img 20.png %}

<h1>3.为什么我们需要CNN?</h1>
全连接神经网络参数过于庞大,例如:假设我们有一张1000x1000的图像,隐藏层我们有100万个神经元,由于每个隐藏层的神经元都要与图像的每个像素点连接,所以这里我们就有$1000000 * 1000000 = 10^{12}$ 个连接,也就需要$10^{12}$个参数,那么这个参数是过于庞大的。

卷积神经网络有两个神器可以用来降低参数,一个叫局部感受野,一叫交权值共享。

<h2>局部感受野</h2>
我们一般认为人对外界的认知是从局部到全局的,而图像的空间联系也是局部的像素联系较为紧密,而距离较远的像素相关性较弱。因而,每个神经元其实没必要对全局图像进行感知,只需要对局部进行感知,然后在更高层将局部的信息综合起来就得到了全局的信息。这种思想也是受启发与生物学里面的系统结构，视觉皮层的神经元就是局部接受信息的（即这些神经元只响应某些特定区域的刺激）
如下图所示：左图为全连接，右图为局部连接。
{% asset_img 21.jpg %}

这个图就是描述上面的例子,这里如果我们采用局部感受野的话可以知道,每个像素不需要和所有神经元都连接,假设局部感受野的大小为10x10,隐藏层的神经元都只和每个感受野连接,此时我们的参数个数为 $1000000 * 10 * 10 = 10^8.$ 这样的话我们的参数个数就变为了原来的万分之一,但是还是很多啊,所以就出现了权值共享

<h2>权值共享</h2>
上面当中每个隐藏层神经元都连接不同的局部感受野,那么如果每个局部感受野的参数都相同,那么我的参数就变为 10*10 = 100.我们可以把这100参数对应的卷积操作看成是提取特征的一种方式,与位置无关。这是CNN的亮点之一,但是有一个问题就是说如果所有的参数都相同,也就是说我们只是提取了特定的某种特征,那么如果我们想要提取多种特征怎么办,这时候我们就需要多增加几个滤波器,每个filter对应提取不同的特征(参数不一样)。

<h1>4.一个简单实例</h1>
假设给定一张图（可能是字母X或者字母O），通过CNN即可识别出是X还是O，如下图所示，那怎么做到的呢
{% asset_img 21.png %}

如果采用经典的神经网络模型，则需要读取整幅图像作为神经网络模型的输入（即全连接的方式），当图像的尺寸越大时，其连接的参数将变得很多，从而导致计算量非常大。
<h2>(1)提取特征</h2>
如果字母X、字母O是固定不变的，那么最简单的方式就是图像之间的像素一一比对就行，但在现实生活中，字体都有着各个形态上的变化（例如手写文字识别），例如平移、缩放、旋转、微变形等等，如下图所示：
{% asset_img 22.png %}
我们的目标是对于各种形态变化的X和O，都能通过CNN准确地识别出来，这就涉及到应该如何有效地提取特征，作为识别的关键因子。
<font color = "red"></font>
对于CNN来说，它是一小块一小块地来进行比对，在两幅图像中大致相同的位置找到一些粗糙的特征（小块图像）进行匹配，相比起传统的整幅图逐一比对的方式，CNN的这种小块匹配方式能够更好的比较两幅图像之间的相似性。如下图：{% asset_img 23.png %}
以字母X为例，可以提取出三个重要特征（两个交叉线、一个对角线），如下图所示：
{% asset_img 24.png %}
假如以像素值"1"代表白色，像素值"-1"代表黑色，则字母X的三个重要特征如下：
{% asset_img 27.png %}
<h2>(2)计算卷积</h2>
在本案例中，要计算一个feature（特征）和其在原图上对应的某一小块的结果，只需将两个小块内对应位置的像素值进行乘法运算，然后将整个小块内乘法运算的结果累加起来，最后再除以小块内像素点总个数即可（注：也可不除以总个数的）。
如果两个像素点都是白色（值均为1），那么$1x1 = 1$，如果均为黑色，那么$(-1)*(-1) = 1$，也就是说，每一对能够匹配上的像素，其相乘结果为1。类似地，任何不匹配的像素相乘结果为-1。具体过程如下（第一个、第二个……、最后一个像素的匹配结果）：
{% asset_img 28.png %}
{% asset_img 29.png %}
{% asset_img 30.png %}

根据卷积的计算方式，第一块特征匹配后的卷积计算如下，结果为1
{% asset_img 31.png %}
对于其它位置的匹配，也是类似（例如中间部分的匹配）
{% asset_img 32.png %}
计算之后的卷积如下：
	{% asset_img 33.png %}
以此类推，对三个特征图像不断地重复着上述过程，通过每一个feature（特征）的卷积操作，会得到一个新的二维数组，称之为feature map。其中的值，越接近1表示对应位置和feature的匹配越完整，越是接近-1，表示对应位置和feature的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。如下图所示：
{% asset_img 34.png %}
可以看出，当图像尺寸增大时，其内部的加法、乘法和除法操作的次数会增加得很快，每一个filter的大小和filter的数目呈线性增长。由于有这么多因素的影响，很容易使得计算量变得相当庞大。

<h2>(3)池化</h2>
采用max-pooling
如下图：
{% asset_img 35.png %}
池化区域往左，第二小块取大值max(0.11,0.33,-0.11,0.33)，作为池化后的结果，如下图：
{% asset_img 36.png %}
其它区域也是类似，取区域内的最大值作为池化后的结果，最后经过池化后，结果如下：
{% asset_img 37.png %}
对所有的feature map执行同样的操作，结果如下：
{% asset_img 38.png %}
<font color = "blue">最大池化（max-pooling）保留了每一小块内的最大值，也就是相当于保留了这一块最佳的匹配结果（因为值越接近1表示匹配越好）。也就是说，它不会具体关注窗口内到底是哪一个地方匹配了，而只关注是不是有某个地方匹配上了。
通过加入池化层，图像缩小了，能很大程度上减少计算量，降低机器负载。</font>

<h2>(4)Relu激活函数</h2>
第一个值，取max(0,0.77)，结果为0.77，如下图
{% asset_img 40.png %}
以此类推，经过ReLU激活函数后，结果如下：
{% asset_img 41.png %}
对所有的feature map执行ReLU激活函数操作，结果如下：
{% asset_img 42.png %}

<h2>(5)深度神经网络</h2>
通过将上面所提到的卷积、激活函数、池化组合在一起，就变成下图：
{% asset_img 43.png %}
通过加大网络的深度，增加更多的层，就得到了深度神经网络，如下图：
{% asset_img 44.png %}

<h2>(6)全连接层</h2>
全连接层在整个卷积神经网络中起到“分类器”的作用，即通过卷积、激活函数、池化等深度网络后，再经过全连接层对结果进行识别分类。
首先将经过卷积、激活函数、池化的深度网络后的结果串起来，如下图所示：
{% asset_img 45.png %}
由于神经网络是属于监督学习，在模型训练时，根据训练样本对模型进行训练，从而得到全连接层的权重（如预测字母X的所有连接的权重）
{% asset_img 46.png %}

在利用该模型进行结果识别时，根据刚才提到的模型训练得出来的权重，以及经过前面的卷积、激活函数、池化等深度网络计算出来的结果，进行加权求和，得到各个结果的预测值，然后取值最大的作为识别的结果（如下图，最后计算出来字母X的识别值为0.92，字母O的识别值为0.51，则结果判定为X）
{% asset_img 47.png %}

<font color = "red">上述这个过程定义的操作为”全连接层“(Fully connected layers)，全连接层也可以有多个，如下图：</font>
{% asset_img 48.png %}

<h2>(7)卷积神经网络</h2>

将以上所有结果串起来后，就形成了一个“卷积神经网络”（CNN）结构，如下图所示:
{% asset_img 49.png %}

再回顾总结一下，卷积神经网络主要由两部分组成，一部分是特征提取（卷积、激活函数、池化），另一部分是分类识别（全连接层），下图便是著名的手写文字识别卷积神经网络结构图：
{% asset_img 50.png %}