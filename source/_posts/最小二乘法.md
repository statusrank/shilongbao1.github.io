---
title: 机器学习————最小二乘法
copyright: true
mathjax: true
tags:
  - 最小二乘法
  - 机器学习
categories: 机器学习
abbrlink: 79466b36
date: 2018-09-07 18:15:46
updated:
---

学习自,[马同学](https://www.matongxue.com/madocs/818.html)并有一定的修改,再次感谢！
##什么是最小二乘法？
最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。<!--more-->
##最小二乘法
### 日用而不知
来看一个生活中的例子。比如说，有五把尺子：{%asset_img 1.jpg %}
用它们来分别测量一线段的长度，得到的数值分别为（颜色指不同的尺子）：{%asset_img 2.svg %}
之所以出现不同的值可能因为：
不同厂家的尺子的生产精度不同
尺子材质不同，热胀冷缩不一样
测量的时候心情起伏不定....
总之就是有误差，这种情况下，一般取平均值来作为线段的长度：$\frac{10.2 + 10.3 + 9.8 + 9.9 + 9.8}{5} = 10$
日常中就是这么使用的。可是作为很事'er的数学爱好者，自然要想下：
这样做有道理吗？
用调和平均数行不行？
用中位数行不行？
用几何平均数行不行？
###最小二乘法
换一种思路来思考刚才的问题。首先，把测试得到的值画在笛卡尔坐标系中，分别记作$y_i$
{%asset_img 3.png %}
其次，把要猜测的线段长度的真实值用平行于横轴的直线来表示（因为是猜测的，所以用虚线来画），记作$y$:
{%asset_img 4.png %}
每个点都向$y$做垂线,垂线的长度就是$|y-y_i|$,也可以理解为测量值和真实值之间的误差:
{%asset_img 5.png %}
因为误差是长度，还要取绝对值，计算起来麻烦，就干脆用平方来代表误差：(平方误差对异常值也很敏感,也即误差大的会加倍大)
$|y-y_i| -> (y-y_i)^2$
总的误差平方就是: $\epsilon = \sum(y-y_i)^2$因为y是猜测的所以可以不断变换,
{%asset_img 6.png%}自然，总的误差$\epsilon$也是在不断变化的。
法国数学家，阿德里安-馬里·勒讓德（1752－1833，这个头像有点抽象）提出让总的误差的平方最小的y就是真值，这是基于，如果误差是随机的，应该围绕真值上下波动.
这就是最小二乘法，即：这就是最小二乘法，即：
$\epsilon=\sum (y-y_i)^2最小\implies 真值y$
这个猜想也蛮符合直觉的，来算一下。
这是一个二次函数，对其求导，导数为0的时候取得最小值：
$$
\begin{aligned}
    \frac{d}{dy}\epsilon
        &=\frac{d}{dy}\sum (y-y_i)^2=2\sum (y-y_i)\\
        \quad\\
        &=2((y-y_1)+(y-y_2)+(y-y_3)+(y-y_4)+(y-y_5))=0
        \quad\\
\end{aligned}$$
进而：

  $  5y=y_1+y_2+y_3+y_4+y_5\implies y=\frac{y_1+y_2+y_3+y_4+y_5}{5}$
正好是算术平均数。
原来算术平均数可以让误差最小啊，这下看来选用它显得讲道理了。
以下这种方法：

$\epsilon=\sum (y-y_i)^2最小\implies 真值y$
就是最小二乘法，所谓“二乘”就是平方的意思，台湾直接翻译为最小平方法。
##推广
算术平均数只是最小二乘法的特例，适用范围比较狭窄。而最小二乘法用途就广泛。比如温度与冰淇淋的销量{%asset_img 7.svg%}看上去像是某种线性关系：{%asset_img 8.png %}
可以假设这种线性关系为：
$f(x)=ax+b$通过最小二乘法的思想：{%asset_img 9.png %}
上图的i,x,y分别为：{%asset_img 10.svg %}
总误差的平方为：
$
\epsilon=\sum (f(x_i)-y_i)^2=\sum (ax_i+b-y_i)^2$
不同的a,b会导致不同的$\epsilon$，根据多元微积分的知识，当：
$$
\begin{cases}
    \frac{\partial}{\partial a}\epsilon=2\sum (ax_i+b-y_i)x_i=0\\
    \quad\\
    \frac{\partial}{\partial b}\epsilon=2\sum (ax_i+b-y_i)=0
\end{cases}$$
这个时候$\epsilon$取最小值。
对于a,b而言，上述方程组为线性方程组，用之前的数据解出来：
$$
\begin{cases}
    a\approx 7.2\\
    \quad\\
    b\approx -73
\end{cases}$$
也就是这根直线：{% asset_img 11.png %}
其实，还可以假设：
$f(x)=ax^2+bx+c$
在这个假设下，可以根据最小二乘法，算出a,b,c，得到下面这根红色的二次曲线：{% asset_img 12.png %}
同一组数据，选择不同的$f(x)$，通过最小二乘法可以得到不一样的拟合曲线{%asset_img 13.png %}
不同的数据，更可以选择不同的$f(x)$，通过最小二乘法可以得到不一样的拟合曲线：{%asset_img 14.jpg %}

##最小二乘法与正态分布
数学王子高斯（1777－1855）也像我们一样心存怀疑。
高斯换了一个思考框架，通过概率统计那一套来思考。
让我们回到最初测量线段长度的问题。高斯想，通过测量得到了这些值：{%asset_img 2.svg %}
每次的测量值$x_i$都和线段长度的真值x之间存在一个误差：

$\epsilon_i=x-x_i$
这些误差最终会形成一个概率分布，只是现在不知道误差的概率分布是什么。假设概率密度函数为：
$p(\epsilon)$
再假设一个联合概率密度函数，这样方便把所有的测量数据利用起来：
$$
\begin{aligned}
    L(x)
        &=p(\epsilon_1)p(\epsilon_2)\cdots p(\epsilon_5)\\
        \quad\\
        &=p(x-x_i)p(x-x_2)\cdots p(x-x_5)
\end{aligned}$$
讲到这里，有些同学可能已经看出来了上面似然函数了（关于似然函数以及马上要讲到的极大似然估计，可以参考“[如何理解极大似然估计法？](https://www.matongxue.com/madocs/447.html)”）。
因为$L(x)$是关于x的函数，并且也是一个概率密度函数（下面分布图形是随便画的）：
{% asset_img 15.png %}
根据极大似然估计的思想，概率最大的最应该出现（既然都出现了，而我又不是“天选之才”，那么自然不会是发生了小概率事件），也就是应该取到下面这点：{% asset_img 16.png %}
当下面这个式子成立时，取得最大值：
$\frac{d}{dx}L(x)=0 $
然后高斯想，最小二乘法给出的答案是：
$x=\overline{x}=\frac{x_1+x_2+x_3+x_4+x_5}{5}$
{% asset_img 17.png %}
那么误差的分布是正态分布吗？
我们相信，误差是由于随机的、无数的、独立的、多个因素造成的，比如之前提到的：
不同厂家的尺子的生产精度不同
尺子材质不同，热胀冷缩不一样
测量的时候心情起伏不定
......
那么根据中心极限定理（参考“[为什么正态分布如此常见？](https://www.matongxue.com/madocs/589.html)”），误差的分布就应该是正态分布。
因为高斯的努力，才真正奠定了最小二乘法的重要地位。
##最小二乘法与梯度下降
最小二乘法和梯度下降都是通过求导来求损失函数的最小值。
###相同点
1.本质相同。两种方法都是在给定已知数据的前提下对变量算出一个一般性的估计函数。然后对新给定的数据进行估算
2.目标相同。都是在已知数据的框架内使得估算值与实际值的平方差尽量更小。估算值与实际值的总平方差的公式为
 $\Delta =\frac{1}{2} \sum_{i=1}^{m}{(f_{\beta }(\bar{x_{i}} )-y_{i})^{2} } $
###不同点
1.实现方法和结果不同,最小二乘法是直接对$\Delta$求导找出全局最小,是非迭代法。而梯度下降法是一种迭代法,先给定一个$\beta$然后向$\Delta$下降最快的方向进行调整$\beta$,在若干次后找到局部最小。
###孰好孰坏
梯度下降法的缺点是到最小点的时候收敛速度变慢,并且对初始点的选择及其敏感,他找到的并不是一个全局最优解,只是一个局部最优的。
最小二乘法缺陷在于,有的时候对与平方差损失函数我们并不太好直接求解出偏导数为0的方程，这时候我们就考虑采用梯度下降法进行迭代了.
