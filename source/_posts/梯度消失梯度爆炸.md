---
title: 神经网络中的梯度消失、爆炸原因及解决办法
copyright: true
mathjax: true
tags: Deep Learning
categories: 深度学习
abbrlink: 1798e6c8
date: 2018-09-05 19:35:56
updated:
---
[本文转载来源](https://blog.csdn.net/qq_25737169/article/details/78847691#commentsedit)
##为什么要使用梯度更新规则？
我们先来简单说一下梯度小时的根源--深度神经网络和反向传播。目前深度学习方法中，深度神经网络的发展造就了我们可以构建更深层的网络完成更复杂的任务，深层网络比如深度卷积网络，LSTM等等，而且最终结果表明，在处理复杂任务上，深度网络比浅层的网络具有更好的效果。但是，目前优化神经网络的方法都是基于反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。
<!--more-->
这样做是有一定原因的，首先，深层网络由许多非线性层堆叠而来,每一层非线性层都可以视为是一个非线性函数$f(x)$因此整个深度网络可以视为是一个复合的非线性多元函数
$F(x) = f_n(...f_3(f_2(f1(x)\times \theta_1 + b_1) \times \theta_2 + b_2)...)$
我们最终的目的是希望这个多元函数可以很好的完成输入到输出之间的映射，假设不同的输入，输出的最优解是$ g(x)$那么，优化深度网络就是为了寻找到合适的权值,满足$loss = L(g(x),f(x))$取得极小值点。$ loss = ||g(x) - f(x)||_2^2$ ,假设损失函数的数据空间是下图这样的，我们最优的权值就是为了寻找下图中的最小值点，对于这种数学寻找最小值问题，采用梯度下降的方法再适合不过了。 
{% asset_img 1.png %}
##梯度消失与梯度爆炸
**梯度消失一般两种情况下容易发生:**一是在深层神经网络中,二是采用了不合适的激活函数(典型的sigmoid函数)。**梯度爆炸**一般出现在深层网络和权值初始化值太大的情况下。
###深层网络角度
{% asset_img 2.png %}
{% asset_img 3.png %}
$ \frac{\partial f_4}{\partial f_3}$ 就是对激活函数进行求导,如果此部分大于1,那么层数增多的时候最终求出的梯度更新将以指数形式增加,即发生梯度爆炸,如果小于1,就会以指数形式衰减,就是梯度消失。如果说从数学上看不够直观的话，下面几个图可以很直观的说明深层网络的梯度问题。
<font color = "red">注：下图中的隐层标号和第一张全连接图隐层标号刚好相反。</font>
图中的曲线表示权值更新的速度，对于下图两个隐层的网络来说，已经可以发现隐藏层2的权值更新速度要比隐藏层1更新的速度慢
{% asset_img 4.png %}
 那么对于四个隐层的网络来说，就更明显了，第四隐藏层比第一隐藏层的更新速度慢了两个数量级：
 {% asset_img 5.png %}
<font color = "blue">**总结：**从深层网络角度来讲,不同层学习的速度差异很大,表现为网络中靠近输出的层学习情况很好,靠近输入的层学习的很慢,有时训练了很久,前几层的全职和刚开始初始化的值差不多。因此梯度消失,爆炸其根本原因在于反向传播训练法则.
</font>
###激活函数角度
其实也注意到了，上文中提到计算权值更新信息的时候需要计算前层偏导信息，因此如果激活函数选择不合适，比如使用sigmoid，梯度消失就会很明显了，原因看下图，左图是sigmoid的损失函数图，右边是其倒数的图像，如果使用sigmoid作为损失函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失，sigmoid函数数学表达式为:
$ g(x) = \frac{1}{1 + e^{-x}}$
{% asset_img 6.jpg %}{% asset_img 7.png %}
同理，tanh作为激活函数，它的导数图如下，可以看出，tanh比sigmoid要好一些，但是它的倒数仍然是小于1的。tanh数学表达为：
$ tanh = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ 
{% asset_img 8.png %}
##梯度消失梯度爆炸的解决方案
###预训练+微调
此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。
###梯度剪切
梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。
###正则化
比较常见的是l1正则化和l2正则化。在各个深度框架中都有相应的API可以使用正则化，比如在tensorflowtensorflow中，若搭建网络的时候已经设置了正则化参数，则调用以下代码可以直接计算出正则损失：
```language
regularization_loss = tf.add_n(tf.losses.get_regularization_losses(scope='my_resnet_50'))
```
如果没有设置初始化参数，也可以使用以下代码计算l2l2正则损失：
```language
l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables() if 'weights' in var.name])
```
正则化是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式：
$ loss = (y - W^Tx)^2 + \lambda ||W^2||$
**因此如果发生爆炸,权重W的参数就会非常大。因此我们加入正则项,就会使得loss很大。**
<font color = "red">实际上梯度消失更常见，但是我们加入正则化也可以防止发生过拟合</font>
###选择好的激活函数relu等
####relu
思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。先看一下relu的数学表达式：
{%asset_img 9.png %}
{%asset_img 10.png %}
从上图中，我们可以很容易看出，relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。
**缺点** :由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）
 -- 输出不是以0为中心的
####leakrelu
leakrelu就是为了解决relu的0区间带来的影响，leakrelu解决了0区间带来的影响，而且包含了relu的所有优点。其数学表达为：leakrelu=max(k∗x,x)leakrelu=max(k∗x,x)其中k是leak系数，一般选择0.01或者0.02，或者通过学习而来
{% asset_img 11.png %}
####elu
elu激活函数也是为了解决relu的0区间带来的影响，但是elu相对于leakrelu来说，计算要更耗时间一些.其数学表达为：{% asset_img 12.png %}
其函数及其导数数学形式为：{% asset_img 13.png %}
###BatchNorm
{% asset_img 14.png %}
###残差网络
[关于Resnet](https://statusrank.xyz/2018/09/03/ResNet/)
事实上，就是残差网络的出现导致了image net比赛的终结，自从残差提出后，几乎所有的深度网络都离不开残差的身影，相比较之前的几层，几十层的深度网络，在残差网络面前都不值一提，残差可以很轻松的构建几百层，一千多层的网络而不用担心梯度消失过快的问题，原因就在于残差的捷径（shortcut）部分，其中残差单元如下图所示： {% asset_img 1.jpg %}
相比较于以前网络的直来直去结构，残差中有很多这样的跨层连接结构，这样的结构在反向传播中具有很大的好处，见下式：
{% asset_img 15.png %}
式子的第一个因子$ \frac{\partial loss}{\partial x_L} $表示的损失函数到达 L 的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。(注：上面的推导并不是严格的证明。)
