---
title: 矩阵分解的几种方式
copyright: true
mathjax: true
tags: 矩阵分解
categories: 数学
abbrlink: 71b24cf3
date: 2018-10-28 22:06:37
updated:
---
[学习自](https://blog.csdn.net/bitcarmanlee/article/details/52662518)感谢。
##1.三角分解(LU分解)
**矩阵的LU分解是将一个矩阵分解为一个下三角矩阵与上三角矩阵的乘积。**
本质上,LU分解是高斯消元的一种表达方式。首先,对矩阵A通过初等行变换将其变为一个上三角矩阵。这个将矩阵A变成上三角矩阵的过程对应的操作系数构成一个下三角矩阵。
<!--more-->
若$Ax = b$是一个非奇异系统,那么高斯消元发将A化简为一个上三角矩阵。若主轴上没有0值，则无需交互行，因此只需进行第3类初等行变换（把第 i 行加上第 j 的 k 倍）即可完成此变换。
{% asset_img 1.png %}
第3类行变换可以通过左乘相应的初等矩阵image实现，对上例来说进行的3个变换就是相应初等矩阵的乘积。注意最右边是一个下三角矩阵L {% asset_img 2.png %} 
从而有$G_1G_2G_3A = U$,$ A = G_1^{-1}G_2^{-1}G_3^{-1}U$.因此$A=LU$,为一个下三角矩阵与一个上三角矩阵的乘积,因此称为LU分解。
注意:
1)U是高斯消元的结果,且对角线上是主元
2)L对角线上是1,对角线下面的元素恰恰是1中用于消除(i,j)位置上元素的乘子
LU分解常用来求解线性方程组,求逆矩阵或者计算行列式。例如在计算行列式的时候，$A=LU$，$det(A)=det(L)det(U)$。而对于三角矩阵来说，行列式的值即为对角线上元素的乘积。所以如果对矩阵进行三角分解以后再求行列式，就会变得非常容易。

**在线性代数中已经证明，如果方阵A是非奇异的，即A的行列式不为0，LU分解总是存在的。** 
##2.QR分解
**QR分解是将矩阵分解为一个正交矩阵与上三角矩阵的乘积**
如图:
{% asset_img 3.png %}
这其中,Q为正交矩阵,$QQ^T = I$，R为上三角矩阵
实际中,QR分解经常被用来解线性最小二乘问题
在提到矩阵的QR分解之前,必须提到施密特正交化,理论上QR分解是由施密特正交化推出来的。
###施密特正交化
在三维空间存在直角坐标系,其中任意一点都可以由(x,y,z)坐标唯一确定,在这个坐标系中,X,Y,Z三轴都是相互正交的。那么推广到n维欧式空间,就是n个线性无关的基向量组成的一组基,n维欧式空间的任意位置,都可以由这组基线性表示。
那么就引出来另一个问题，怎么得到一组两两相互正交的正交基呢?这一过程就是Gram–Schmidt正交化。下面简单推理一下Gram–Schmidt正交化方法的得出过程。重点是**正交投影**这种思想。
{%asset_img 4.png %}
###QR分解
对于可逆矩阵A的列向量组$\alpha_1,...\alpha_N$进行施密特正交化,可得标准正交向量
{%asset_img 5.png %}
用矩阵表达:
{%asset_img 6.png %}
** 由此得到QR分解的定义:**
对于n阶方阵A，若存在正交矩阵Q和上三角矩阵R,使得$A=QR$，则该式称为矩阵A的完全QR分解或者正交三角分解。（对于可逆矩阵A存在完全QR分解）
从上面可以看出矩阵QR分解是由施密特正交化推理出来的一种方阵分解形式,矩阵QR分解的计算方法也是以施密特正交化为核心。通过施密特正交化求出正交矩阵Q,在通过$R=Q^TA$得到矩阵R。
 这里对于Gram–Schmidt正交化求正交矩阵Q提出一种改进的计算方法(改进的地方是每产生一个单位正交向量后，就用后续的向量减去它，消去其中包含这个正交向量的部分)：
 {%asset_img 7.png %}
其实对于非方阵的$m\times n$(m≥n)阶矩阵A也可能存在QR分解的，$A = QR$。这时Q为$m\times n$阶的半正交矩阵，R为$n\times n$阶上三角矩阵。这时的QR分解不是完整的(方阵)，因此称为约化QR分解(对于列满秩矩阵A必存在约化$QR$分解)。

同时也可以通过扩充矩阵A为方阵或者对矩阵R补零，可以得到完全QR分解。
##3.Jordan分解
我们将下面的$k\times k$阶方阵{%asset_img 8.png %}
##4.特征值(EVD)分解
###特征值
如果说一个向量v是方阵A的特征向量,则一定可以表示成下面的形式$Av = \lambda v$
这时候$\lambda$被称为特征向量v对应的特征值。
我们知道矩阵乘法本质就是线性变换,即对向量进行旋转、压缩、拉伸等,而对于特征向量我们是只对其进行了拉伸或压缩,大小就是特征值。特征值越大拉伸的幅度越大,特征值绝对值小于1压缩。
###正交矩阵
**正交矩阵是在欧几里得空间里的叫法,在酋空间里叫酋矩阵,一个正交矩阵对应的变换叫做正交变换,这个变换的特点是不改变向量的大小**
如图:{% asset_img 9.png %}
假设二维空间中的向量OA，它在标准坐标系（也就是e1和e2表示的坐标系）中的表示为(a,b)’(‘表示转置)；现在用另一组坐标e1’、e2’表示为(a’,b’)’;那么存在矩阵U使得(a’,b’)’=U(a,b)’,而矩阵U就是正交矩阵。通过上图可知，正交变换只是将变换向量用另一组正交基来表示，在这个过程中并没有对向量作拉伸，也不改变向量的空间位置，对两个向量同时做正交变换，变换的前后两个向量的夹角显然也不会改变。上图是一个旋转的正交变换，可以把e1’、e2’坐标系看做是e1、e2坐标系经过旋转某个θ角度得到，具体的旋转规则如下：
{%asset_img 10.png %}
a’和b’实际上是x在e1’和e2’轴上投影的大小，所以直接做内积可得：{%asset_img 11.png %}
从图中可以看到： {%asset_img 12.png %}
所以有:{%asset_img 13.png %}
正交矩阵U的行(列)之间都是单位正交向量，它对向量做旋转变换。这里解释一下：旋转是相对的，就那上面的图来说，我们可以说向量的空间位置没有变，标准参考系向左旋转了θ角度，而如果我选择了e1’、e2’作为新的标准坐标系，那么在新坐标系中OA（原标准坐标系的表示）就变成了OA’，这样看来就好像坐标系不动，把OA往顺时针方向旋转了θ角度，这个操作实现起来很简单：将变换后的向量坐标仍然表示在当前坐标系中。 
{%asset_img 14.png %}
<font color = "red">总结：正交矩阵的行（列）向量都是两两正交的单位向量，正交矩阵对应的变换为正交变换，它有两种表现：旋转和反射。正交矩阵将标准正交基映射为标准正交基（即图中从e1、e2到e1’、e2’）</font>
###特征值分解
这里我们讨论对称矩阵。对阵阵总能相似对角化,对称阵不同特征值对应的特征向量两两正交。一个矩阵能相似对角化说明其特征子空间为其列空间,若不能对角化则其特征子空间为列空间的子空间。现在假设存在$m\times m$(特征值分解要求必须为方阵)的满秩对称矩阵A,他有m个不同的特征值$\lambda_i$对应的单位特征向量为$x_i$,则有:{%asset_img 15.png %}
进而:{%asset_img 16.png %}
** 所以可以得到A的特征值分解(由于对称矩阵特征向量两两正交,所以U为正交矩阵,$A^T = A^{-1})$**{%asset_img 17.png %} 这里假设A有m个不同的特征值，实际上，只要A是对称阵其均有如上分解.
矩阵A分解了,相应的其对应的映射也分解为三个映射。现在假设有$x$,用A将其变换到A的列空间中,那么首先由$U^T$对x做变换 {%asset_img 18.png %}
U是正交阵U'也是正交阵，所以U'对x的变换是正交变换，它将x用新的坐标系来表示，这个坐标系就是**A的所有正交的特征向量构成的坐标系**。比如将x用A的所有特征向量表示为：{%asset_img 19.png %}
则通过第一个变换就可以把x表示为[a1 a2 ... am]'：{%asset_img 20.png %}
紧接着，在新的坐标系表示下，由中间那个对角矩阵对新的向量坐标换，其结果就是将向量往各个轴方向拉伸或压缩：{%asset_img 21.png %}
从上图可以看到，如果A不是满秩的话，那么就是说对角阵的对角线上元素存在0，这时候就会导致维度退化，这样就会使映射后的向量落入m维空间的子空间中。
最后一个变换就是U对拉伸或压缩后的向量做变换，由于U和UT是互为逆矩阵，所以U变换是UT变换的逆变换。 
因此，从对称阵的分解对应的映射分解来分析一个矩阵的变换特点是非常直观的。假设对称阵特征值全为1那么显然它就是单位阵，如果对称阵的特征值有个别是0其他全是1，那么它就是一个正交投影矩阵，它将m维向量投影到它的列空间中。 
根据对称阵A的特征向量，如果A是$2 \times 2$的，那么就可以在二维平面中找到这样一个矩形，是的这个矩形经过A变换后还是矩形： {%asset_img 22.png %}
**这个矩形的选择就是让其边都落在A的特征向量方向上，如果选择其他矩形的话变换后的图形就不是矩形了！**
##5.奇异值(SVD)分解 
###奇异值
上面说了特征值分解是提取矩阵特征很不错的方法,但这只是针对方阵而言的,在现实世界中大部分的矩阵并不是方阵,这是描述这些普通矩阵的重要特征就会用到**奇异值分解**，他是可以适应任意矩阵分解的方法:$A = U\sum V^T$
假设A是一个$m\times n$的矩阵,那么得到的U是一个$m \times m$的方阵（U里面的向量称为左奇异向量），$\sum$是一个$m \times n$的矩阵（除了对角线外其他元素都为0,对角线上的元素称为奇异值）,$V^T$是一个$n\times n$的矩阵里面的向量也是正交的,（称为右奇异向量）,如图:
{% asset_img 28.png %}

###为什么？
现在我们来分析一下,对于任意$M\times N$的矩阵,能否找到一组正交基使得经过它变换后还是正交基？答案是肯定的，它就是SVD分解的精髓所在。
现在假设存在$M\times N$矩阵A,事实上A矩阵将n维空间中的向量映射到k维空间中,$ k = R(A)$
现在目标就是在n维空间中找一组正交基,使得经过A变换后还是正交的。假设我们已经找到这样一组正交基: $ v_1,v_2,...v_n$,则A矩阵将这组基映射为: $Av_1,Av_2,..Av_n$。如果要让他们两两正交,即：{% asset_img 23.png %}
根据假设存在:{% asset_img 24.png %}
所以如果正交基v选择为$A^TA$的特征向量的话,由于$A^TA$是对阵矩阵,v之间两两正交,那么{% asset_img 25.png %}
**这样就找到正交基使得经过矩阵A线性变化后还是正交基了,现在我们将映射后的正交基单位化** 
因为 
$ |Av_i|^2 = (Av_i)^TAv_i = v_i^TA^TAv_i = \lambda |v_i|^2 = \lambda_i>= 0$ 
(这里因为$v_i$是单位特征向量,x为特征向量,则kx一定也是；所以这里我们只关注单位特征向量)
所以取单位向量:{% asset_img 26.png %}
由此可得 {% asset_img 27.png %}
当k < i <= m时，对$u_1，u_2，...，u_k$进行扩展u(k+1),...,um，使得$u_1，u_2，...，u_m$为m维空间中的一组正交基，即{%asset_img 29.png %}
同样的，对v1，v2，...，vk进行扩展v(k+1),...,vn（这n-k个向量存在于A的零空间中，即Ax=0的解空间的基），使得v1，v2，...，vn为n维空间中的一组正交基，即{%asset_img 30.png %}
则可得到{%asset_img 31.png %}
从而得到A的奇异值分解$A = U \sum V^T$.
V是$n\times n$的正交阵,是$A^TA$的特征向量
U是$m\times m $的正交阵，$u_i = \frac{1}{\sqrt \lambda_i}Av_i$
这里其实可以发现 V 是$ A^TA$的特征向量构成的矩阵,$Av_i$其实是$AA^T$的特征向量,而$A^TA,AA^T$的特征值是相同的!!!! 
$\sum$是$n \times n$的对角阵,$\sqrt \lambda_i$
奇异值跟特征值相似,在矩阵$\sum$也是按照从大到小的方式排列,而且奇异值减少的特别快。在很多的情况下前10%甚至1%的奇异值之和就占了全部奇异值之和的99%以上。也就是说可以用**前r个大的奇异值来近似的描述矩阵**，这里定义奇异值的分解： 
{%asset_img 32.png %}
r是一个远远小于m和n的值，矩阵的乘法看起来是这个样子{%asset_img 33.png %}
<fong color = "red">右边的三个矩阵相乘的结果将会是一个接近于A的矩阵,r越接近n则相乘的结果越接近于A，而这三个矩阵的面积之和(在存储观点来说,矩阵面积越小存储量就越小)要远远小于原始的矩阵A,我们如果想要压缩空间来表示原矩阵A,我们只需要存下$U,\sum,V$这三个矩阵即可</font>
##6.满秩分解
对任意$m \times n$的矩阵均存在如下分解: {% asset_img 34.png %}
** 这个可以应用在数据降维压缩上！在数据相关性特别大的情况下存储X和Y矩阵比存储A矩阵占用空间更小！**
###证明
这个证明需要用到上面的SVD分解。{% asset_img 35.png %}
利用矩阵分块乘法展开得：{% asset_img 36.png %}
可以看到第二项为0，有{% asset_img 37.png %}
令:{% asset_img 38.png %}
{% asset_img 39.png %}
则A=XY即是A的满秩分解。
